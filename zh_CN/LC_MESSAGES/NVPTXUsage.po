# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 20\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-07 18:12+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../NVPTXUsage.rst:3
msgid "User Guide for NVPTX Back-end"
msgstr ""

#: ../../../NVPTXUsage.rst:11
msgid "Introduction"
msgstr ""

#: ../../../NVPTXUsage.rst:13
msgid ""
"To support GPU programming, the NVPTX back-end supports a subset of LLVM IR "
"along with a defined set of conventions used to represent GPU programming "
"concepts. This document provides an overview of the general usage of the "
"back- end, including a description of the conventions used and the set of "
"accepted LLVM IR."
msgstr ""

#: ../../../NVPTXUsage.rst:21
msgid ""
"This document assumes a basic familiarity with CUDA and the PTX assembly "
"language. Information about the CUDA Driver API and the PTX assembly "
"language can be found in the `CUDA documentation <http://docs.nvidia.com/"
"cuda/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:29
msgid "Conventions"
msgstr ""

#: ../../../NVPTXUsage.rst:32
msgid "Marking Functions as Kernels"
msgstr ""

#: ../../../NVPTXUsage.rst:34
msgid ""
"In PTX, there are two types of functions: *device functions*, which are only "
"callable by device code, and *kernel functions*, which are callable by host "
"code. By default, the back-end will emit device functions. Metadata is used "
"to declare a function as a kernel function. This metadata is attached to the "
"``nvvm.annotations`` named metadata object, and has the following format:"
msgstr ""

#: ../../../NVPTXUsage.rst:44
msgid ""
"The first parameter is a reference to the kernel function. The following "
"example shows a kernel function calling a device function in LLVM IR. The "
"function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not."
msgstr ""

#: ../../../NVPTXUsage.rst:66
msgid "When compiled, the PTX kernel functions are callable by host-side code."
msgstr ""

#: ../../../NVPTXUsage.rst:72 ../../../NVPTXUsage.rst:1280
msgid "Address Spaces"
msgstr ""

#: ../../../NVPTXUsage.rst:74
msgid "The NVPTX back-end uses the following address space mapping:"
msgstr ""

#: ../../../NVPTXUsage.rst:77
msgid "Address Space"
msgstr ""

#: ../../../NVPTXUsage.rst:77
msgid "Memory Space"
msgstr ""

#: ../../../NVPTXUsage.rst:79
msgid "0"
msgstr ""

#: ../../../NVPTXUsage.rst:79
msgid "Generic"
msgstr ""

#: ../../../NVPTXUsage.rst:80
msgid "1"
msgstr ""

#: ../../../NVPTXUsage.rst:80
msgid "Global"
msgstr ""

#: ../../../NVPTXUsage.rst:81
msgid "2"
msgstr ""

#: ../../../NVPTXUsage.rst:81
msgid "Internal Use"
msgstr ""

#: ../../../NVPTXUsage.rst:82
msgid "3"
msgstr ""

#: ../../../NVPTXUsage.rst:82
msgid "Shared"
msgstr ""

#: ../../../NVPTXUsage.rst:83
msgid "4"
msgstr ""

#: ../../../NVPTXUsage.rst:83
msgid "Constant"
msgstr ""

#: ../../../NVPTXUsage.rst:84
msgid "5"
msgstr ""

#: ../../../NVPTXUsage.rst:84
msgid "Local"
msgstr ""

#: ../../../NVPTXUsage.rst:87
msgid ""
"Every global variable and pointer type is assigned to one of these address "
"spaces, with 0 being the default address space. Intrinsics are provided "
"which can be used to convert pointers between the generic and non-generic "
"address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:92
msgid ""
"As an example, the following IR will define an array ``@g`` that resides in "
"global device memory."
msgstr ""

#: ../../../NVPTXUsage.rst:99
msgid ""
"LLVM IR functions can read and write to this array, and host-side code can "
"copy data to it by name with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:102
msgid ""
"Note that since address space 0 is the generic space, it is illegal to have "
"global variables in address space 0.  Address space 0 is the default address "
"space in LLVM, so the ``addrspace(N)`` annotation is *required* for global "
"variables."
msgstr ""

#: ../../../NVPTXUsage.rst:109
msgid "Triples"
msgstr ""

#: ../../../NVPTXUsage.rst:111
msgid ""
"The NVPTX target uses the module triple to select between 32/64-bit code "
"generation and the driver-compiler interface to use. The triple architecture "
"can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The "
"operating system should be one of ``cuda`` or ``nvcl``, which determines the "
"interface used by the generated code to communicate with the driver.  Most "
"users will want to use ``cuda`` as the operating system, which makes the "
"generated PTX compatible with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:119
msgid "Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:121
msgid "Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:128
msgid "NVPTX Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:131
msgid "Reading PTX Special Registers"
msgstr ""

#: ../../../NVPTXUsage.rst:134
msgid "'``llvm.nvvm.read.ptx.sreg.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:137 ../../../NVPTXUsage.rst:179
#: ../../../NVPTXUsage.rst:198 ../../../NVPTXUsage.rst:225
#: ../../../NVPTXUsage.rst:260 ../../../NVPTXUsage.rst:290
#: ../../../NVPTXUsage.rst:325 ../../../NVPTXUsage.rst:359
#: ../../../NVPTXUsage.rst:387 ../../../NVPTXUsage.rst:415
#: ../../../NVPTXUsage.rst:441 ../../../NVPTXUsage.rst:472
#: ../../../NVPTXUsage.rst:508 ../../../NVPTXUsage.rst:536
#: ../../../NVPTXUsage.rst:560 ../../../NVPTXUsage.rst:588
#: ../../../NVPTXUsage.rst:630 ../../../NVPTXUsage.rst:661
#: ../../../NVPTXUsage.rst:694 ../../../NVPTXUsage.rst:722
#: ../../../NVPTXUsage.rst:756 ../../../NVPTXUsage.rst:786
#: ../../../NVPTXUsage.rst:829 ../../../NVPTXUsage.rst:859
#: ../../../NVPTXUsage.rst:890 ../../../NVPTXUsage.rst:917
#: ../../../NVPTXUsage.rst:946
msgid "Syntax:"
msgstr ""

#: ../../../NVPTXUsage.rst:156 ../../../NVPTXUsage.rst:186
#: ../../../NVPTXUsage.rst:205 ../../../NVPTXUsage.rst:240
#: ../../../NVPTXUsage.rst:271 ../../../NVPTXUsage.rst:301
#: ../../../NVPTXUsage.rst:335 ../../../NVPTXUsage.rst:366
#: ../../../NVPTXUsage.rst:394 ../../../NVPTXUsage.rst:423
#: ../../../NVPTXUsage.rst:449 ../../../NVPTXUsage.rst:479
#: ../../../NVPTXUsage.rst:515 ../../../NVPTXUsage.rst:543
#: ../../../NVPTXUsage.rst:567 ../../../NVPTXUsage.rst:599
#: ../../../NVPTXUsage.rst:639 ../../../NVPTXUsage.rst:672
#: ../../../NVPTXUsage.rst:703 ../../../NVPTXUsage.rst:733
#: ../../../NVPTXUsage.rst:765 ../../../NVPTXUsage.rst:805
#: ../../../NVPTXUsage.rst:838 ../../../NVPTXUsage.rst:866
#: ../../../NVPTXUsage.rst:897 ../../../NVPTXUsage.rst:924
#: ../../../NVPTXUsage.rst:954
msgid "Overview:"
msgstr ""

#: ../../../NVPTXUsage.rst:158
msgid ""
"The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX "
"special registers, in particular the kernel launch bounds.  These registers "
"map in the following way to CUDA builtins:"
msgstr ""

#: ../../../NVPTXUsage.rst:163
msgid "CUDA Builtin"
msgstr ""

#: ../../../NVPTXUsage.rst:163
msgid "PTX Special Register Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:165
msgid "``threadId``"
msgstr ""

#: ../../../NVPTXUsage.rst:165
msgid "``@llvm.nvvm.read.ptx.sreg.tid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:166
msgid "``blockIdx``"
msgstr ""

#: ../../../NVPTXUsage.rst:166
msgid "``@llvm.nvvm.read.ptx.sreg.ctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:167
msgid "``blockDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:167
msgid "``@llvm.nvvm.read.ptx.sreg.ntid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:168
msgid "``gridDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:168
msgid "``@llvm.nvvm.read.ptx.sreg.nctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:173
msgid "Barriers"
msgstr ""

#: ../../../NVPTXUsage.rst:176
msgid "'``llvm.nvvm.barrier0``'"
msgstr ""

#: ../../../NVPTXUsage.rst:188
msgid ""
"The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0`` "
"instruction, equivalent to the ``__syncthreads()`` call in CUDA."
msgstr ""

#: ../../../NVPTXUsage.rst:192
msgid "Electing a thread"
msgstr ""

#: ../../../NVPTXUsage.rst:195
msgid "'``llvm.nvvm.elect.sync``'"
msgstr ""

#: ../../../NVPTXUsage.rst:207
msgid ""
"The '``@llvm.nvvm.elect.sync``' intrinsic generates the ``elect.sync`` PTX "
"instruction, which elects one predicated active leader thread from a set of "
"threads specified by ``membermask``. The behavior is undefined if the "
"executing thread is not in ``membermask``. The laneid of the elected thread "
"is captured in the i32 return value. The i1 return value is set to ``True`` "
"for the leader thread and ``False`` for all the other threads. Election of a "
"leader thread happens deterministically, i.e. the same leader thread is "
"elected for the same ``membermask`` every time. For more information, refer "
"PTX ISA `<https://docs.nvidia.com/cuda/parallel-thread-execution/index."
"html#parallel-synchronization-and-communication-instructions-elect-sync>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:219
msgid "Membar/Fences"
msgstr ""

#: ../../../NVPTXUsage.rst:222
msgid "'``llvm.nvvm.fence.proxy.tensormap_generic.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:242
msgid ""
"The ``@llvm.nvvm.fence.proxy.tensormap_generic.*`` is a uni-directional "
"fence used to establish ordering between a prior memory access performed via "
"the generic `proxy<https://docs.nvidia.com/cuda/parallel-thread-execution/"
"index.html#proxies>_` and a subsequent memory access performed via the "
"tensormap proxy. ``nvvm.fence.proxy.tensormap_generic.release`` can form a "
"release sequence that synchronizes with an acquire sequence that contains "
"the ``nvvm.fence.proxy.tensormap_generic.acquire`` proxy fence. The "
"following table describes the mapping between LLVM Intrinsic and the PTX "
"instruction:"
msgstr ""

#: ../../../NVPTXUsage.rst:245
msgid "NVVM Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:245
msgid "PTX Instruction"
msgstr ""

#: ../../../NVPTXUsage.rst:247
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:247
msgid "``fence.proxy.tensormap::generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:248
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.acquire.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:248
msgid "``fence.proxy.tensormap::generic.acquire.* [addr], size``"
msgstr ""

#: ../../../NVPTXUsage.rst:251
msgid ""
"The address operand ``addr`` and the operand ``size`` together specify the "
"memory range ``[addr, addr+size)`` on which the ordering guarantees on the "
"memory accesses across the proxies is to be provided. The only supported "
"value for the ``size`` operand is ``128`` and must be an immediate. Generic "
"Addressing is used unconditionally, and the address specified by the operand "
"addr must fall within the ``.global`` state space. Otherwise, the behavior "
"is undefined. For more information, see `PTX ISA <https://docs.nvidia.com/"
"cuda/parallel-thread-execution/#parallel-synchronization-and-communication-"
"instructions-membar>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:254
msgid "Address Space Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:257
msgid "'``llvm.nvvm.isspacep.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:273
msgid ""
"The '``llvm.nvvm.isspacep.*``' intrinsics determine whether the provided "
"generic pointer references memory which falls within a particular address "
"space."
msgstr ""

#: ../../../NVPTXUsage.rst:277 ../../../NVPTXUsage.rst:308
#: ../../../NVPTXUsage.rst:342 ../../../NVPTXUsage.rst:374
#: ../../../NVPTXUsage.rst:402 ../../../NVPTXUsage.rst:429
#: ../../../NVPTXUsage.rst:456
msgid "Semantics:"
msgstr ""

#: ../../../NVPTXUsage.rst:279
msgid ""
"If the given pointer in the generic address space refers to memory which "
"falls within the state space of the intrinsic (and therefore could be safely "
"address space casted to this space), 1 is returned, otherwise 0 is returned."
msgstr ""

#: ../../../NVPTXUsage.rst:284
msgid "Arithmetic Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:287
msgid "'``llvm.nvvm.idp2a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:303
msgid ""
"The '``llvm.nvvm.idp2a.[us].[us]``' intrinsics performs a 2-element vector "
"dot product followed by addition. They corresponds directly to the ``dp2a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:310
msgid ""
"The 32-bit value in ``%a`` is broken into 2 16-bit values which are extended "
"to 32 bits. For the '``llvm.nvvm.idp2a.u.[us]``' variants zero-extension is "
"used, while for the '``llvm.nvvm.idp2a.s.[us]``' sign-extension is used. Two "
"bytes are selected from ``%b``, if ``%is.hi`` is true, the most significant "
"bytes are selected, otherwise the least significant bytes are selected. "
"These bytes are then extended to 32-bits. For the '``llvm.nvvm.idp2a.[us]."
"u``' variants zero-extension is used, while for the '``llvm.nvvm.idp2a.[us]."
"s``' sign-extension is used. The dot product of these 2-element vectors is "
"added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:322
msgid "'``llvm.nvvm.idp4a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:337
msgid ""
"The '``llvm.nvvm.idp4a.[us].[us]``' intrinsics perform a 4-element vector "
"dot product followed by addition. They corresponds directly to the ``dp4a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:344
msgid ""
"Each of the 4 bytes in both ``%a`` and ``%b`` are extended to 32-bit "
"integers forming 2 ``<4 x i32>``. For ``%a``, zero-extension is used in the "
"'``llvm.nvvm.idp4a.u.[us]``' variants, while sign-extension is used with "
"'``llvm.nvvm.idp4a.s.[us]``' variants. Similarly, for ``%b``, zero-extension "
"is used in the '``llvm.nvvm.idp4a.[us].u``' variants, while sign-extension "
"is used with '``llvm.nvvm.idp4a.[us].s``' variants. The dot product of these "
"4-element vectors is added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:353
msgid "Bit Manipulation Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:356
msgid "'``llvm.nvvm.fshl.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:368
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsics performs a clamped "
"funnel shift left. These intrinsics are very similar to '``llvm.fshl``', "
"except the shift ammont is clamped at the integer width (instead of modulo "
"it). Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:376
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsic functions performs a "
"clamped funnel shift left: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted left, and the most significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:384
msgid "'``llvm.nvvm.fshr.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:396
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsics perform a clamped funnel "
"shift right. These intrinsics are very similar to '``llvm.fshr``', except "
"the shift ammont is clamped at the integer width (instead of modulo it). "
"Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:404
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsic functions performs a "
"clamped funnel shift right: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted right, and the least significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:412
msgid "'``llvm.nvvm.flo.u.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:425
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics identifies the bit position "
"of the leading one, returning either it's offset from the most or least "
"significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:431
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics returns the bit position of "
"the most significant 1. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:438
msgid "'``llvm.nvvm.flo.s.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:451
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics identifies the bit position "
"of the leading non-sign bit, returning either it's offset from the most or "
"least significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:458
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics returns the bit position of "
"the most significant 0 for negative inputs and the most significant 1 for "
"non-negative inputs. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:466
msgid "TMA family of Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:469
msgid "'``llvm.nvvm.cp.async.bulk.global.to.shared.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:481
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.global.to.shared.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.global.*`` family of PTX "
"instructions. These instructions initiate an asynchronous copy of bulk data "
"from global memory to shared::cluster memory. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:488 ../../../NVPTXUsage.rst:610
msgid ""
"The last two arguments to these intrinsics are boolean flags indicating "
"support for cache_hint and/or multicast modifiers. These flag arguments must "
"be compile-time constants. The backend looks through these flags and lowers "
"the intrinsics appropriately."
msgstr ""

#: ../../../NVPTXUsage.rst:493
msgid ""
"The Nth argument (denoted by ``i1 %flag_ch``) when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:497
msgid ""
"The [N-1]th argument (denoted by ``i1 %flag_mc``) when set, indicates the "
"presence of a multicast mask (``i16 %mc``) and generates the PTX instruction "
"with the ``.multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:501 ../../../NVPTXUsage.rst:529
#: ../../../NVPTXUsage.rst:553
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:505
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.global``'"
msgstr ""

#: ../../../NVPTXUsage.rst:517
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.global``' intrinsic "
"corresponds to the ``cp.async.bulk.global.shared::cta.*`` set of PTX "
"instructions. These instructions initiate an asynchronous copy from shared::"
"cta to global memory. The 32-bit operand ``%size`` specifies the amount of "
"memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:523 ../../../NVPTXUsage.rst:681
#: ../../../NVPTXUsage.rst:743 ../../../NVPTXUsage.rst:816
msgid ""
"The last argument to these intrinsics is a boolean flag indicating support "
"for cache_hint. This flag argument must be a compile-time constant. When "
"set, it indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:533
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:545
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.shared::cta.*`` PTX "
"instruction. This instruction initiates an asynchronous copy from shared::"
"cta to shared::cluster memory. The destination has to be in the shared "
"memory of a different CTA within the cluster. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:557
msgid "'``llvm.nvvm.cp.async.bulk.prefetch.L2``'"
msgstr ""

#: ../../../NVPTXUsage.rst:569
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.prefetch.L2``' intrinsic corresponds to the "
"``cp.async.bulk.prefetch.L2.*`` family of PTX instructions. These "
"instructions initiate an asynchronous prefetch of bulk data from global "
"memory to the L2 cache. The 32-bit operand ``%size`` specifies the amount of "
"memory to be prefetched in terms of bytes and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:576
msgid ""
"The last argument to these intrinsics is boolean flag indicating support for "
"cache_hint. These flag argument must be compile-time constant. When set, it "
"indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:581
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:585
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:601
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``tile`` mode. In tile mode, the multi-dimensional layout of the source "
"tensor is preserved at the destination. The dimension of the tensor data "
"ranges from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 "
"%d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:615
msgid ""
"The Nth argument (denoted by ``i1 flag_ch``) when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:619
msgid ""
"The [N-1]th argument (denoted by ``i1 flag_mc``) when set, indicates the "
"presence of a multicast mask (``i16 %mc``) and generates the PTX instruction "
"with the ``.multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:623 ../../../NVPTXUsage.rst:654
#: ../../../NVPTXUsage.rst:687 ../../../NVPTXUsage.rst:715
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:627
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:641
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``im2col`` mode. In im2col mode, some dimensions of the source tensor are "
"unrolled into a single dimensional column at the destination. In this mode, "
"the tensor has to be at least three-dimensional. Along with the tensor "
"coordinates, im2col offsets are also specified (denoted by ``i16 im2col0..."
"i16 %im2col2``). The number of im2col offsets is two less than the number of "
"dimensions of the tensor operation. The last two arguments to these "
"intrinsics are boolean flags, with the same functionality as described in "
"the ``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:658
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:674
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``tile`` mode. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:691
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:705
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``im2col`` mode. "
"In this mode, the tensor has to be at least three-dimensional. Unlike the "
"``g2s`` variants, there are no im2col_offsets for these intrinsics. The last "
"argument to these intrinsics is a boolean flag, with the same functionality "
"as described in the ``s2g.tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:719
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:735
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In tile mode, the multi-"
"dimensional layout of the source tensor is preserved at the destination. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:749 ../../../NVPTXUsage.rst:779
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:753
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:767
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In im2col mode, some "
"dimensions of the source tensor are unrolled into a single dimensional "
"column at the destination. In this mode, the tensor has to be at least three-"
"dimensional. Along with the tensor coordinates, im2col offsets are also "
"specified (denoted by ``i16 im2col0...i16 %im2col2``). The number of im2col "
"offsets is two less than the number of dimensions of the tensor operation. "
"The last argument to these intrinsics is a boolean flag, with the same "
"functionality as described in the ``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:783
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:807
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.tile.[1-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[1-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``tile`` mode. The dimension of the tensor data ranges "
"from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 %d4`` "
"arguments. The supported reduction operations are {add, min, max, inc, dec, "
"and, or, xor} as described in the ``tile.1d`` intrinsics."
msgstr ""

#: ../../../NVPTXUsage.rst:822 ../../../NVPTXUsage.rst:849
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"reduce-async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:826
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:840
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.im2col.[3-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[3-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``im2col`` mode. In this mode, the tensor has to be at "
"least three-dimensional. The supported reduction operations supported are "
"the same as the ones in the tile mode. The last argument to these intrinsics "
"is a boolean flag, with the same functionality as described in the ``tile`` "
"mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:853
msgid "Warp Group Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:856
msgid "'``llvm.nvvm.wgmma.fence.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:868
msgid ""
"The '``@llvm.nvvm.wgmma.fence.sync.aligned``' intrinsic generates the "
"``wgmma.fence.sync.aligned`` PTX instruction, which establishes an ordering "
"between prior accesses to any warpgroup registers and subsequent accesses to "
"the same registers by a ``wgmma.mma_async`` instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:873
msgid ""
"The ``wgmma.fence`` instruction must be issued by all warps of the warpgroup "
"in the following locations:"
msgstr ""

#: ../../../NVPTXUsage.rst:876
msgid "Before the first ``wgmma.mma_async`` operation in a warpgroup."
msgstr ""

#: ../../../NVPTXUsage.rst:877
msgid ""
"Between a register access by a thread in the warpgroup and any ``wgmma."
"mma_async`` instruction that accesses the same registers, except when these "
"are accumulator register accesses across multiple ``wgmma.mma_async`` "
"instructions of the same shape in which case an ordering guarantee is "
"provided by default."
msgstr ""

#: ../../../NVPTXUsage.rst:883
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"fence>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:887
msgid "'``llvm.nvvm.wgmma.commit_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:899
msgid ""
"The '``@llvm.nvvm.wgmma.commit_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned`` PTX instruction, which creates a new "
"wgmma-group per warpgroup and batches all prior ``wgmma.mma_async`` "
"instructions initiated by the executing warp but not committed to any wgmma-"
"group into the new wgmma-group. If there are no uncommitted ``wgmma "
"mma_async`` instructions then, ``wgmma.commit_group`` results in an empty "
"wgmma-group."
msgstr ""

#: ../../../NVPTXUsage.rst:907
msgid ""
"An executing thread can wait for the completion of all ``wgmma.mma_async`` "
"operations in a wgmma-group by using ``wgmma.wait_group``."
msgstr ""

#: ../../../NVPTXUsage.rst:910
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"commit-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:914
msgid "'``llvm.nvvm.wgmma.wait_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:926
msgid ""
"The '``@llvm.nvvm.wgmma.wait_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned N`` PTX instruction, which will cause the "
"executing thread to wait until only ``N`` or fewer of the most recent wgmma-"
"groups are pending and all the prior wgmma-groups committed by the executing "
"threads are complete. For example, when ``N`` is 0, the executing thread "
"waits on all the prior wgmma-groups to complete. Operand ``N`` is an integer "
"constant."
msgstr ""

#: ../../../NVPTXUsage.rst:934
msgid ""
"Accessing the accumulator register or the input register containing the "
"fragments of matrix A of a ``wgmma.mma_async`` instruction without first "
"performing a ``wgmma.wait_group`` instruction that waits on a wgmma-group "
"including that ``wgmma.mma_async`` instruction is undefined behavior."
msgstr ""

#: ../../../NVPTXUsage.rst:939
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"wait-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:943
msgid "'``llvm.nvvm.griddepcontrol.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:956
msgid ""
"The ``griddepcontrol`` intrinsics allows the dependent grids and "
"prerequisite grids as defined by the runtime, to control execution in the "
"following way:"
msgstr ""

#: ../../../NVPTXUsage.rst:958
msgid ""
"``griddepcontrol.launch_dependents`` intrinsic signals that the dependents "
"can be scheduled, before the current grid completes. The intrinsic can be "
"invoked by multiple threads in the current CTA and repeated invocations of "
"the intrinsic will have no additional side effects past that of the first "
"invocation."
msgstr ""

#: ../../../NVPTXUsage.rst:960
msgid ""
"``griddepcontrol.wait`` intrinsic causes the executing thread to wait until "
"all prerequisite grids in flight have completed and all the memory "
"operations from the prerequisite grids are performed and made visible to the "
"current grid."
msgstr ""

#: ../../../NVPTXUsage.rst:962
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#parallel-synchronization-and-communication-instructions-"
"griddepcontrol>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:966
msgid "Other Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:968
msgid ""
"For the full set of NVPTX intrinsics, please see the ``include/llvm/IR/"
"IntrinsicsNVVM.td`` file in the LLVM source tree."
msgstr ""

#: ../../../NVPTXUsage.rst:975
msgid "Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:977
msgid ""
"The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` "
"that implements many common mathematical functions. This library can be used "
"as a high-performance math library for any compilers using the LLVM NVPTX "
"target. The library can be found under ``nvvm/libdevice/`` in the CUDA "
"Toolkit and there is a separate version for each compute architecture."
msgstr ""

#: ../../../NVPTXUsage.rst:983
msgid ""
"For a list of all math functions implemented in libdevice, see `libdevice "
"Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:986
msgid ""
"To accommodate various math-related compiler flags that can affect code "
"generation of libdevice code, the library code depends on a special LLVM IR "
"pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. "
"This pass looks for calls to the ``@__nvvm_reflect`` function and replaces "
"them with constants based on the defined reflection parameters. Such "
"conditional code often follows a pattern:"
msgstr ""

#: ../../../NVPTXUsage.rst:1002
msgid "The default value for all unspecified reflection parameters is zero."
msgstr ""

#: ../../../NVPTXUsage.rst:1004
msgid ""
"The ``NVVMReflect`` pass should be executed early in the optimization "
"pipeline, immediately after the link stage. The ``internalize`` pass is also "
"recommended to remove unused math functions from the resulting PTX. For an "
"input IR module ``module.bc``, the following compilation flow is recommended:"
msgstr ""

#: ../../../NVPTXUsage.rst:1009
msgid ""
"The ``NVVMReflect`` pass will attempt to remove dead code even without "
"optimizations. This allows potentially incompatible instructions to be "
"avoided at all optimizations levels by using the ``__CUDA_ARCH`` argument."
msgstr ""

#: ../../../NVPTXUsage.rst:1013
msgid "Save list of external functions in ``module.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:1014
msgid "Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:1015
msgid "Internalize all functions not in list from (1)"
msgstr ""

#: ../../../NVPTXUsage.rst:1016
msgid "Eliminate all unused internal functions"
msgstr ""

#: ../../../NVPTXUsage.rst:1017
msgid "Run ``NVVMReflect`` pass"
msgstr ""

#: ../../../NVPTXUsage.rst:1018
msgid "Run standard optimization pipeline"
msgstr ""

#: ../../../NVPTXUsage.rst:1022
msgid ""
"``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the "
"libdevice functions. It is possible to link two IR modules that have been "
"linked against libdevice using different reflection variables."
msgstr ""

#: ../../../NVPTXUsage.rst:1026
msgid ""
"Since the ``NVVMReflect`` pass replaces conditionals with constants, it will "
"often leave behind dead code of the form:"
msgstr ""

#: ../../../NVPTXUsage.rst:1040
msgid ""
"Therefore, it is recommended that ``NVVMReflect`` is executed early in the "
"optimization pipeline before dead-code elimination."
msgstr ""

#: ../../../NVPTXUsage.rst:1043
msgid ""
"The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the "
"beginning of your pass manager; just use the following code when setting up "
"your pass manager and the PassBuilder will use "
"``registerPassBuilderCallbacks`` to let NVPTXTargetMachine::"
"registerPassBuilderCallbacks add the pass to the pass manager:"
msgstr ""

#: ../../../NVPTXUsage.rst:1057
msgid "Reflection Parameters"
msgstr ""

#: ../../../NVPTXUsage.rst:1059
msgid ""
"The libdevice library currently uses the following reflection parameters to "
"control code generation:"
msgstr ""

#: ../../../NVPTXUsage.rst:1063
msgid "Flag"
msgstr ""

#: ../../../NVPTXUsage.rst:1063
msgid "Description"
msgstr ""

#: ../../../NVPTXUsage.rst:1065
msgid "``__CUDA_FTZ=[0,1]``"
msgstr ""

#: ../../../NVPTXUsage.rst:1065
msgid "Use optimized code paths that flush subnormals to zero"
msgstr ""

#: ../../../NVPTXUsage.rst:1068
msgid ""
"The value of this flag is determined by the \"nvvm-reflect-ftz\" module "
"flag. The following sets the ftz flag to 1."
msgstr ""

#: ../../../NVPTXUsage.rst:1076
msgid ""
"(``i32 4`` indicates that the value set here overrides the value in another "
"module we link with.  See the `LangRef <LangRef.html#module-flags-metadata>` "
"for details.)"
msgstr ""

#: ../../../NVPTXUsage.rst:1081
msgid "Executing PTX"
msgstr ""

#: ../../../NVPTXUsage.rst:1083
msgid ""
"The most common way to execute PTX assembly on a GPU device is to use the "
"CUDA Driver API. This API is a low-level interface to the GPU driver and "
"allows for JIT compilation of PTX code to native GPU machine code."
msgstr ""

#: ../../../NVPTXUsage.rst:1087
msgid "Initializing the Driver API:"
msgstr ""

#: ../../../NVPTXUsage.rst:1101
msgid "JIT compiling a PTX string to a device binary:"
msgstr ""

#: ../../../NVPTXUsage.rst:1114
msgid ""
"For full examples of executing PTX assembly, please see the `CUDA Samples "
"<https://developer.nvidia.com/cuda-downloads>`_ distribution."
msgstr ""

#: ../../../NVPTXUsage.rst:1119
msgid "Common Issues"
msgstr ""

#: ../../../NVPTXUsage.rst:1122
msgid "ptxas complains of undefined function: __nvvm_reflect"
msgstr ""

#: ../../../NVPTXUsage.rst:1124
msgid ""
"When linking with libdevice, the ``NVVMReflect`` pass must be used. See :ref:"
"`libdevice` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:1129
msgid "Tutorial: A Simple Compute Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:1131
msgid ""
"To start, let us take a look at a simple compute kernel written directly in "
"LLVM IR. The kernel implements vector addition, where each thread computes "
"one element of the output vector C from the input vectors A and B.  To make "
"this easier, we also assume that only a single CTA (thread block) will be "
"launched, and that it will be one dimensional."
msgstr ""

#: ../../../NVPTXUsage.rst:1139
msgid "The Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:1178
msgid ""
"We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:"
msgstr ""

#: ../../../NVPTXUsage.rst:1187
msgid ""
"If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32`` "
"in the module data layout string and use ``nvptx-nvidia-cuda`` as the target "
"triple."
msgstr ""

#: ../../../NVPTXUsage.rst:1192
msgid "The output we get from ``llc`` (as of LLVM 3.4):"
msgstr ""

#: ../../../NVPTXUsage.rst:1234
msgid "Dissecting the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:1236
msgid "Now let us dissect the LLVM IR that makes up this kernel."
msgstr ""

#: ../../../NVPTXUsage.rst:1239
msgid "Data Layout"
msgstr ""

#: ../../../NVPTXUsage.rst:1241
msgid ""
"The data layout string determines the size in bits of common data types, "
"their ABI alignment, and their storage size.  For NVPTX, you should use one "
"of the following:"
msgstr ""

#: ../../../NVPTXUsage.rst:1245
msgid "32-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:1251
msgid "64-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:1259
msgid "Target Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1261
msgid ""
"In this example, we use the ``@llvm.nvvm.read.ptx.sreg.tid.x`` intrinsic to "
"read the X component of the current thread's ID, which corresponds to a read "
"of register ``%tid.x`` in PTX. The NVPTX back-end supports a large set of "
"intrinsics.  A short list is shown below; please see ``include/llvm/IR/"
"IntrinsicsNVVM.td`` for the full list."
msgstr ""

#: ../../../NVPTXUsage.rst:1269
msgid "Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:1269
msgid "CUDA Equivalent"
msgstr ""

#: ../../../NVPTXUsage.rst:1271
msgid "``i32 @llvm.nvvm.read.ptx.sreg.tid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:1271
msgid "threadIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:1272
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:1272
msgid "blockIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:1273
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ntid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:1273
msgid "blockDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:1274
msgid "``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:1274
msgid "gridDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:1275
msgid "``void @llvm.nvvm.barrier0()``"
msgstr ""

#: ../../../NVPTXUsage.rst:1275
msgid "__syncthreads()"
msgstr ""

#: ../../../NVPTXUsage.rst:1282
msgid ""
"You may have noticed that all of the pointer types in the LLVM IR example "
"had an explicit address space specifier. What is address space 1? NVIDIA GPU "
"devices (generally) have four types of memory:"
msgstr ""

#: ../../../NVPTXUsage.rst:1286
msgid "Global: Large, off-chip memory"
msgstr ""

#: ../../../NVPTXUsage.rst:1287
msgid "Shared: Small, on-chip memory shared among all threads in a CTA"
msgstr ""

#: ../../../NVPTXUsage.rst:1288
msgid "Local: Per-thread, private memory"
msgstr ""

#: ../../../NVPTXUsage.rst:1289
msgid "Constant: Read-only memory shared across all threads"
msgstr ""

#: ../../../NVPTXUsage.rst:1291
msgid ""
"These different types of memory are represented in LLVM IR as address "
"spaces. There is also a fifth address space used by the NVPTX code generator "
"that corresponds to the \"generic\" address space.  This address space can "
"represent addresses in any other address space (with a few exceptions).  "
"This allows users to write IR functions that can load/store memory using the "
"same instructions. Intrinsics are provided to convert pointers between the "
"generic and non-generic address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:1299
msgid ""
"See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:1303
msgid "Kernel Metadata"
msgstr ""

#: ../../../NVPTXUsage.rst:1305
msgid ""
"In PTX, a function can be either a `kernel` function (callable from the host "
"program), or a `device` function (callable only from GPU code). You can "
"think of `kernel` functions as entry-points in the GPU program. To mark an "
"LLVM IR function as a `kernel` function, we make use of special LLVM "
"metadata. The NVPTX back-end will look for a named metadata node called "
"``nvvm.annotations``. This named metadata must contain a list of metadata "
"that describe the IR. For our purposes, we need to declare a metadata node "
"that assigns the \"kernel\" attribute to the LLVM IR function that should be "
"emitted as a PTX `kernel` function. These metadata nodes take the form:"
msgstr ""

#: ../../../NVPTXUsage.rst:1319
msgid "For the previous example, we have:"
msgstr ""

#: ../../../NVPTXUsage.rst:1326
msgid ""
"Here, we have a single metadata declaration in ``nvvm.annotations``. This "
"metadata annotates our ``@kernel`` function with the ``kernel`` attribute."
msgstr ""

#: ../../../NVPTXUsage.rst:1331
msgid "Running the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:1333
msgid ""
"Generating PTX from LLVM IR is all well and good, but how do we execute it "
"on a real GPU device? The CUDA Driver API provides a convenient mechanism "
"for loading and JIT compiling PTX to a native GPU device, and launching a "
"kernel. The API is similar to OpenCL.  A simple example showing how to load "
"and execute our vector addition code is shown below. Note that for brevity "
"this code does not perform much error checking!"
msgstr ""

#: ../../../NVPTXUsage.rst:1342
msgid ""
"You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline "
"compile PTX to machine code (SASS) for a specific GPU architecture. Such "
"binaries can be loaded by the CUDA Driver API in the same way as PTX. This "
"can be useful for reducing startup time by precompiling the PTX kernels."
msgstr ""

#: ../../../NVPTXUsage.rst:1471
msgid ""
"You will need to link with the CUDA driver and specify the path to cuda.h."
msgstr ""

#: ../../../NVPTXUsage.rst:1477
msgid ""
"We don't need to specify a path to ``libcuda.so`` since this is installed in "
"a system location by the driver, not the CUDA toolkit."
msgstr ""

#: ../../../NVPTXUsage.rst:1480
msgid ""
"If everything goes as planned, you should see the following output when "
"running the compiled program:"
msgstr ""

#: ../../../NVPTXUsage.rst:1508
msgid ""
"You will likely see a different device identifier based on your hardware"
msgstr ""

#: ../../../NVPTXUsage.rst:1512
msgid "Tutorial: Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:1514
msgid ""
"In this tutorial, we show a simple example of linking LLVM IR with the "
"libdevice library. We will use the same kernel as the previous tutorial, "
"except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``. "
"Libdevice provides an ``__nv_powf`` function that we will use."
msgstr ""

#: ../../../NVPTXUsage.rst:1558
msgid "To compile this kernel, we perform the following steps:"
msgstr ""

#: ../../../NVPTXUsage.rst:1560
msgid "Link with libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:1561
msgid "Internalize all but the public kernel function"
msgstr ""

#: ../../../NVPTXUsage.rst:1562
msgid "Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0"
msgstr ""

#: ../../../NVPTXUsage.rst:1563
msgid "Optimize the linked module"
msgstr ""

#: ../../../NVPTXUsage.rst:1564
msgid "Codegen the module"
msgstr ""

#: ../../../NVPTXUsage.rst:1567
msgid ""
"These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc`` "
"tools. In a complete compiler, these steps can also be performed entirely "
"programmatically by setting up an appropriate pass configuration (see :ref:"
"`libdevice`)."
msgstr ""

#: ../../../NVPTXUsage.rst:1580
msgid ""
"The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any "
"undefined variables will default to zero. It is shown here for evaluation "
"purposes."
msgstr ""

#: ../../../NVPTXUsage.rst:1585
msgid "This gives us the following PTX (excerpt):"
msgstr ""
