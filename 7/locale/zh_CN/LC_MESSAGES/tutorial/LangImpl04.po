# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-07 18:13+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../tutorial/LangImpl04.rst:3
msgid "Kaleidoscope: Adding JIT and Optimizer Support"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:9
msgid "Chapter 4 Introduction"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:11
msgid ""
"Welcome to Chapter 4 of the \"`Implementing a language with LLVM <index."
"html>`_\" tutorial. Chapters 1-3 described the implementation of a simple "
"language and added support for generating LLVM IR. This chapter describes "
"two new techniques: adding optimizer support to your language, and adding "
"JIT compiler support. These additions will demonstrate how to get nice, "
"efficient code for the Kaleidoscope language."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:20
msgid "Trivial Constant Folding"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:22
msgid ""
"Our demonstration for Chapter 3 is elegant and easy to extend. "
"Unfortunately, it does not produce wonderful code. The IRBuilder, however, "
"does give us obvious optimizations when compiling simple code:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:36
msgid ""
"This code is not a literal transcription of the AST built by parsing the "
"input. That would be:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:50
msgid ""
"Constant folding, as seen above, in particular, is a very common and very "
"important optimization: so much so that many language implementors implement "
"constant folding support in their AST representation."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:54
msgid ""
"With LLVM, you don't need this support in the AST. Since all calls to build "
"LLVM IR go through the LLVM IR builder, the builder itself checked to see if "
"there was a constant folding opportunity when you call it. If so, it just "
"does the constant fold and return the constant instead of creating an "
"instruction."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:60
msgid ""
"Well, that was easy :). In practice, we recommend always using ``IRBuilder`` "
"when generating code like this. It has no \"syntactic overhead\" for its use "
"(you don't have to uglify your compiler with constant checks everywhere) and "
"it can dramatically reduce the amount of LLVM IR that is generated in some "
"cases (particular for languages with a macro preprocessor or that use a lot "
"of constants)."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:67
msgid ""
"On the other hand, the ``IRBuilder`` is limited by the fact that it does all "
"of its analysis inline with the code as it is built. If you take a slightly "
"more complex example:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:83
msgid ""
"In this case, the LHS and RHS of the multiplication are the same value. We'd "
"really like to see this generate \"``tmp = x+3; result = tmp*tmp;``\" "
"instead of computing \"``x+3``\" twice."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:87
msgid ""
"Unfortunately, no amount of local analysis will be able to detect and "
"correct this. This requires two transformations: reassociation of "
"expressions (to make the add's lexically identical) and Common Subexpression "
"Elimination (CSE) to delete the redundant add instruction. Fortunately, LLVM "
"provides a broad range of optimizations that you can use, in the form of "
"\"passes\"."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:95
msgid "LLVM Optimization Passes"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:97
msgid ""
"LLVM provides many optimization passes, which do many different sorts of "
"things and have different tradeoffs. Unlike other systems, LLVM doesn't hold "
"to the mistaken notion that one set of optimizations is right for all "
"languages and for all situations. LLVM allows a compiler implementor to make "
"complete decisions about what optimizations to use, in which order, and in "
"what situation."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:104
msgid ""
"As a concrete example, LLVM supports both \"whole module\" passes, which "
"look across as large of body of code as they can (often a whole file, but if "
"run at link time, this can be a substantial portion of the whole program). "
"It also supports and includes \"per-function\" passes which just operate on "
"a single function at a time, without looking at other functions. For more "
"information on passes and how they are run, see the `How to Write a Pass <../"
"WritingAnLLVMPass.html>`_ document and the `List of LLVM Passes <../Passes."
"html>`_."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:113
msgid ""
"For Kaleidoscope, we are currently generating functions on the fly, one at a "
"time, as the user types them in. We aren't shooting for the ultimate "
"optimization experience in this setting, but we also want to catch the easy "
"and quick stuff where possible. As such, we will choose to run a few per-"
"function optimizations as the user types the function in. If we wanted to "
"make a \"static Kaleidoscope compiler\", we would use exactly the code we "
"have now, except that we would defer running the optimizer until the entire "
"file has been parsed."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:122
msgid ""
"In order to get per-function optimizations going, we need to set up a "
"`FunctionPassManager <../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to "
"hold and organize the LLVM optimizations that we want to run. Once we have "
"that, we can add a set of optimizations to run. We'll need a new "
"FunctionPassManager for each module that we want to optimize, so we'll write "
"a function to create and initialize both the module and pass manager for us:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:151
msgid ""
"This code initializes the global module ``TheModule``, and the function pass "
"manager ``TheFPM``, which is attached to ``TheModule``. Once the pass "
"manager is set up, we use a series of \"add\" calls to add a bunch of LLVM "
"passes."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:155
msgid ""
"In this case, we choose to add four optimization passes. The passes we "
"choose here are a pretty standard set of \"cleanup\" optimizations that are "
"useful for a wide variety of code. I won't delve into what they do but, "
"believe me, they are a good starting place :)."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:160
msgid ""
"Once the PassManager is set up, we need to make use of it. We do this by "
"running it after our newly created function is constructed (in "
"``FunctionAST::codegen()``), but before it is returned to the client:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:179
msgid ""
"As you can see, this is pretty straightforward. The ``FunctionPassManager`` "
"optimizes and updates the LLVM Function\\* in place, improving (hopefully) "
"its body. With this in place, we can try our test above again:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:195
msgid ""
"As expected, we now get our nicely optimized code, saving a floating point "
"add instruction from every execution of this function."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:198
msgid ""
"LLVM provides a wide variety of optimizations that can be used in certain "
"circumstances. Some `documentation about the various passes <../Passes."
"html>`_ is available, but it isn't very complete. Another good source of "
"ideas can come from looking at the passes that ``Clang`` runs to get "
"started. The \"``opt``\" tool allows you to experiment with passes from the "
"command line, so you can see if they do anything."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:206
msgid ""
"Now that we have reasonable code coming out of our front-end, let's talk "
"about executing it!"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:210
msgid "Adding a JIT Compiler"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:212
msgid ""
"Code that is available in LLVM IR can have a wide variety of tools applied "
"to it. For example, you can run optimizations on it (as we did above), you "
"can dump it out in textual or binary forms, you can compile the code to an "
"assembly file (.s) for some target, or you can JIT compile it. The nice "
"thing about the LLVM IR representation is that it is the \"common currency\" "
"between many different parts of the compiler."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:219
msgid ""
"In this section, we'll add JIT compiler support to our interpreter. The "
"basic idea that we want for Kaleidoscope is to have the user enter function "
"bodies as they do now, but immediately evaluate the top-level expressions "
"they type in. For example, if they type in \"1 + 2;\", we should evaluate "
"and print out 3. If they define a function, they should be able to call it "
"from the command line."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:226
msgid ""
"In order to do this, we first prepare the environment to create code for the "
"current native target and declare and initialize the JIT. This is done by "
"calling some ``InitializeNativeTarget\\*`` functions and adding a global "
"variable ``TheJIT``, and initializing it in ``main``:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:260
msgid "We also need to setup the data layout for the JIT:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:273
msgid ""
"The KaleidoscopeJIT class is a simple JIT built specifically for these "
"tutorials, available inside the LLVM source code at llvm-src/examples/"
"Kaleidoscope/include/KaleidoscopeJIT.h. In later chapters we will look at "
"how it works and extend it with new features, but for now we will take it as "
"given. Its API is very simple: ``addModule`` adds an LLVM IR module to the "
"JIT, making its functions available for execution; ``removeModule`` removes "
"a module, freeing any memory associated with the code in that module; and "
"``findSymbol`` allows us to look up pointers to the compiled code."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:283
msgid ""
"We can take this simple API and change our code that parses top-level "
"expressions to look like this:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:311
msgid ""
"If parsing and codegen succeeed, the next step is to add the module "
"containing the top-level expression to the JIT. We do this by calling "
"addModule, which triggers code generation for all the functions in the "
"module, and returns a handle that can be used to remove the module from the "
"JIT later. Once the module has been added to the JIT it can no longer be "
"modified, so we also open a new module to hold subsequent code by calling "
"``InitializeModuleAndPassManager()``."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:318
msgid ""
"Once we've added the module to the JIT we need to get a pointer to the final "
"generated code. We do this by calling the JIT's findSymbol method, and "
"passing the name of the top-level expression function: ``__anon_expr``. "
"Since we just added this function, we assert that findSymbol returned a "
"result."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:323
msgid ""
"Next, we get the in-memory address of the ``__anon_expr`` function by "
"calling ``getAddress()`` on the symbol. Recall that we compile top-level "
"expressions into a self-contained LLVM function that takes no arguments and "
"returns the computed double. Because the LLVM JIT compiler matches the "
"native platform ABI, this means that you can just cast the result pointer to "
"a function pointer of that type and call it directly. This means, there is "
"no difference between JIT compiled code and native machine code that is "
"statically linked into your application."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:332
msgid ""
"Finally, since we don't support re-evaluation of top-level expressions, we "
"remove the module from the JIT when we're done to free the associated "
"memory. Recall, however, that the module we created a few lines earlier (via "
"``InitializeModuleAndPassManager``) is still open and waiting for new code "
"to be added."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:338
msgid "With just these two changes, let's see how Kaleidoscope works now!"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:351
msgid ""
"Well this looks like it is basically working. The dump of the function shows "
"the \"no argument function that always returns double\" that we synthesize "
"for each top-level expression that is typed in. This demonstrates very basic "
"functionality, but can we do more?"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:381
msgid ""
"Function definitions and calls also work, but something went very wrong on "
"that last line. The call looks valid, so what happened? As you may have "
"guessed from the API a Module is a unit of allocation for the JIT, and "
"testfunc was part of the same module that contained anonymous expression. "
"When we removed that module from the JIT to free the memory for the "
"anonymous expression, we deleted the definition of ``testfunc`` along with "
"it. Then, when we tried to call testfunc a second time, the JIT could no "
"longer find it."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:389
msgid ""
"The easiest way to fix this is to put the anonymous expression in a separate "
"module from the rest of the function definitions. The JIT will happily "
"resolve function calls across module boundaries, as long as each of the "
"functions called has a prototype, and is added to the JIT before it is "
"called. By putting the anonymous expression in a different module we can "
"delete it without affecting the rest of the functions."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:396
msgid ""
"In fact, we're going to go a step further and put every function in its own "
"module. Doing so allows us to exploit a useful property of the "
"KaleidoscopeJIT that will make our environment more REPL-like: Functions can "
"be added to the JIT more than once (unlike a module where every function "
"must have a unique definition). When you look up a symbol in KaleidoscopeJIT "
"it will always return the most recent definition:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:427
msgid ""
"To allow each function to live in its own module we'll need a way to re-"
"generate previous function declarations into each new module we open:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:469
msgid ""
"To enable this, we'll start by adding a new global, ``FunctionProtos``, that "
"holds the most recent prototype for each function. We'll also add a "
"convenience method, ``getFunction()``, to replace calls to ``TheModule-"
">getFunction()``. Our convenience method searches ``TheModule`` for an "
"existing function declaration, falling back to generating a new declaration "
"from FunctionProtos if it doesn't find one. In ``CallExprAST::codegen()`` we "
"just need to replace the call to ``TheModule->getFunction()``. In "
"``FunctionAST::codegen()`` we need to update the FunctionProtos map first, "
"then call ``getFunction()``. With this done, we can always obtain a function "
"declaration in the current module for any previously declared function."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:480
msgid "We also need to update HandleDefinition and HandleExtern:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:513
msgid ""
"In HandleDefinition, we add two lines to transfer the newly defined function "
"to the JIT and open a new module. In HandleExtern, we just need to add one "
"line to add the prototype to FunctionProtos."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:517
msgid ""
"With these changes made, let's try our REPL again (I removed the dump of the "
"anonymous functions this time, you should get the idea by now :) :"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:530
msgid "It works!"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:532
msgid ""
"Even with this simple code, we get some surprisingly powerful capabilities - "
"check this out:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:576
msgid ""
"Whoa, how does the JIT know about sin and cos? The answer is surprisingly "
"simple: The KaleidoscopeJIT has a straightforward symbol resolution rule "
"that it uses to find symbols that aren't available in any given module: "
"First it searches all the modules that have already been added to the JIT, "
"from the most recent to the oldest, to find the newest definition. If no "
"definition is found inside the JIT, it falls back to calling "
"\"``dlsym(\"sin\")``\" on the Kaleidoscope process itself. Since \"``sin``\" "
"is defined within the JIT's address space, it simply patches up calls in the "
"module to call the libm version of ``sin`` directly. But in some cases this "
"even goes further: as sin and cos are names of standard math functions, the "
"constant folder will directly evaluate the function calls to the correct "
"result when called with constants like in the \"``sin(1.0)``\" above."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:589
msgid ""
"In the future we'll see how tweaking this symbol resolution rule can be used "
"to enable all sorts of useful features, from security (restricting the set "
"of symbols available to JIT'd code), to dynamic code generation based on "
"symbol names, and even lazy compilation."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:594
msgid ""
"One immediate benefit of the symbol resolution rule is that we can now "
"extend the language by writing arbitrary C++ code to implement operations. "
"For example, if we add:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:612
msgid ""
"Note, that for Windows we need to actually export the functions because the "
"dynamic symbol loader will use GetProcAddress to find the symbols."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:615
msgid ""
"Now we can produce simple output to the console by using things like: "
"\"``extern putchard(x); putchard(120);``\", which prints a lowercase 'x' on "
"the console (120 is the ASCII code for 'x'). Similar code could be used to "
"implement file I/O, console input, and many other capabilities in "
"Kaleidoscope."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:621
msgid ""
"This completes the JIT and optimizer chapter of the Kaleidoscope tutorial. "
"At this point, we can compile a non-Turing-complete programming language, "
"optimize and JIT compile it in a user-driven way. Next up we'll look into "
"`extending the language with control flow constructs <LangImpl05.html>`_, "
"tackling some interesting LLVM IR issues along the way."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:629
msgid "Full Code Listing"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:631
msgid ""
"Here is the complete code listing for our running example, enhanced with the "
"LLVM JIT and optimizer. To build this example, use:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:641
msgid ""
"If you are compiling this on Linux, make sure to add the \"-rdynamic\" "
"option as well. This makes sure that the external functions are resolved "
"properly at runtime."
msgstr ""

#: ../../../tutorial/LangImpl04.rst:645
msgid "Here is the code:"
msgstr ""

#: ../../../tutorial/LangImpl04.rst:650
msgid "`Next: Extending the language: control flow <LangImpl05.html>`_"
msgstr ""
