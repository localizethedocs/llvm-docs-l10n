# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM main\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-13 08:36+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../NVPTXUsage.rst:3
msgid "User Guide for NVPTX Back-end"
msgstr ""

#: ../../../NVPTXUsage.rst:11
msgid "Introduction"
msgstr ""

#: ../../../NVPTXUsage.rst:13
msgid ""
"To support GPU programming, the NVPTX back-end supports a subset of LLVM IR "
"along with a defined set of conventions used to represent GPU programming "
"concepts. This document provides an overview of the general usage of the "
"back- end, including a description of the conventions used and the set of "
"accepted LLVM IR."
msgstr ""

#: ../../../NVPTXUsage.rst:21
msgid ""
"This document assumes a basic familiarity with CUDA and the PTX assembly "
"language. Information about the CUDA Driver API and the PTX assembly "
"language can be found in the `CUDA documentation <http://docs.nvidia.com/"
"cuda/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:29
msgid "Conventions"
msgstr ""

#: ../../../NVPTXUsage.rst:32
msgid "Marking Functions as Kernels"
msgstr ""

#: ../../../NVPTXUsage.rst:34
msgid ""
"In PTX, there are two types of functions: *device functions*, which are only "
"callable by device code, and *kernel functions*, which are callable by host "
"code. By default, the back-end will emit device functions. The "
"``ptx_kernel`` calling convention is used to declare a function as a kernel "
"function."
msgstr ""

#: ../../../NVPTXUsage.rst:39
msgid ""
"The following example shows a kernel function calling a device function in "
"LLVM IR. The function ``@my_kernel`` is callable from host code, but "
"``@my_fmad`` is not."
msgstr ""

#: ../../../NVPTXUsage.rst:58
msgid "When compiled, the PTX kernel functions are callable by host-side code."
msgstr ""

#: ../../../NVPTXUsage.rst:62
msgid "Parameter Attributes"
msgstr ""

#: ../../../NVPTXUsage.rst:71
msgid "``\"nvvm.grid_constant\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:65
msgid ""
"This attribute may be attached to a ``byval`` parameter of a kernel function "
"to indicate that the parameter should be lowered as a direct reference to "
"the grid-constant memory of the parameter, as opposed to a copy of the "
"parameter in local memory. Writing to a grid-constant parameter is undefined "
"behavior. Unlike a normal ``byval`` parameter, the address of a grid-"
"constant parameter is not unique to a given function invocation but instead "
"is shared by all kernels in the grid."
msgstr ""

#: ../../../NVPTXUsage.rst:76
msgid "Function Attributes"
msgstr ""

#: ../../../NVPTXUsage.rst:80
msgid "``\"nvvm.maxclusterrank\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:79
msgid ""
"This attribute specifies the maximum number of blocks per cluster. Must be "
"non-zero. Only supported for Hopper+."
msgstr ""

#: ../../../NVPTXUsage.rst:84
msgid "``\"nvvm.minctasm\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:83
msgid ""
"This indicates a hint/directive to the compiler/driver, asking it to put at "
"least these many CTAs on an SM."
msgstr ""

#: ../../../NVPTXUsage.rst:88
msgid "``\"nvvm.maxnreg\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:87
msgid ""
"This attribute indicates the maximum number of registers to be used for the "
"kernel function."
msgstr ""

#: ../../../NVPTXUsage.rst:94
msgid "``\"nvvm.maxntid\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:91
msgid ""
"This attribute declares the maximum number of threads in the thread block "
"(CTA). The maximum number of threads is the product of the maximum extent in "
"each dimension. Exceeding the maximum number of threads results in a runtime "
"error or kernel launch failure."
msgstr ""

#: ../../../NVPTXUsage.rst:100
msgid "``\"nvvm.reqntid\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:97
msgid ""
"This attribute declares the exact number of threads in the thread block "
"(CTA). The number of threads is the product of the value in each dimension. "
"Specifying a different CTA dimension at launch will result in a runtime "
"error or kernel launch failure."
msgstr ""

#: ../../../NVPTXUsage.rst:106
msgid "``\"nvvm.cluster_dim\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:103
msgid ""
"This attribute declares the number of thread blocks (CTAs) in the cluster. "
"The total number of CTAs is the product of the number of CTAs in each "
"dimension. Specifying a different cluster dimension at launch will result in "
"a runtime error or kernel launch failure. Only supported for Hopper+."
msgstr ""

#: ../../../NVPTXUsage.rst:112
msgid "``\"nvvm.blocksareclusters\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:109
msgid ""
"This attribute implies that the grid launch configuration for the "
"corresponding kernel function is specifying the number of clusters instead "
"of the number of thread blocks. This attribute is only allowed for kernel "
"functions and requires ``nvvm.reqntid`` and ``nvvm.cluster_dim`` attributes."
msgstr ""

#: ../../../NVPTXUsage.rst:117 ../../../NVPTXUsage.rst:2891
msgid "Address Spaces"
msgstr ""

#: ../../../NVPTXUsage.rst:119
msgid "The NVPTX back-end uses the following address space mapping:"
msgstr ""

#: ../../../NVPTXUsage.rst:122
msgid "Address Space"
msgstr ""

#: ../../../NVPTXUsage.rst:122
msgid "Memory Space"
msgstr ""

#: ../../../NVPTXUsage.rst:124 ../../../NVPTXUsage.rst:805
#: ../../../NVPTXUsage.rst:813 ../../../NVPTXUsage.rst:821
#: ../../../NVPTXUsage.rst:829 ../../../NVPTXUsage.rst:837
#: ../../../NVPTXUsage.rst:845 ../../../NVPTXUsage.rst:2179
#: ../../../NVPTXUsage.rst:2199 ../../../NVPTXUsage.rst:2272
#: ../../../NVPTXUsage.rst:2333 ../../../NVPTXUsage.rst:2353
#: ../../../NVPTXUsage.rst:2404 ../../../NVPTXUsage.rst:2415
#: ../../../NVPTXUsage.rst:2426
msgid "0"
msgstr ""

#: ../../../NVPTXUsage.rst:124
msgid "Generic"
msgstr ""

#: ../../../NVPTXUsage.rst:125 ../../../NVPTXUsage.rst:807
#: ../../../NVPTXUsage.rst:815 ../../../NVPTXUsage.rst:823
#: ../../../NVPTXUsage.rst:831 ../../../NVPTXUsage.rst:839
#: ../../../NVPTXUsage.rst:847 ../../../NVPTXUsage.rst:1917
#: ../../../NVPTXUsage.rst:2180 ../../../NVPTXUsage.rst:2190
#: ../../../NVPTXUsage.rst:2200 ../../../NVPTXUsage.rst:2263
#: ../../../NVPTXUsage.rst:2273 ../../../NVPTXUsage.rst:2334
#: ../../../NVPTXUsage.rst:2344 ../../../NVPTXUsage.rst:2354
#: ../../../NVPTXUsage.rst:2405 ../../../NVPTXUsage.rst:2416
#: ../../../NVPTXUsage.rst:2427
msgid "1"
msgstr ""

#: ../../../NVPTXUsage.rst:125
msgid "Global"
msgstr ""

#: ../../../NVPTXUsage.rst:126 ../../../NVPTXUsage.rst:809
#: ../../../NVPTXUsage.rst:817 ../../../NVPTXUsage.rst:825
#: ../../../NVPTXUsage.rst:833 ../../../NVPTXUsage.rst:841
#: ../../../NVPTXUsage.rst:849 ../../../NVPTXUsage.rst:1917
#: ../../../NVPTXUsage.rst:1918 ../../../NVPTXUsage.rst:2181
#: ../../../NVPTXUsage.rst:2191 ../../../NVPTXUsage.rst:2201
#: ../../../NVPTXUsage.rst:2264 ../../../NVPTXUsage.rst:2274
#: ../../../NVPTXUsage.rst:2335 ../../../NVPTXUsage.rst:2345
#: ../../../NVPTXUsage.rst:2355 ../../../NVPTXUsage.rst:2406
#: ../../../NVPTXUsage.rst:2417 ../../../NVPTXUsage.rst:2428
msgid "2"
msgstr ""

#: ../../../NVPTXUsage.rst:126
msgid "Internal Use"
msgstr ""

#: ../../../NVPTXUsage.rst:127 ../../../NVPTXUsage.rst:811
#: ../../../NVPTXUsage.rst:819 ../../../NVPTXUsage.rst:827
#: ../../../NVPTXUsage.rst:835 ../../../NVPTXUsage.rst:843
#: ../../../NVPTXUsage.rst:851 ../../../NVPTXUsage.rst:2182
#: ../../../NVPTXUsage.rst:2202 ../../../NVPTXUsage.rst:2275
#: ../../../NVPTXUsage.rst:2336 ../../../NVPTXUsage.rst:2356
#: ../../../NVPTXUsage.rst:2407 ../../../NVPTXUsage.rst:2418
#: ../../../NVPTXUsage.rst:2429
msgid "3"
msgstr ""

#: ../../../NVPTXUsage.rst:127
msgid "Shared"
msgstr ""

#: ../../../NVPTXUsage.rst:128 ../../../NVPTXUsage.rst:1917
#: ../../../NVPTXUsage.rst:1918 ../../../NVPTXUsage.rst:1919
msgid "4"
msgstr ""

#: ../../../NVPTXUsage.rst:128
msgid "Constant"
msgstr ""

#: ../../../NVPTXUsage.rst:129
msgid "5"
msgstr ""

#: ../../../NVPTXUsage.rst:129
msgid "Local"
msgstr ""

#: ../../../NVPTXUsage.rst:130
msgid "7"
msgstr ""

#: ../../../NVPTXUsage.rst:130
msgid "Shared Cluster"
msgstr ""

#: ../../../NVPTXUsage.rst:133
msgid ""
"Every global variable and pointer type is assigned to one of these address "
"spaces, with 0 being the default address space. Intrinsics are provided "
"which can be used to convert pointers between the generic and non-generic "
"address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:138
msgid ""
"As an example, the following IR will define an array ``@g`` that resides in "
"global device memory."
msgstr ""

#: ../../../NVPTXUsage.rst:145
msgid ""
"LLVM IR functions can read and write to this array, and host-side code can "
"copy data to it by name with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:148
msgid ""
"Note that since address space 0 is the generic space, it is illegal to have "
"global variables in address space 0.  Address space 0 is the default address "
"space in LLVM, so the ``addrspace(N)`` annotation is *required* for global "
"variables."
msgstr ""

#: ../../../NVPTXUsage.rst:155
msgid "Triples"
msgstr ""

#: ../../../NVPTXUsage.rst:157
msgid ""
"The NVPTX target uses the module triple to select between 32/64-bit code "
"generation and the driver-compiler interface to use. The triple architecture "
"can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The "
"operating system should be one of ``cuda`` or ``nvcl``, which determines the "
"interface used by the generated code to communicate with the driver.  Most "
"users will want to use ``cuda`` as the operating system, which makes the "
"generated PTX compatible with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:165
msgid "Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:167
msgid "Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:172
msgid "NVPTX Architecture Hierarchy and Ordering"
msgstr ""

#: ../../../NVPTXUsage.rst:174
msgid ""
"GPU architectures: sm_2Y/sm_3Y/sm_5Y/sm_6Y/sm_7Y/sm_8Y/sm_9Y/sm_10Y/sm_12Y "
"('Y' represents version within the architecture) The architectures have name "
"of form ``sm_XYz`` where ``X`` represent the generation number, ``Y`` "
"represents the version within the architecture, and ``z`` represents the "
"optional feature suffix. If ``X1Y1 <= X2Y2``, then GPU capabilities of "
"``sm_X1Y1`` are included in ``sm_X2Y2``. For example, take ``sm_90`` (9 "
"represents ``X``, 0 represents ``Y``, and no feature suffix) and ``sm_103`` "
"architectures (10 represents ``X``, 3 represents ``Y``, and no feature "
"suffix). Since 90 <= 103, ``sm_90`` is compatible with ``sm_103``."
msgstr ""

#: ../../../NVPTXUsage.rst:184
msgid ""
"The family-specific variants have ``f`` feature suffix and they follow "
"following order: ``sm_X{Y2}f > sm_X{Y1}f`` iff ``Y2 > Y1`` ``sm_XY{f} > "
"sm_{XY}{}``"
msgstr ""

#: ../../../NVPTXUsage.rst:189
msgid ""
"For example, take ``sm_100f`` (10 represents ``X``, 0 represents ``Y``, and "
"``f`` represents ``z``) and ``sm_103f`` (10 represents ``X``, 3 represents "
"``Y``, and ``f`` represents ``z``) architecture variants. Since ``Y1 < Y2``, "
"``sm_100f`` is compatible with ``sm_103f``. Similarly based on the second "
"rule, ``sm_90`` is compatible with ``sm_103f``."
msgstr ""

#: ../../../NVPTXUsage.rst:194
msgid ""
"Some counter examples, take ``sm_100f`` and ``sm_120f`` (12 represents "
"``X``, 0 represents ``Y``, and ``f`` represents ``z``) architecture "
"variants. Since both belongs to different family i.e. ``X1 != X2``, "
"``sm_100f`` is not compatible with ``sm_120f``."
msgstr ""

#: ../../../NVPTXUsage.rst:199
msgid ""
"The architecture-specific variants have ``a`` feature suffix and they follow "
"following order: ``sm_XY{a} > sm_XY{f} > sm_{XY}{}``"
msgstr ""

#: ../../../NVPTXUsage.rst:203
msgid ""
"For example, take ``sm_103a`` (10 represents ``X``, 3 represents ``Y``, and "
"``a`` represents ``z``), ``sm_103f``, and ``sm_103`` architecture variants. "
"The ``sm_103`` is compatible with ``sm_103a`` and ``sm_103f``, and "
"``sm_103f`` is compatible with ``sm_103a``."
msgstr ""

#: ../../../NVPTXUsage.rst:207
msgid "Encoding := Arch * 10 + 2 (for 'f') + 1 (for 'a') Arch := X * 10 + Y"
msgstr ""

#: ../../../NVPTXUsage.rst:210
msgid ""
"For example, ``sm_103f`` is encoded as 1032 (103 * 10 + 2) and ``sm_103a`` "
"is encoded as 1033 (103 * 10 + 2 + 1)."
msgstr ""

#: ../../../NVPTXUsage.rst:213
msgid "This encoding allows simple partial ordering of the architectures."
msgstr ""

#: ../../../NVPTXUsage.rst:215
msgid ""
"Compare Family and Arch by dividing FullSMVersion by 100 and 10 respectively "
"before the comparison."
msgstr ""

#: ../../../NVPTXUsage.rst:217
msgid ""
"Compare within the family by comparing FullSMVersion, given both belongs to "
"the same family."
msgstr ""

#: ../../../NVPTXUsage.rst:219
msgid "Detect ``a`` variants by checking FullSMVersion & 1."
msgstr ""

#: ../../../NVPTXUsage.rst:224
msgid "NVPTX Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:227
msgid "Reading PTX Special Registers"
msgstr ""

#: ../../../NVPTXUsage.rst:230
msgid "'``llvm.nvvm.read.ptx.sreg.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:233 ../../../NVPTXUsage.rst:275
#: ../../../NVPTXUsage.rst:332 ../../../NVPTXUsage.rst:359
#: ../../../NVPTXUsage.rst:394 ../../../NVPTXUsage.rst:421
#: ../../../NVPTXUsage.rst:450 ../../../NVPTXUsage.rst:477
#: ../../../NVPTXUsage.rst:503 ../../../NVPTXUsage.rst:538
#: ../../../NVPTXUsage.rst:572 ../../../NVPTXUsage.rst:600
#: ../../../NVPTXUsage.rst:628 ../../../NVPTXUsage.rst:654
#: ../../../NVPTXUsage.rst:682 ../../../NVPTXUsage.rst:712
#: ../../../NVPTXUsage.rst:738 ../../../NVPTXUsage.rst:772
#: ../../../NVPTXUsage.rst:861 ../../../NVPTXUsage.rst:897
#: ../../../NVPTXUsage.rst:929 ../../../NVPTXUsage.rst:953
#: ../../../NVPTXUsage.rst:981 ../../../NVPTXUsage.rst:1028
#: ../../../NVPTXUsage.rst:1052 ../../../NVPTXUsage.rst:1091
#: ../../../NVPTXUsage.rst:1149 ../../../NVPTXUsage.rst:1193
#: ../../../NVPTXUsage.rst:1236 ../../../NVPTXUsage.rst:1283
#: ../../../NVPTXUsage.rst:1324 ../../../NVPTXUsage.rst:1352
#: ../../../NVPTXUsage.rst:1395 ../../../NVPTXUsage.rst:1440
#: ../../../NVPTXUsage.rst:1483 ../../../NVPTXUsage.rst:1513
#: ../../../NVPTXUsage.rst:1544 ../../../NVPTXUsage.rst:1571
#: ../../../NVPTXUsage.rst:1600 ../../../NVPTXUsage.rst:1640
#: ../../../NVPTXUsage.rst:1670 ../../../NVPTXUsage.rst:1695
#: ../../../NVPTXUsage.rst:1721 ../../../NVPTXUsage.rst:1748
#: ../../../NVPTXUsage.rst:1773 ../../../NVPTXUsage.rst:1795
#: ../../../NVPTXUsage.rst:1819 ../../../NVPTXUsage.rst:1883
#: ../../../NVPTXUsage.rst:1937 ../../../NVPTXUsage.rst:2123
#: ../../../NVPTXUsage.rst:2209 ../../../NVPTXUsage.rst:2282
#: ../../../NVPTXUsage.rst:2364 ../../../NVPTXUsage.rst:2439
#: ../../../NVPTXUsage.rst:2472 ../../../NVPTXUsage.rst:2505
#: ../../../NVPTXUsage.rst:2532 ../../../NVPTXUsage.rst:2563
msgid "Syntax:"
msgstr ""

#: ../../../NVPTXUsage.rst:252 ../../../NVPTXUsage.rst:288
#: ../../../NVPTXUsage.rst:339 ../../../NVPTXUsage.rst:374
#: ../../../NVPTXUsage.rst:405 ../../../NVPTXUsage.rst:429
#: ../../../NVPTXUsage.rst:462 ../../../NVPTXUsage.rst:486
#: ../../../NVPTXUsage.rst:514 ../../../NVPTXUsage.rst:548
#: ../../../NVPTXUsage.rst:579 ../../../NVPTXUsage.rst:607
#: ../../../NVPTXUsage.rst:636 ../../../NVPTXUsage.rst:662
#: ../../../NVPTXUsage.rst:692 ../../../NVPTXUsage.rst:720
#: ../../../NVPTXUsage.rst:745 ../../../NVPTXUsage.rst:785
#: ../../../NVPTXUsage.rst:868 ../../../NVPTXUsage.rst:905
#: ../../../NVPTXUsage.rst:936 ../../../NVPTXUsage.rst:960
#: ../../../NVPTXUsage.rst:1003 ../../../NVPTXUsage.rst:1036
#: ../../../NVPTXUsage.rst:1060 ../../../NVPTXUsage.rst:1104
#: ../../../NVPTXUsage.rst:1166 ../../../NVPTXUsage.rst:1206
#: ../../../NVPTXUsage.rst:1253 ../../../NVPTXUsage.rst:1296
#: ../../../NVPTXUsage.rst:1333 ../../../NVPTXUsage.rst:1365
#: ../../../NVPTXUsage.rst:1412 ../../../NVPTXUsage.rst:1459
#: ../../../NVPTXUsage.rst:1492 ../../../NVPTXUsage.rst:1520
#: ../../../NVPTXUsage.rst:1551 ../../../NVPTXUsage.rst:1578
#: ../../../NVPTXUsage.rst:1608 ../../../NVPTXUsage.rst:1650
#: ../../../NVPTXUsage.rst:1678 ../../../NVPTXUsage.rst:1703
#: ../../../NVPTXUsage.rst:1731 ../../../NVPTXUsage.rst:1756
#: ../../../NVPTXUsage.rst:1781 ../../../NVPTXUsage.rst:1803
#: ../../../NVPTXUsage.rst:1845 ../../../NVPTXUsage.rst:1892
#: ../../../NVPTXUsage.rst:1946 ../../../NVPTXUsage.rst:2143
#: ../../../NVPTXUsage.rst:2244 ../../../NVPTXUsage.rst:2310
#: ../../../NVPTXUsage.rst:2382 ../../../NVPTXUsage.rst:2447
#: ../../../NVPTXUsage.rst:2480 ../../../NVPTXUsage.rst:2512
#: ../../../NVPTXUsage.rst:2541 ../../../NVPTXUsage.rst:2570
msgid "Overview:"
msgstr ""

#: ../../../NVPTXUsage.rst:254
msgid ""
"The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX "
"special registers, in particular the kernel launch bounds.  These registers "
"map in the following way to CUDA builtins:"
msgstr ""

#: ../../../NVPTXUsage.rst:259
msgid "CUDA Builtin"
msgstr ""

#: ../../../NVPTXUsage.rst:259
msgid "PTX Special Register Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:261
msgid "``threadId``"
msgstr ""

#: ../../../NVPTXUsage.rst:261
msgid "``@llvm.nvvm.read.ptx.sreg.tid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:262
msgid "``blockIdx``"
msgstr ""

#: ../../../NVPTXUsage.rst:262
msgid "``@llvm.nvvm.read.ptx.sreg.ctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:263
msgid "``blockDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:263
msgid "``@llvm.nvvm.read.ptx.sreg.ntid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:264
msgid "``gridDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:264
msgid "``@llvm.nvvm.read.ptx.sreg.nctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:269
msgid "Barriers"
msgstr ""

#: ../../../NVPTXUsage.rst:272
msgid "'``llvm.nvvm.barrier.cta.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:290
msgid ""
"The '``@llvm.nvvm.barrier.cta.*``' family of intrinsics perform barrier "
"synchronization and communication within a CTA. They can be used by the "
"threads within the CTA for synchronization and communication."
msgstr ""

#: ../../../NVPTXUsage.rst:295 ../../../NVPTXUsage.rst:411
#: ../../../NVPTXUsage.rst:437 ../../../NVPTXUsage.rst:467
#: ../../../NVPTXUsage.rst:492 ../../../NVPTXUsage.rst:521
#: ../../../NVPTXUsage.rst:555 ../../../NVPTXUsage.rst:587
#: ../../../NVPTXUsage.rst:615 ../../../NVPTXUsage.rst:642
#: ../../../NVPTXUsage.rst:669 ../../../NVPTXUsage.rst:699
#: ../../../NVPTXUsage.rst:726 ../../../NVPTXUsage.rst:751
#: ../../../NVPTXUsage.rst:792
msgid "Semantics:"
msgstr ""

#: ../../../NVPTXUsage.rst:297
msgid ""
"Operand %id specifies a logical barrier resource and must fall within the "
"range 0 through 15. When present, operand %n specifies the number of threads "
"participating in the barrier. When specifying a thread count, the value must "
"be a multiple of the warp size. With the '``@llvm.nvvm.barrier.cta.sync.*``' "
"variants, the '``.all``' suffix indicates that all threads in the CTA should "
"participate in the barrier while the '``.count``' suffix indicates that only "
"the threads specified by the %n operand should participate in the barrier."
msgstr ""

#: ../../../NVPTXUsage.rst:305
msgid ""
"All forms of the '``@llvm.nvvm.barrier.cta.*``' intrinsic cause the "
"executing thread to wait for all non-exited threads from its warp and then "
"marks the warp's arrival at the barrier. In addition to signaling its "
"arrival at the barrier, the '``@llvm.nvvm.barrier.cta.sync.*``' intrinsics "
"cause the executing thread to wait for non-exited threads of all other warps "
"participating in the barrier to arrive. On the other hand, the '``@llvm.nvvm."
"barrier.cta.arrive.*``' intrinsic does not cause the executing thread to "
"wait for threads of other participating warps."
msgstr ""

#: ../../../NVPTXUsage.rst:314
msgid ""
"When a barrier completes, the waiting threads are restarted without delay, "
"and the barrier is reinitialized so that it can be immediately reused."
msgstr ""

#: ../../../NVPTXUsage.rst:317
msgid ""
"The '``@llvm.nvvm.barrier.cta.*``' intrinsic has an optional '``.aligned``' "
"modifier to indicate textual alignment of the barrier. When specified, it "
"indicates that all threads in the CTA will execute the same '``@llvm.nvvm."
"barrier.cta.*``' instruction. In conditionally executed code, an aligned "
"'``@llvm.nvvm.barrier.cta.*``' instruction should only be used if it is "
"known that all threads in the CTA evaluate the condition identically, "
"otherwise behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:326
msgid "Electing a thread"
msgstr ""

#: ../../../NVPTXUsage.rst:329
msgid "'``llvm.nvvm.elect.sync``'"
msgstr ""

#: ../../../NVPTXUsage.rst:341
msgid ""
"The '``@llvm.nvvm.elect.sync``' intrinsic generates the ``elect.sync`` PTX "
"instruction, which elects one predicated active leader thread from a set of "
"threads specified by ``membermask``. The behavior is undefined if the "
"executing thread is not in ``membermask``. The laneid of the elected thread "
"is captured in the i32 return value. The i1 return value is set to ``True`` "
"for the leader thread and ``False`` for all the other threads. Election of a "
"leader thread happens deterministically, i.e. the same leader thread is "
"elected for the same ``membermask`` every time. For more information, refer "
"PTX ISA `<https://docs.nvidia.com/cuda/parallel-thread-execution/index."
"html#parallel-synchronization-and-communication-instructions-elect-sync>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:353
msgid "Membar/Fences"
msgstr ""

#: ../../../NVPTXUsage.rst:356
msgid "'``llvm.nvvm.fence.proxy.tensormap_generic.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:376
msgid ""
"The ``@llvm.nvvm.fence.proxy.tensormap_generic.*`` is a uni-directional "
"fence used to establish ordering between a prior memory access performed via "
"the generic `proxy<https://docs.nvidia.com/cuda/parallel-thread-execution/"
"index.html#proxies>_` and a subsequent memory access performed via the "
"tensormap proxy. ``nvvm.fence.proxy.tensormap_generic.release`` can form a "
"release sequence that synchronizes with an acquire sequence that contains "
"the ``nvvm.fence.proxy.tensormap_generic.acquire`` proxy fence. The "
"following table describes the mapping between LLVM Intrinsic and the PTX "
"instruction:"
msgstr ""

#: ../../../NVPTXUsage.rst:379
msgid "NVVM Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:379
msgid "PTX Instruction"
msgstr ""

#: ../../../NVPTXUsage.rst:381
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:381
msgid "``fence.proxy.tensormap::generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:382
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.acquire.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:382
msgid "``fence.proxy.tensormap::generic.acquire.* [addr], size``"
msgstr ""

#: ../../../NVPTXUsage.rst:385
msgid ""
"The address operand ``addr`` and the operand ``size`` together specify the "
"memory range ``[addr, addr+size)`` on which the ordering guarantees on the "
"memory accesses across the proxies is to be provided. The only supported "
"value for the ``size`` operand is ``128`` and must be an immediate. Generic "
"Addressing is used unconditionally, and the address specified by the operand "
"addr must fall within the ``.global`` state space. Otherwise, the behavior "
"is undefined. For more information, see `PTX ISA <https://docs.nvidia.com/"
"cuda/parallel-thread-execution/#parallel-synchronization-and-communication-"
"instructions-membar>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:388
msgid "Address Space Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:391
msgid "'``llvm.nvvm.isspacep.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:407
msgid ""
"The '``llvm.nvvm.isspacep.*``' intrinsics determine whether the provided "
"generic pointer references memory which falls within a particular address "
"space."
msgstr ""

#: ../../../NVPTXUsage.rst:413 ../../../NVPTXUsage.rst:439
msgid ""
"If the given pointer in the generic address space refers to memory which "
"falls within the state space of the intrinsic (and therefore could be safely "
"address space casted to this space), 1 is returned, otherwise 0 is returned."
msgstr ""

#: ../../../NVPTXUsage.rst:418
msgid "'``llvm.nvvm.mapa.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:431
msgid ""
"The '``llvm.nvvm.mapa.*``' intrinsics map a shared memory pointer ``p`` of "
"another CTA with ``%rank`` to the current CTA. The ``llvm.nvvm.mapa`` form "
"expects a generic pointer to shared memory and returns a generic pointer to "
"shared cluster memory. The ``llvm.nvvm.mapa.shared.cluster`` form expects a "
"pointer to shared memory and returns a pointer to shared cluster memory. "
"They corresponds directly to the ``mapa`` and ``mapa.shared.cluster`` PTX "
"instructions."
msgstr ""

#: ../../../NVPTXUsage.rst:444
msgid "Arithmetic Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:447
msgid "'``llvm.nvvm.fabs.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:464
msgid ""
"The '``llvm.nvvm.fabs.*``' intrinsics return the absolute value of the "
"operand."
msgstr ""

#: ../../../NVPTXUsage.rst:469
msgid ""
"Unlike, '``llvm.fabs.*``', these intrinsics do not perfectly preserve NaN "
"values. Instead, a NaN input yeilds an unspecified NaN output."
msgstr ""

#: ../../../NVPTXUsage.rst:474
msgid "'``llvm.nvvm.fabs.ftz.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:488
msgid ""
"The '``llvm.nvvm.fabs.ftz.*``' intrinsics return the absolute value of the "
"operand, flushing subnormals to sign preserving zero."
msgstr ""

#: ../../../NVPTXUsage.rst:494
msgid ""
"Before the absolute value is taken, the input is flushed to sign preserving "
"zero if it is a subnormal. In addition, unlike '``llvm.fabs.*``', a NaN "
"input yields an unspecified NaN output."
msgstr ""

#: ../../../NVPTXUsage.rst:500
msgid "'``llvm.nvvm.idp2a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:516
msgid ""
"The '``llvm.nvvm.idp2a.[us].[us]``' intrinsics performs a 2-element vector "
"dot product followed by addition. They corresponds directly to the ``dp2a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:523
msgid ""
"The 32-bit value in ``%a`` is broken into 2 16-bit values which are extended "
"to 32 bits. For the '``llvm.nvvm.idp2a.u.[us]``' variants zero-extension is "
"used, while for the '``llvm.nvvm.idp2a.s.[us]``' sign-extension is used. Two "
"bytes are selected from ``%b``, if ``%is.hi`` is true, the most significant "
"bytes are selected, otherwise the least significant bytes are selected. "
"These bytes are then extended to 32-bits. For the '``llvm.nvvm.idp2a.[us]."
"u``' variants zero-extension is used, while for the '``llvm.nvvm.idp2a.[us]."
"s``' sign-extension is used. The dot product of these 2-element vectors is "
"added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:535
msgid "'``llvm.nvvm.idp4a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:550
msgid ""
"The '``llvm.nvvm.idp4a.[us].[us]``' intrinsics perform a 4-element vector "
"dot product followed by addition. They corresponds directly to the ``dp4a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:557
msgid ""
"Each of the 4 bytes in both ``%a`` and ``%b`` are extended to 32-bit "
"integers forming 2 ``<4 x i32>``. For ``%a``, zero-extension is used in the "
"'``llvm.nvvm.idp4a.u.[us]``' variants, while sign-extension is used with "
"'``llvm.nvvm.idp4a.s.[us]``' variants. Similarly, for ``%b``, zero-extension "
"is used in the '``llvm.nvvm.idp4a.[us].u``' variants, while sign-extension "
"is used with '``llvm.nvvm.idp4a.[us].s``' variants. The dot product of these "
"4-element vectors is added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:566
msgid "Bit Manipulation Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:569
msgid "'``llvm.nvvm.fshl.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:581
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsics performs a clamped "
"funnel shift left. These intrinsics are very similar to '``llvm.fshl``', "
"except the shift amount is clamped at the integer width (instead of modulo "
"it). Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:589
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsic functions performs a "
"clamped funnel shift left: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted left, and the most significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:597
msgid "'``llvm.nvvm.fshr.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:609
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsics perform a clamped funnel "
"shift right. These intrinsics are very similar to '``llvm.fshr``', except "
"the shift amount is clamped at the integer width (instead of modulo it). "
"Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:617
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsic functions performs a "
"clamped funnel shift right: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted right, and the least significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:625
msgid "'``llvm.nvvm.flo.u.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:638
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics identifies the bit position "
"of the leading one, returning either it's offset from the most or least "
"significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:644
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics returns the bit position of "
"the most significant 1. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:651
msgid "'``llvm.nvvm.flo.s.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:664
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics identifies the bit position "
"of the leading non-sign bit, returning either it's offset from the most or "
"least significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:671
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics returns the bit position of "
"the most significant 0 for negative inputs and the most significant 1 for "
"non-negative inputs. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:679
msgid "'``llvm.nvvm.{zext,sext}.{wrap,clamp}``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:694
msgid ""
"The '``llvm.nvvm.{zext,sext}.{wrap,clamp}``' family of intrinsics extracts "
"the low bits of the input value, and zero- or sign-extends them back to the "
"original width."
msgstr ""

#: ../../../NVPTXUsage.rst:701
msgid ""
"The '``llvm.nvvm.{zext,sext}.{wrap,clamp}``' family of intrinsics returns "
"extension of N lowest bits of operand %a. For the '``wrap``' variants, N is "
"the value of operand %b modulo 32. For the '``clamp``' variants, N is the "
"value of operand %b clamped to the range [0, 32]. The N lowest bits are then "
"zero-extended the case of the '``zext``' variants, or sign-extended the case "
"of the '``sext``' variants. If N is 0, the result is 0."
msgstr ""

#: ../../../NVPTXUsage.rst:709
msgid "'``llvm.nvvm.bmsk.{wrap,clamp}``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:722
msgid ""
"The '``llvm.nvvm.bmsk.{wrap,clamp}``' family of intrinsics creates a bit "
"mask given a starting bit position and a bit width."
msgstr ""

#: ../../../NVPTXUsage.rst:728
msgid ""
"The '``llvm.nvvm.bmsk.{wrap,clamp}``' family of intrinsics returns a value "
"with all bits set to 0 except for %b bits starting at bit position %a. For "
"the '``wrap``' variants, the values of %a and %b modulo 32 are used. For the "
"'``clamp``' variants, the values of %a and %b are clamped to the range [0, "
"32], which in practice is equivalent to using them as is."
msgstr ""

#: ../../../NVPTXUsage.rst:735
msgid "'``llvm.nvvm.prmt``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:747
msgid ""
"The '``llvm.nvvm.prmt``' constructs a permutation of the bytes of the first "
"two operands, selecting based on the third operand."
msgstr ""

#: ../../../NVPTXUsage.rst:753
msgid ""
"The bytes in the first two source operands are numbered from 0 to 7: {%hi, "
"%lo} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}}. For each byte in the target "
"register, a 4-bit selection value is defined."
msgstr ""

#: ../../../NVPTXUsage.rst:757
msgid ""
"The 3 lsbs of the selection value specify which of the 8 source bytes should "
"be moved into the target position. The msb defines if the byte value should "
"be copied, or if the sign (msb of the byte) should be replicated over all 8 "
"bits of the target position (sign extend of the byte value); msb=0 means "
"copy the literal value; msb=1 means replicate the sign."
msgstr ""

#: ../../../NVPTXUsage.rst:763
msgid ""
"These 4-bit selection values are pulled from the lower 16-bits of the "
"%selector operand, with the least significant selection value corresponding "
"to the least significant byte of the destination."
msgstr ""

#: ../../../NVPTXUsage.rst:769
msgid "'``llvm.nvvm.prmt.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:787
msgid ""
"The '``llvm.nvvm.prmt.*``' family of intrinsics constructs a permutation of "
"the bytes of the first one or two operands, selecting based on the 2 least "
"significant bits of the final operand."
msgstr ""

#: ../../../NVPTXUsage.rst:794
msgid ""
"As with the generic '``llvm.nvvm.prmt``' intrinsic, the bytes in the first "
"one or two source operands are numbered. The first source operand (%lo) is "
"numbered {b3, b2, b1, b0}, in the case of the '``f4e``' and '``b4e``' "
"variants, the second source operand (%hi) is numbered {b7, b6, b5, b4}."
msgstr ""

#: ../../../NVPTXUsage.rst:799
msgid ""
"Depending on the 2 least significant bits of the %selector operand, the "
"result of the permutation is defined as follows:"
msgstr ""

#: ../../../NVPTXUsage.rst:803
msgid "Mode"
msgstr ""

#: ../../../NVPTXUsage.rst:803
msgid "%selector[1:0]"
msgstr ""

#: ../../../NVPTXUsage.rst:803
msgid "Output"
msgstr ""

#: ../../../NVPTXUsage.rst:805
msgid "'``f4e``'"
msgstr ""

#: ../../../NVPTXUsage.rst:805 ../../../NVPTXUsage.rst:829
#: ../../../NVPTXUsage.rst:843
msgid "{3, 2, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:807
msgid "{4, 3, 2, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:809
msgid "{5, 4, 3, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:811
msgid "{6, 5, 4, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:813
msgid "'``b4e``'"
msgstr ""

#: ../../../NVPTXUsage.rst:813
msgid "{5, 6, 7, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:815
msgid "{6, 7, 0, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:817
msgid "{7, 0, 1, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:819
msgid "{0, 1, 2, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:821
msgid "'``rc8``'"
msgstr ""

#: ../../../NVPTXUsage.rst:821 ../../../NVPTXUsage.rst:837
msgid "{0, 0, 0, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:823
msgid "{1, 1, 1, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:825
msgid "{2, 2, 2, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:827 ../../../NVPTXUsage.rst:835
msgid "{3, 3, 3, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:829
msgid "'``ecl``'"
msgstr ""

#: ../../../NVPTXUsage.rst:831
msgid "{3, 2, 1, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:833
msgid "{3, 2, 2, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:837
msgid "'``ecr``'"
msgstr ""

#: ../../../NVPTXUsage.rst:839
msgid "{1, 1, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:841
msgid "{2, 2, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:845
msgid "'``rc16``'"
msgstr ""

#: ../../../NVPTXUsage.rst:845 ../../../NVPTXUsage.rst:849
msgid "{1, 0, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:847 ../../../NVPTXUsage.rst:851
msgid "{3, 2, 3, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:855
msgid "TMA family of Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:858
msgid "'``llvm.nvvm.cp.async.bulk.global.to.shared.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:870
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.global.to.shared.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.global.*`` family of PTX "
"instructions. These instructions initiate an asynchronous copy of bulk data "
"from global memory to shared::cluster memory. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:877
msgid ""
"The last two arguments to these intrinsics are boolean flags indicating "
"support for cache_hint and/or multicast modifiers. These flag arguments must "
"be compile-time constants. The backend looks through these flags and lowers "
"the intrinsics appropriately."
msgstr ""

#: ../../../NVPTXUsage.rst:882
msgid ""
"The Nth argument (denoted by ``i1 %flag_ch``) when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:886
msgid ""
"The [N-1]th argument (denoted by ``i1 %flag_mc``) when set, indicates the "
"presence of a multicast mask (``i16 %mc``) and generates the PTX instruction "
"with the ``.multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:890 ../../../NVPTXUsage.rst:922
#: ../../../NVPTXUsage.rst:946
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:894
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.global``'"
msgstr ""

#: ../../../NVPTXUsage.rst:907
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.global``' intrinsic "
"corresponds to the ``cp.async.bulk.global.shared::cta.*`` set of PTX "
"instructions. These instructions initiate an asynchronous copy from shared::"
"cta to global memory. The 32-bit operand ``%size`` specifies the amount of "
"memory to be copied (in bytes) and it must be a multiple of 16. For the ``."
"bytemask`` variant, the 16-bit wide mask operand specifies whether the i-th "
"byte of each 16-byte wide chunk of source data is copied to the destination."
msgstr ""

#: ../../../NVPTXUsage.rst:916
msgid ""
"The ``i1 %flag_ch`` argument to these intrinsics is a boolean flag "
"indicating support for cache_hint. This flag argument must be a compile-time "
"constant. When set, it indicates a valid cache_hint (``i64 %ch``) and "
"generates the ``.L2::cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:926
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:938
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.shared::cta.*`` PTX "
"instruction. This instruction initiates an asynchronous copy from shared::"
"cta to shared::cluster memory. The destination has to be in the shared "
"memory of a different CTA within the cluster. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:950
msgid "'``llvm.nvvm.cp.async.bulk.prefetch.L2``'"
msgstr ""

#: ../../../NVPTXUsage.rst:962
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.prefetch.L2``' intrinsic corresponds to the "
"``cp.async.bulk.prefetch.L2.*`` family of PTX instructions. These "
"instructions initiate an asynchronous prefetch of bulk data from global "
"memory to the L2 cache. The 32-bit operand ``%size`` specifies the amount of "
"memory to be prefetched in terms of bytes and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:969
msgid ""
"The last argument to these intrinsics is boolean flag indicating support for "
"cache_hint. These flag argument must be compile-time constant. When set, it "
"indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:974
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:978
msgid "'``llvm.nvvm.prefetch.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1005
msgid ""
"The '``@llvm.nvvm.prefetch.*``' and '``@llvm.nvvm.prefetchu.*``' intrinsic "
"correspond to the '``prefetch.*``;' and '``prefetchu.*``' family of PTX "
"instructions. The '``prefetch.*``' instructions bring the cache line "
"containing the specified address in global or local memory address space "
"into the specified cache level (L1 or L2). If the '``.tensormap``' qualifier "
"is specified then the prefetch instruction brings the cache line containing "
"the specified address in the '``.const``' or '``.param memory``' state space "
"for subsequent use by the '``cp.async.bulk.tensor``' instruction. The "
"'`prefetchu.*``' instruction brings the cache line containing the specified "
"generic address into the specified uniform cache level. If no address space "
"is specified, it is assumed to be generic address. The intrinsic uses and "
"eviction priority which can be accessed by the '``.level::"
"eviction_priority``' modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:1017
msgid "A prefetch to a shared memory location performs no operation."
msgstr ""

#: ../../../NVPTXUsage.rst:1018
msgid ""
"A prefetch into the uniform cache requires a generic address, and no "
"operation occurs if the address maps to a const, local, or shared memory "
"location."
msgstr ""

#: ../../../NVPTXUsage.rst:1021
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#data-movement-and-conversion-instructions-"
"prefetch-prefetchu>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1025
msgid "'``llvm.nvvm.applypriority.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1038
msgid ""
"The '``@llvm.nvvm.applypriority.*``'  applies the cache eviction priority "
"specified by the .level::eviction_priority qualifier to the address range "
"[a..a+size) in the specified cache level. If no state space is specified "
"then Generic Addressing is used. If the specified address does not fall "
"within the address window of .global state space then the behavior is "
"undefined. The operand size is an integer constant that specifies the amount "
"of data, in bytes, in the specified cache level on which the priority is to "
"be applied. The only supported value for the size operand is 128."
msgstr ""

#: ../../../NVPTXUsage.rst:1045
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#data-movement-and-conversion-instructions-"
"applypriority>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1049
msgid "``llvm.nvvm.discard.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1062
msgid ""
"The *effects* of the ``@llvm.nvvm.discard.L2*`` intrinsics are those of a "
"non-atomic non-volatile ``llvm.memset`` that writes ``undef`` to the "
"destination address range ``[%ptr, %ptr + immarg)``. The ``%ptr`` must be "
"aligned by 128 bytes. Subsequent reads from the address range may read "
"``undef`` until the memory is overwritten with a different value. These "
"operations *hint* the implementation that data in the L2 cache can be "
"destructively discarded without writing it back to memory. The operand "
"``immarg`` is an integer constant that specifies the length in bytes of the "
"address range ``[%ptr, %ptr + immarg)`` to write ``undef`` into. The only "
"supported value for the ``immarg`` operand is ``128``. If generic addressing "
"is used and the specified address does not fall within the address window of "
"global memory (``addrspace(1)``) the behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1085
msgid ""
"For more information, refer to the  `CUDA C++ discard documentation <https://"
"nvidia.github.io/cccl/libcudacxx/extended_api/memory_access_properties/"
"discard_memory.html>`__ and to the `PTX ISA discard documentation <https://"
"docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-"
"instructions-discard>`__ ."
msgstr ""

#: ../../../NVPTXUsage.rst:1088
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1106
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``tile`` mode. In tile mode, the multi-dimensional layout of the source "
"tensor is preserved at the destination. The dimension of the tensor data "
"ranges from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 "
"%d4`` arguments. In ``tile.gather4`` mode, four rows in a 2D tensor are "
"combined to form a single 2D destination tensor. The first coordinate ``i32 "
"%x0`` denotes the column index followed by four coordinates indicating the "
"four row-indices. So, this mode takes a total of 5 coordinates as input "
"arguments. For more information on ``gather4`` mode, refer PTX ISA `<https://"
"docs.nvidia.com/cuda/parallel-thread-execution/#tensor-tiled-scatter4-"
"gather4-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1121
msgid ""
"The last three arguments to these intrinsics are flags indicating support "
"for multicast, cache_hint and cta_group::1/2 modifiers. These flag arguments "
"must be compile-time constants. The backend looks through these flags and "
"lowers the intrinsics appropriately."
msgstr ""

#: ../../../NVPTXUsage.rst:1127
msgid ""
"The argument denoted by ``i1 %flag_ch`` when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1131
msgid ""
"The argument denoted by ``i1 %flag_mc`` when set, indicates the presence of "
"a multicast mask (``i16 %mc``) and generates the PTX instruction with the ``."
"multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:1135
msgid ""
"The argument denoted by ``i32 %flag_cta_group`` takes values within the "
"range [0, 3) i.e. {0,1,2}. When the value of ``%flag_cta_group`` is not "
"within the range, it may raise an error from the Verifier. The default value "
"is '0' with no cta_group modifier in the instruction. The values of '1' and "
"'2' lower to ``cta_group::1`` and ``cta_group::2`` variants of the PTX "
"instruction respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1142 ../../../NVPTXUsage.rst:1186
#: ../../../NVPTXUsage.rst:1229 ../../../NVPTXUsage.rst:1276
#: ../../../NVPTXUsage.rst:1317 ../../../NVPTXUsage.rst:1345
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1146
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1168
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``im2col`` mode. In im2col mode, some dimensions of the source tensor are "
"unrolled into a single dimensional column at the destination. In this mode, "
"the tensor has to be at least three-dimensional. Along with the tensor "
"coordinates, im2col offsets are also specified (denoted by ``i16 im2col0..."
"i16 %im2col2``). For the ``im2col`` mode, the number of offsets is two less "
"than the number of dimensions of the tensor operation. For the ``im2col.w`` "
"and ``im2col.w.128`` mode, the number of offsets is always 2, denoted by "
"``i16 %wHalo`` and ``i16 %wOffset`` arguments. For more information on "
"``im2col.w`` and ``im2col.w.128`` modes, refer PTX ISA `<https://docs.nvidia."
"com/cuda/parallel-thread-execution/#tensor-im2col-w-w128-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1183
msgid ""
"The last three arguments to these intrinsics are flags, with the same "
"functionality as described in the ``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1190
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.cta.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1208
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.cta.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.shared::cta.global.*`` set "
"of PTX instructions. These instructions initiate an asynchronous copy of "
"tensor data from global memory to shared::cta memory in ``tile`` mode. In "
"tile mode, the multi-dimensional layout of the source tensor is preserved at "
"the destination. The dimension of the tensor data ranges from 1d to 5d with "
"the coordinates specified by the ``i32 %d0 ... i32 %d4`` arguments. In "
"``tile.gather4`` mode, four rows in a 2D tensor are combined to form a "
"single 2D destination tensor. The first coordinate ``i32 %x0`` denotes the "
"column index followed by four coordinates indicating the four row-indices. "
"So, this mode takes a total of 5 coordinates as input arguments. For more "
"information on ``gather4`` mode, refer PTX ISA `<https://docs.nvidia.com/"
"cuda/parallel-thread-execution/#tensor-tiled-scatter4-gather4-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1223 ../../../NVPTXUsage.rst:1270
#: ../../../NVPTXUsage.rst:1311 ../../../NVPTXUsage.rst:1382
#: ../../../NVPTXUsage.rst:1470
msgid ""
"The last argument to these intrinsics is a boolean flag indicating support "
"for cache_hint. This flag argument must be a compile-time constant. When "
"set, it indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1233
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.cta.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1255
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.cta.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.shared::cta.global.*`` set "
"of PTX instructions. These instructions initiate an asynchronous copy of "
"tensor data from global memory to shared::cta memory in ``im2col`` mode. In "
"im2col mode, some dimensions of the source tensor are unrolled into a single "
"dimensional column at the destination. In this mode, the tensor has to be at "
"least three-dimensional. Along with the tensor coordinates, im2col offsets "
"are also specified (denoted by ``i16 im2col0...i16 %im2col2``). For the "
"``im2col`` mode, the number of offsets is two less than the number of "
"dimensions of the tensor operation. For the ``im2col.w`` and ``im2col."
"w.128`` mode, the number of offsets is always 2, denoted by ``i16 %wHalo`` "
"and ``i16 %wOffset`` arguments. For more information on ``im2col.w`` and "
"``im2col.w.128`` modes, refer PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensor-im2col-w-w128-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1280
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1298
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``tile`` mode. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments. In ``tile.scatter4`` "
"mode, a single 2D source tensor is divided into four rows in the 2D "
"destination tensor. The first coordinate ``i32 %x0`` denotes the column "
"index followed by four coordinates indicating the four row-indices. So, this "
"mode takes a total of 5 coordinates as input arguments. For more information "
"on ``scatter4`` mode, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#tensor-tiled-scatter4-gather4-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1321
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1335
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``im2col`` mode. "
"In this mode, the tensor has to be at least three-dimensional. Unlike the "
"``g2s`` variants, there are no im2col_offsets for these intrinsics. The last "
"argument to these intrinsics is a boolean flag, with the same functionality "
"as described in the ``s2g.tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1349
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1367
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In tile mode, the multi-"
"dimensional layout of the source tensor is preserved at the destination. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:1375
msgid ""
"In ``tile.gather4`` mode, four rows in the 2-dimnesional source tensor are "
"fetched to the L2 cache. The first coordinate ``i32 %x0`` denotes the column "
"index followed by four coordinates indicating the four row-indices. So, this "
"mode takes a total of 5 coordinates as input arguments. For more information "
"on ``gather4`` mode, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#tensor-tiled-scatter4-gather4-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1388 ../../../NVPTXUsage.rst:1433
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1392
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1414
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In im2col mode, some "
"dimensions of the source tensor are unrolled into a single dimensional "
"column at the destination. In this mode, the tensor has to be at least three-"
"dimensional. Along with the tensor coordinates, im2col offsets are also "
"specified (denoted by ``i16 im2col0...i16 %im2col2``). For ``im2col`` mode, "
"the number of offsets is two less than the number of dimensions of the "
"tensor operation. For the ``im2col.w`` and ``im2col.w.128`` modes, the "
"number of offsets is always 2, denoted by ``i16 %wHalo`` and ``i16 "
"%wOffset`` arguments. For more information on ``im2col.w`` and ``im2col."
"w.128`` modes, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-thread-"
"execution/#tensor-im2col-w-w128-modes>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1430
msgid ""
"The last argument to these intrinsics is a boolean flag, with the same "
"functionality as described in the ``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1437
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1461
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.tile.[1-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[1-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``tile`` mode. The dimension of the tensor data ranges "
"from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 %d4`` "
"arguments. The supported reduction operations are {add, min, max, inc, dec, "
"and, or, xor} as described in the ``tile.1d`` intrinsics."
msgstr ""

#: ../../../NVPTXUsage.rst:1476 ../../../NVPTXUsage.rst:1503
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"reduce-async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1480
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1494
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.im2col.[3-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[3-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``im2col`` mode. In this mode, the tensor has to be at "
"least three-dimensional. The supported reduction operations supported are "
"the same as the ones in the tile mode. The last argument to these intrinsics "
"is a boolean flag, with the same functionality as described in the ``tile`` "
"mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1507
msgid "Warp Group Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1510
msgid "'``llvm.nvvm.wgmma.fence.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1522
msgid ""
"The '``@llvm.nvvm.wgmma.fence.sync.aligned``' intrinsic generates the "
"``wgmma.fence.sync.aligned`` PTX instruction, which establishes an ordering "
"between prior accesses to any warpgroup registers and subsequent accesses to "
"the same registers by a ``wgmma.mma_async`` instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1527
msgid ""
"The ``wgmma.fence`` instruction must be issued by all warps of the warpgroup "
"in the following locations:"
msgstr ""

#: ../../../NVPTXUsage.rst:1530
msgid "Before the first ``wgmma.mma_async`` operation in a warpgroup."
msgstr ""

#: ../../../NVPTXUsage.rst:1531
msgid ""
"Between a register access by a thread in the warpgroup and any ``wgmma."
"mma_async`` instruction that accesses the same registers, except when these "
"are accumulator register accesses across multiple ``wgmma.mma_async`` "
"instructions of the same shape in which case an ordering guarantee is "
"provided by default."
msgstr ""

#: ../../../NVPTXUsage.rst:1537
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"fence>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1541
msgid "'``llvm.nvvm.wgmma.commit_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1553
msgid ""
"The '``@llvm.nvvm.wgmma.commit_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned`` PTX instruction, which creates a new "
"wgmma-group per warpgroup and batches all prior ``wgmma.mma_async`` "
"instructions initiated by the executing warp but not committed to any wgmma-"
"group into the new wgmma-group. If there are no uncommitted ``wgmma "
"mma_async`` instructions then, ``wgmma.commit_group`` results in an empty "
"wgmma-group."
msgstr ""

#: ../../../NVPTXUsage.rst:1561
msgid ""
"An executing thread can wait for the completion of all ``wgmma.mma_async`` "
"operations in a wgmma-group by using ``wgmma.wait_group``."
msgstr ""

#: ../../../NVPTXUsage.rst:1564
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"commit-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1568
msgid "'``llvm.nvvm.wgmma.wait_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1580
msgid ""
"The '``@llvm.nvvm.wgmma.wait_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned N`` PTX instruction, which will cause the "
"executing thread to wait until only ``N`` or fewer of the most recent wgmma-"
"groups are pending and all the prior wgmma-groups committed by the executing "
"threads are complete. For example, when ``N`` is 0, the executing thread "
"waits on all the prior wgmma-groups to complete. Operand ``N`` is an integer "
"constant."
msgstr ""

#: ../../../NVPTXUsage.rst:1588
msgid ""
"Accessing the accumulator register or the input register containing the "
"fragments of matrix A of a ``wgmma.mma_async`` instruction without first "
"performing a ``wgmma.wait_group`` instruction that waits on a wgmma-group "
"including that ``wgmma.mma_async`` instruction is undefined behavior."
msgstr ""

#: ../../../NVPTXUsage.rst:1593
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"wait-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1597
msgid "'``llvm.nvvm.griddepcontrol.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1610
msgid ""
"The ``griddepcontrol`` intrinsics allows the dependent grids and "
"prerequisite grids as defined by the runtime, to control execution in the "
"following way:"
msgstr ""

#: ../../../NVPTXUsage.rst:1612
msgid ""
"``griddepcontrol.launch_dependents`` intrinsic signals that the dependents "
"can be scheduled, before the current grid completes. The intrinsic can be "
"invoked by multiple threads in the current CTA and repeated invocations of "
"the intrinsic will have no additional side effects past that of the first "
"invocation."
msgstr ""

#: ../../../NVPTXUsage.rst:1614
msgid ""
"``griddepcontrol.wait`` intrinsic causes the executing thread to wait until "
"all prerequisite grids in flight have completed and all the memory "
"operations from the prerequisite grids are performed and made visible to the "
"current grid."
msgstr ""

#: ../../../NVPTXUsage.rst:1616
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#parallel-synchronization-and-communication-instructions-"
"griddepcontrol>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1620
msgid "TCGEN05 family of Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1622
msgid ""
"The llvm.nvvm.tcgen05.* intrinsics model the TCGEN05 family of instructions "
"exposed by PTX. These intrinsics use 'Tensor Memory' (henceforth ``tmem``). "
"NVPTX represents this memory using ``addrspace(6)`` and is always 32-bits."
msgstr ""

#: ../../../NVPTXUsage.rst:1626
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensor-memory>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1629
msgid ""
"The tensor-memory pointers may only be used with the tcgen05 intrinsics. "
"There are specialized load/store instructions provided (tcgen05.ld/st) to "
"work with tensor-memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1633
msgid ""
"See the PTX ISA for more information on tensor-memory load/store "
"instructions `<https://docs.nvidia.com/cuda/parallel-thread-execution/"
"#tensor-memory-and-register-load-store-instructions>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1637
msgid "'``llvm.nvvm.tcgen05.alloc``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1652
msgid ""
"The '``@llvm.nvvm.tcgen05.alloc.*``' intrinsics correspond to the ``tcgen05."
"alloc.cta_group*.sync.aligned.b32`` family of PTX instructions. The "
"``tcgen05.alloc`` is a potentially blocking instruction which dynamically "
"allocates the specified number of columns in the Tensor Memory and writes "
"the address of the allocated Tensor Memory into shared memory at the "
"location specified by ``%dst``. The 32-bit operand ``%ncols`` specifies the "
"number of columns to be allocated and it must be a power-of-two. The ``."
"shared`` variant explicitly uses shared memory address space for the "
"``%dst`` operand. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` variants of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1663 ../../../NVPTXUsage.rst:1688
#: ../../../NVPTXUsage.rst:1714
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensor-memory-allocation-and-management-"
"instructions>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1667
msgid "'``llvm.nvvm.tcgen05.dealloc``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1680
msgid ""
"The '``@llvm.nvvm.tcgen05.dealloc.*``' intrinsics correspond to the "
"``tcgen05.dealloc.*`` set of PTX instructions. The ``tcgen05.dealloc`` "
"instructions deallocates the Tensor Memory specified by the Tensor Memory "
"address ``%tmem_addr``. The operand ``%tmem_addr`` must point to a previous "
"Tensor Memory allocation. The 32-bit operand ``%ncols`` specifies the number "
"of columns to be de-allocated. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` variants of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1692
msgid "'``llvm.nvvm.tcgen05.relinq.alloc.permit``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1705
msgid ""
"The '``@llvm.nvvm.tcgen05.relinq.alloc.permit.*``' intrinsics correspond to "
"the ``tcgen05.relinquish_alloc_permit.*`` set of PTX instructions. This "
"instruction specifies that the CTA of the executing thread is relinquishing "
"the right to allocate Tensor Memory. So, it is illegal for a CTA to perform "
"``tcgen05.alloc`` after any of its constituent threads execute ``tcgen05."
"relinquish_alloc_permit``. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` flavors of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1718
msgid "'``llvm.nvvm.tcgen05.commit``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1733
msgid ""
"The '``@llvm.nvvm.tcgen05.commit.*``' intrinsics correspond to the ``tcgen05."
"commit.{cg1/cg2}.mbarrier::arrive::one.*`` set of PTX instructions. The "
"``tcgen05.commit`` is an asynchronous instruction which makes the mbarrier "
"object (``%mbar``) track the completion of all prior asynchronous tcgen05 "
"operations. The ``.mc`` variants allow signaling on the mbarrier objects of "
"multiple CTAs (specified by ``%mc``) in the cluster. The ``.cg1`` and ``."
"cg2`` variants generate ``cta_group::1`` and ``cta_group::2`` flavors of the "
"instruction respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1741
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen-async-sync-operations-commit>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1745
msgid "'``llvm.nvvm.tcgen05.wait``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1758
msgid ""
"The '``@llvm.nvvm.tcgen05.wait.ld/st``' intrinsics correspond to the "
"``tcgen05.wait::{ld/st}.sync.aligned`` pair of PTX instructions. The "
"``tcgen05.wait::ld`` causes the executing thread to block until all prior "
"``tcgen05.ld`` operations issued by the executing thread have completed. The "
"``tcgen05.wait::st`` causes the executing thread to block until all prior "
"``tcgen05.st`` operations issued by the executing thread have completed."
msgstr ""

#: ../../../NVPTXUsage.rst:1766
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-wait>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1770
msgid "'``llvm.nvvm.tcgen05.fence``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1783
msgid ""
"The '``@llvm.nvvm.tcgen05.fence.*``' intrinsics correspond to the ``tcgen05."
"fence::{before/after}_thread_sync`` pair of PTX instructions. These "
"instructions act as code motion fences for asynchronous tcgen05 operations."
msgstr ""

#: ../../../NVPTXUsage.rst:1788
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-"
"fence>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1792
msgid "'``llvm.nvvm.tcgen05.shift``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1805
msgid ""
"The '``@llvm.nvvm.tcgen05.shift.{cg1/cg2}``' intrinsics correspond to the "
"``tcgen05.shift.{cg1/cg2}`` PTX instructions. The ``tcgen05.shift`` is an "
"asynchronous instruction which initiates the shifting of 32-byte elements "
"downwards across all the rows, except the last, by one row. The address "
"operand ``%tmem_addr`` specifies the base address of the matrix in the "
"Tensor Memory whose rows must be down shifted."
msgstr ""

#: ../../../NVPTXUsage.rst:1812
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-shift>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1816
msgid "'``llvm.nvvm.tcgen05.cp``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1847
msgid ""
"The '``@llvm.nvvm.tcgen05.cp.{shape}.{src_fmt}.{cg1/cg2}``' intrinsics "
"correspond to the ``tcgen05.cp.*`` family of PTX instructions. The ``tcgen05."
"cp`` instruction initiates an asynchronous copy operation from shared memory "
"to the location specified by ``%tmem_addr`` in Tensor Memory. The 64-bit "
"register operand ``%sdesc`` is the matrix descriptor representing the source "
"matrix in shared memory that needs to be copied."
msgstr ""

#: ../../../NVPTXUsage.rst:1854
msgid ""
"The valid shapes for the copy operation are: {128x256b, 4x256b, 128x128b, "
"64x128b_warpx2_02_13, 64x128b_warpx2_01_23, 32x128b_warpx4}."
msgstr ""

#: ../../../NVPTXUsage.rst:1857
msgid ""
"Shapes ``64x128b`` and ``32x128b`` require dedicated multicast qualifiers, "
"which are appended to the corresponding intrinsic names."
msgstr ""

#: ../../../NVPTXUsage.rst:1860
msgid ""
"Optionally, the data can be decompressed from the source format in the "
"shared memory to the destination format in Tensor Memory during the copy "
"operation. Currently, only ``.b8x16`` is supported as destination format. "
"The valid source formats are ``.b6x16_p32`` and ``.b4x16_p64``."
msgstr ""

#: ../../../NVPTXUsage.rst:1865
msgid ""
"When the source format is ``.b6x16_p32``, a contiguous set of 16 elements of "
"6-bits each followed by four bytes of padding (``_p32``) in shared memory is "
"decompressed into 16 elements of 8-bits (``.b8x16``) each in the Tensor "
"Memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1869
msgid ""
"When the source format is ``.b4x16_p64``, a contiguous set of 16 elements of "
"4-bits each followed by eight bytes of padding (``_p64``) in shared memory "
"is decompressed into 16 elements of 8-bits (``.b8x16``) each in the Tensor "
"Memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1873
msgid ""
"For more information on the decompression schemes, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/#optional-"
"decompression>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1876
msgid ""
"For more information on the tcgen05.cp instruction, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-"
"instructions-tcgen05-cp>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1880
msgid "'``llvm.nvvm.tcgen05.ld.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1894
msgid ""
"This group of intrinsics asynchronously load data from the Tensor Memory at "
"the location specified by the 32-bit address operand `tmem_addr` into the "
"destination registers, collectively across all threads of the warps."
msgstr ""

#: ../../../NVPTXUsage.rst:1898 ../../../NVPTXUsage.rst:1951
msgid ""
"All the threads in the warp must specify the same value of `tmem_addr`, "
"which must be the base address of the collective load operation. Otherwise, "
"the behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1901 ../../../NVPTXUsage.rst:1954
msgid ""
"The `shape` qualifier and the `num` qualifier together determines the total "
"dimension of the data ('n') which is loaded from the Tensor Memory. The "
"`shape` qualifier indicates the base dimension of data. The `num` qualifier "
"indicates the repeat factor on the base dimension resulting in the total "
"dimension of the data that is accessed."
msgstr ""

#: ../../../NVPTXUsage.rst:1905 ../../../NVPTXUsage.rst:1958
msgid "Allowed values for the 'num' are `x1, x2, x4, x8, x16, x32, x64, x128`."
msgstr ""

#: ../../../NVPTXUsage.rst:1907 ../../../NVPTXUsage.rst:1960
msgid ""
"Allowed values for the 'shape' in the first intrinsic are `16x64b, 16x128b, "
"16x256b, 32x32b`."
msgstr ""

#: ../../../NVPTXUsage.rst:1909 ../../../NVPTXUsage.rst:1962
msgid "Allowed value for the 'shape' in the second intrinsic is `16x32bx2`."
msgstr ""

#: ../../../NVPTXUsage.rst:1911
msgid ""
"The result of the intrinsic is a vector consisting of one or more 32-bit "
"registers derived from `shape` and `num` as shown below."
msgstr ""

#: ../../../NVPTXUsage.rst:1915
msgid "num/shape"
msgstr ""

#: ../../../NVPTXUsage.rst:1915
msgid "16x32bx2/16x64b/32x32b"
msgstr ""

#: ../../../NVPTXUsage.rst:1915
msgid "16x128b"
msgstr ""

#: ../../../NVPTXUsage.rst:1915
msgid "16x256b"
msgstr ""

#: ../../../NVPTXUsage.rst:1917
msgid "x1"
msgstr ""

#: ../../../NVPTXUsage.rst:1918
msgid "x2"
msgstr ""

#: ../../../NVPTXUsage.rst:1918 ../../../NVPTXUsage.rst:1919
#: ../../../NVPTXUsage.rst:1920
msgid "8"
msgstr ""

#: ../../../NVPTXUsage.rst:1919
msgid "x4"
msgstr ""

#: ../../../NVPTXUsage.rst:1919 ../../../NVPTXUsage.rst:1920
#: ../../../NVPTXUsage.rst:1921 ../../../NVPTXUsage.rst:2095
msgid "16"
msgstr ""

#: ../../../NVPTXUsage.rst:1920
msgid "x8"
msgstr ""

#: ../../../NVPTXUsage.rst:1920 ../../../NVPTXUsage.rst:1921
#: ../../../NVPTXUsage.rst:1922 ../../../NVPTXUsage.rst:2043
#: ../../../NVPTXUsage.rst:2091 ../../../NVPTXUsage.rst:2099
#: ../../../NVPTXUsage.rst:2101 ../../../NVPTXUsage.rst:2103
#: ../../../NVPTXUsage.rst:2105
msgid "32"
msgstr ""

#: ../../../NVPTXUsage.rst:1921
msgid "x16"
msgstr ""

#: ../../../NVPTXUsage.rst:1921 ../../../NVPTXUsage.rst:1922
#: ../../../NVPTXUsage.rst:1923 ../../../NVPTXUsage.rst:2039
#: ../../../NVPTXUsage.rst:2048
msgid "64"
msgstr ""

#: ../../../NVPTXUsage.rst:1922
msgid "x32"
msgstr ""

#: ../../../NVPTXUsage.rst:1922 ../../../NVPTXUsage.rst:1923
#: ../../../NVPTXUsage.rst:1924 ../../../NVPTXUsage.rst:2089
#: ../../../NVPTXUsage.rst:2093 ../../../NVPTXUsage.rst:2097
msgid "128"
msgstr ""

#: ../../../NVPTXUsage.rst:1923
msgid "x64"
msgstr ""

#: ../../../NVPTXUsage.rst:1923 ../../../NVPTXUsage.rst:1924
msgid "NA"
msgstr ""

#: ../../../NVPTXUsage.rst:1924
msgid "x128"
msgstr ""

#: ../../../NVPTXUsage.rst:1927
msgid ""
"The last argument `i1 %pack` is a compile-time constant which when set, "
"indicates that the adjacent columns are packed into a single 32-bit element "
"during the load"
msgstr ""

#: ../../../NVPTXUsage.rst:1929
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-ld>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1934
msgid "'``llvm.nvvm.tcgen05.st.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1948
msgid ""
"This group of intrinsics asynchronously store data from the source vector "
"into the Tensor Memory at the location specified by the 32-bit address "
"operand 'tmem_addr` collectively across all threads of the warps."
msgstr ""

#: ../../../NVPTXUsage.rst:1964
msgid ""
"`args` argument is a vector consisting of one or more 32-bit registers "
"derived from `shape` and `num` as listed in the table listed in the `tcgen05."
"ld` section."
msgstr ""

#: ../../../NVPTXUsage.rst:1967
msgid ""
"Each shape support an `unpack` mode to allow a 32-bit element in the "
"register to be unpacked into two 16-bit elements and store them in adjacent "
"columns. `unpack` mode can be enabled by setting the `%unpack` operand to 1 "
"and can be disabled by setting it to 0."
msgstr ""

#: ../../../NVPTXUsage.rst:1969
msgid ""
"The last argument `i1 %unpack` is a compile-time constant which when set, "
"indicates that a 32-bit element in the register to be unpacked into two 16-"
"bit elements and store them in adjacent columns."
msgstr ""

#: ../../../NVPTXUsage.rst:1971
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-st>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1975
msgid "tcgen05.mma Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1978
msgid "Overview"
msgstr ""

#: ../../../NVPTXUsage.rst:1980
msgid ""
"`tcgen05.mma` operation of shape `M x N x K` perform matrix multiplication "
"and accumulation of the form: `D =  A * B + D` where:"
msgstr ""

#: ../../../NVPTXUsage.rst:1983
msgid ""
"the `A` matrix has shape `M x K`, in either `Tensor Memory` or `Shared "
"Memory`"
msgstr ""

#: ../../../NVPTXUsage.rst:1984
msgid ""
"the `B` matrix has shape `K x N`, in `Shared Memory` of the current CTA and, "
"optionally in peer CTA"
msgstr ""

#: ../../../NVPTXUsage.rst:1985
msgid "the `D` matrix is of the shape `M x N`, in `Tensor Memory`"
msgstr ""

#: ../../../NVPTXUsage.rst:1987
msgid ""
"Optionally an input predicate can be used to disable the input "
"(`%enable_inp_d`) from the accumulator matrix and the following operation "
"can be performed as `D = A * B`"
msgstr ""

#: ../../../NVPTXUsage.rst:1990
msgid ""
"The matrix multiplication and accumulation operations are categorized into "
"various kinds based on input types and the throughput of the multiplication "
"operation. The following table shows the different kinds of MMA operations "
"that are supported:"
msgstr ""

#: ../../../NVPTXUsage.rst:1995
msgid ".kind"
msgstr ""

#: ../../../NVPTXUsage.rst:1995
msgid "Supported Input Types"
msgstr ""

#: ../../../NVPTXUsage.rst:1997
msgid "f16"
msgstr ""

#: ../../../NVPTXUsage.rst:1997
msgid "F16 and BF16"
msgstr ""

#: ../../../NVPTXUsage.rst:1999
msgid "tf32"
msgstr ""

#: ../../../NVPTXUsage.rst:1999 ../../../NVPTXUsage.rst:2180
#: ../../../NVPTXUsage.rst:2334 ../../../NVPTXUsage.rst:2405
msgid "TF32"
msgstr ""

#: ../../../NVPTXUsage.rst:2001
msgid "f8f6f4"
msgstr ""

#: ../../../NVPTXUsage.rst:2001
msgid "All combinations of F8, F6, and F4"
msgstr ""

#: ../../../NVPTXUsage.rst:2003
msgid "i8"
msgstr ""

#: ../../../NVPTXUsage.rst:2003
msgid "Signed and Unsigned 8-bit Integers"
msgstr ""

#: ../../../NVPTXUsage.rst:2005
msgid "mxf8f6f4"
msgstr ""

#: ../../../NVPTXUsage.rst:2005
msgid "MX-floating point formats"
msgstr ""

#: ../../../NVPTXUsage.rst:2007
msgid "mxf4"
msgstr ""

#: ../../../NVPTXUsage.rst:2007
msgid "MX-floating point formats (FP4)"
msgstr ""

#: ../../../NVPTXUsage.rst:2009
msgid "mxf4nvf4"
msgstr ""

#: ../../../NVPTXUsage.rst:2009
msgid "MXF4 + custom NVIDIA 4-bit floating point (with common scaling factor)"
msgstr ""

#: ../../../NVPTXUsage.rst:2013
msgid ""
"`tcgen05.mma.sp` supports sparse variant of `A` with shape `M x K` stored in "
"packed form as `M X (K / 2)` in memory. The `%spmetadata` specifies the "
"mapping of the `K / 2` non-zero elements to the `K` elements before "
"performing the MMA operation."
msgstr ""

#: ../../../NVPTXUsage.rst:2017
msgid ""
"`tcgen05.mma.block_scale` perform matrix multiplication with block scaling "
"`D = (A * scale_A)  * (B * scale_B) + D` where scaling of input matrices "
"from memory to form the matrix `A` and matrix `B` before performing the MMA "
"operation. Scale factors for `A` and `B` matrices need to be duplicated to "
"all 32 lane partitions of tensor memory. The shape of `%scale_a` and "
"`%scale_b` matrices depend on the `.scale_vectorsize` described in `here "
"<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-"
"valid-comb>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2024
msgid ""
"The sparsity metadata (`%spmetadata`) as well as the block-scale inputs for "
"`A / B` matrices (`%scale_a` and `%scale_b`) reside in Tensor Memory."
msgstr ""

#: ../../../NVPTXUsage.rst:2027
msgid ""
"To facilitate opportunistic re-use of `A / B` matrix data across a sequence "
"of MMA operations, the `A/B` matrices are loaded into a collector buffer "
"(`%collector_usage_a_op_flag`, `%collector_usage_b_buffer_flag`, and "
"`%collector_usage_b_op_flag`). The flag value of the collector_usage flag in "
"the intrinsic specifies the nature of the re-use"
msgstr ""

#: ../../../NVPTXUsage.rst:2032
msgid ""
"There are three kinds of matrix descriptors used by the tcgen05 family of "
"instructions:"
msgstr ""

#: ../../../NVPTXUsage.rst:2035
msgid "Descriptor"
msgstr ""

#: ../../../NVPTXUsage.rst:2035 ../../../NVPTXUsage.rst:2677
msgid "Description"
msgstr ""

#: ../../../NVPTXUsage.rst:2035
msgid "Size (bits)"
msgstr ""

#: ../../../NVPTXUsage.rst:2037
msgid "Shared Memory Descriptor"
msgstr ""

#: ../../../NVPTXUsage.rst:2037
msgid ""
"Describes properties of multiplicand matrix in shared memory, including its "
"location within the CTA's shared memory. `PTX ISA <https://docs.nvidia.com/"
"cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2042
msgid "Instruction Descriptor"
msgstr ""

#: ../../../NVPTXUsage.rst:2042
msgid ""
"Describes shapes, types, and details of all matrices and the MMA operation. "
"`PTX ISA <https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-"
"zero-column-mask-descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2046
msgid "Zero-Column Mask Descriptor"
msgstr ""

#: ../../../NVPTXUsage.rst:2046
msgid ""
"Generates a mask specifying which columns of B matrix are zeroed in the MMA "
"operation, regardless of values in shared memory. Total mask size = N bits "
"`PTX ISA <https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-"
"instruction-descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2053
msgid ""
"`tcgen05.mma` can be used for general matrix multiplication or for "
"convolution operations. In case of convolutions, the `activations` can be "
"stored in either matrix `A` or matrix `B` while the `weights` will be stored "
"in the other matrix"
msgstr ""

#: ../../../NVPTXUsage.rst:2057
msgid ""
"`tcgen05.mma` has an optional collector qualifier to specify when an `A` or "
"`B` matrix is new to the sequence and should be loaded, unchanged within the "
"sequence and, should be reused, or the last use in the sequence and should "
"be discarded. The collector qualifier is used to give the TensorCore "
"permission to reuse a previously loaded `A` or `B` matrix; however reuse is "
"opportunistic in that the TensorCore may reload a matrix even when it has "
"permission to reuse that matrix. Thus, the source memory of an A or B matrix "
"must not be modified while the MMA instruction using those matrices has not "
"completed - regardless of collector qualifier permissions."
msgstr ""

#: ../../../NVPTXUsage.rst:2067
msgid ""
"The `cta_group::1` specifies that the operation is performed on the Tensor "
"Memory of the executing threads CTA only. The `cta_group::2` specifies that "
"the MMA operation is performed on the Tensor Memory of the executing "
"threads CTA and its peer CTA."
msgstr ""

#: ../../../NVPTXUsage.rst:2071
msgid ""
"The vector operand `%disable_output_lane` specifies the lane(s) in the "
"Tensor Memory that should be not be updated with the resultant matrix D. "
"Elements of the vector operand disable-output-lane forms a mask where each "
"bit corresponds to a lane of the Tensor Memory, with least significant bit "
"of the first element of the vector (leftmost in syntax) corresponding to the "
"lane 0 of the Tensor Memory. If a bit in the mask is 1, then the "
"corresponding lane in the Tensor Memory for the resultant matrix D will not "
"be updated"
msgstr ""

#: ../../../NVPTXUsage.rst:2080
msgid "Intrinsic Design:"
msgstr ""

#: ../../../NVPTXUsage.rst:2082
msgid ""
"Given the broad feature set of `tcgen05.mma` instruction modeling these "
"through intrinsics is highly complex, and the following table outlines the "
"large number of intrinsics required to fully support the `tcgen05.mma` "
"instruction set."
msgstr ""

#: ../../../NVPTXUsage.rst:2087
msgid "variant"
msgstr ""

#: ../../../NVPTXUsage.rst:2087
msgid "Configuration"
msgstr ""

#: ../../../NVPTXUsage.rst:2087
msgid "Total Variants"
msgstr ""

#: ../../../NVPTXUsage.rst:2089
msgid "tcgen05.mma.shared"
msgstr ""

#: ../../../NVPTXUsage.rst:2089 ../../../NVPTXUsage.rst:2097
msgid "2 (space) x 2 (sp) x 4 (kind) x 2 (cta_group) x 4 (collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2091
msgid "tcgen05.mma.tensor.ashift"
msgstr ""

#: ../../../NVPTXUsage.rst:2091 ../../../NVPTXUsage.rst:2099
msgid "2 (sp) x 4 (kind) x 2 (cta_group) x 2 (collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2093
msgid "tcgen05.mma.scale_d"
msgstr ""

#: ../../../NVPTXUsage.rst:2093
msgid "2 (space) x 2 (sp) x 2 (kind) x 2 (cta_group) x 4 (collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2095
msgid "tcgen05.mma.scale_d.tensor.ashift"
msgstr ""

#: ../../../NVPTXUsage.rst:2095
msgid "2 (sp) x 2 (kind) x 2 (cta_group) x 2 (collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2097
msgid "tcgen05.mma.disable_output_lane"
msgstr ""

#: ../../../NVPTXUsage.rst:2099
msgid "tcgen05.mma.disable_output_lane..."
msgstr ""

#: ../../../NVPTXUsage.rst:2101 ../../../NVPTXUsage.rst:2103
#: ../../../NVPTXUsage.rst:2105
msgid "tcgen05.mma.block_scale"
msgstr ""

#: ../../../NVPTXUsage.rst:2101
msgid ""
"2 (space) x 1 (mxf4nvf4) x 2 (cta_group) x 2 (scale_vec_size) x 4 "
"(collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2103
msgid ""
"2 (space) x 1 (mxf4) x 2 (cta_group) x 2 (scale_vec_size) x 4 "
"(collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2105
msgid ""
"2 (space) x 1 (mxf8f6f4) x 2 (cta_group) x 2 (scale_vec_size) x 4 "
"(collector_usage)"
msgstr ""

#: ../../../NVPTXUsage.rst:2107
msgid "tcgen05.mma.ws"
msgstr ""

#: ../../../NVPTXUsage.rst:2107
msgid ""
"2 (space) x 2 (sp) x 4 (kind) x 2 (zero_col_mask) x 4 (collector_usage_op) x "
"4 (collector_buffer)"
msgstr ""

#: ../../../NVPTXUsage.rst:2107
msgid "256"
msgstr ""

#: ../../../NVPTXUsage.rst:2109
msgid "Total"
msgstr ""

#: ../../../NVPTXUsage.rst:2109
msgid "816"
msgstr ""

#: ../../../NVPTXUsage.rst:2113
msgid ""
"To reduce the number of possible intrinsic variations, we've modeled the "
"`tcgen05.mma` instructions using flag operands. We've added range checks to "
"these flags to prevent invalid values. We also expanded some flags back into "
"intrinsic modifiers to avoid supporting invalid combinations of features."
msgstr ""

#: ../../../NVPTXUsage.rst:2120
msgid "'``llvm.nvvm.tcgen05.mma.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:2145
msgid ""
"`nvvm.tcgen05.mma` is an asynchronous intrinsic which initiates an `M x N x "
"K` matrix multiply and accumulate operation, `D = A * B + D` where the `A` "
"matrix is `M x K`, the `B` matrix is `K x N`, and the `D` matrix is `M x N`. "
"The operation of the form `D = A*B` is issued when the input predicate "
"argument `%enable_inp_d` is false. The optional immediate argument "
"`%scale_d_imm` can be specified to scale the input matrix `D` as follows: `D "
"= A * B + D * (2 ^ - %scale_d_imm)`. The valid range of values for argument "
"`%scale_d_imm` is `[0, 15]`. The 32-bit register operand idesc is the "
"instruction descriptor as described in `Instruction descriptor <https://docs."
"nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2154
msgid ""
"`nvvm.tcgen05.mma` has single thread semantics, unlike the collective "
"instructions `nvvm.mma.sync` or the PTX `wgmma.mma_async` instruction. So, a "
"single thread issuing the `nvvm.tcgen05.mma` will result in the initiation "
"of the whole matrix and accumulate operation"
msgstr ""

#: ../../../NVPTXUsage.rst:2159
msgid ""
"When `.sp` is specifed, the dimension of A matrix is `M x (K/2)` and "
"requires specifiying an additional `%spmetadata` argument"
msgstr ""

#: ../../../NVPTXUsage.rst:2162 ../../../NVPTXUsage.rst:2320
msgid ""
"`.ashift` shifts the rows of the A matrix down by one row, except for the "
"last row in the Tensor Memory. `.ashift` is only allowed with M = 128 or M = "
"256."
msgstr ""

#: ../../../NVPTXUsage.rst:2165 ../../../NVPTXUsage.rst:2322
msgid ""
"The `%collector_usage_a_op_flag` flag specifies the usage of collector "
"buffer for matrix `A`. It is illegal to specify either of `USE` or `FILL` "
"for `%collector_usage_a_op_flag` along with `.ashift`"
msgstr ""

#: ../../../NVPTXUsage.rst:2169 ../../../NVPTXUsage.rst:2253
#: ../../../NVPTXUsage.rst:2324
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-mma-instructions-mma>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2172 ../../../NVPTXUsage.rst:2256
#: ../../../NVPTXUsage.rst:2326 ../../../NVPTXUsage.rst:2397
msgid ""
"The following tables describes the possible values of the flag arguments"
msgstr ""

#: ../../../NVPTXUsage.rst:2174
msgid "`%kind_flag` flag:"
msgstr ""

#: ../../../NVPTXUsage.rst:2177 ../../../NVPTXUsage.rst:2331
#: ../../../NVPTXUsage.rst:2402
msgid "`kind_flag`"
msgstr ""

#: ../../../NVPTXUsage.rst:2177 ../../../NVPTXUsage.rst:2188
#: ../../../NVPTXUsage.rst:2197 ../../../NVPTXUsage.rst:2261
#: ../../../NVPTXUsage.rst:2270 ../../../NVPTXUsage.rst:2331
#: ../../../NVPTXUsage.rst:2342 ../../../NVPTXUsage.rst:2351
#: ../../../NVPTXUsage.rst:2402 ../../../NVPTXUsage.rst:2413
#: ../../../NVPTXUsage.rst:2424
msgid "value"
msgstr ""

#: ../../../NVPTXUsage.rst:2179 ../../../NVPTXUsage.rst:2333
#: ../../../NVPTXUsage.rst:2404
msgid "F16"
msgstr ""

#: ../../../NVPTXUsage.rst:2181 ../../../NVPTXUsage.rst:2335
#: ../../../NVPTXUsage.rst:2406
msgid "F8F6F4"
msgstr ""

#: ../../../NVPTXUsage.rst:2182 ../../../NVPTXUsage.rst:2336
#: ../../../NVPTXUsage.rst:2407
msgid "I8"
msgstr ""

#: ../../../NVPTXUsage.rst:2185
msgid "`%cta_group_flag` flag:"
msgstr ""

#: ../../../NVPTXUsage.rst:2188 ../../../NVPTXUsage.rst:2342
msgid "`cta_group_flag`"
msgstr ""

#: ../../../NVPTXUsage.rst:2190 ../../../NVPTXUsage.rst:2263
#: ../../../NVPTXUsage.rst:2344
msgid "CG1"
msgstr ""

#: ../../../NVPTXUsage.rst:2191 ../../../NVPTXUsage.rst:2264
#: ../../../NVPTXUsage.rst:2345
msgid "CG2"
msgstr ""

#: ../../../NVPTXUsage.rst:2194
msgid "`%collector_usage_a_op_flag` flag:"
msgstr ""

#: ../../../NVPTXUsage.rst:2197 ../../../NVPTXUsage.rst:2270
#: ../../../NVPTXUsage.rst:2351
msgid "`collector_usage_a_op_flag`"
msgstr ""

#: ../../../NVPTXUsage.rst:2199 ../../../NVPTXUsage.rst:2272
#: ../../../NVPTXUsage.rst:2353 ../../../NVPTXUsage.rst:2426
msgid "DISCARD"
msgstr ""

#: ../../../NVPTXUsage.rst:2200 ../../../NVPTXUsage.rst:2273
#: ../../../NVPTXUsage.rst:2354 ../../../NVPTXUsage.rst:2427
msgid "LASTUSE"
msgstr ""

#: ../../../NVPTXUsage.rst:2201 ../../../NVPTXUsage.rst:2274
#: ../../../NVPTXUsage.rst:2355 ../../../NVPTXUsage.rst:2428
msgid "USE"
msgstr ""

#: ../../../NVPTXUsage.rst:2202 ../../../NVPTXUsage.rst:2275
#: ../../../NVPTXUsage.rst:2356 ../../../NVPTXUsage.rst:2429
msgid "FILL"
msgstr ""

#: ../../../NVPTXUsage.rst:2206
msgid "'``llvm.nvvm.tcgen05.mma.block_scale*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:2245
msgid ""
"`nvvm.tcgen05.mma.block_scale` is an asynchronous intrinsic which initiates "
"an `M x N x K` matrix multiply and accumulate operation, `D = (A * scale_a)  "
"* (B * scale_b) + D` where the `A` matrix is `M x K`, the `B` matrix is `K x "
"N`, and the `D` matrix is `M x N`. The matrices `A` and `B` are scaled with "
"`%scale_A` and `%scale_B` matrices respectively before performing the matrix "
"multiply and accumulate operation. The operation of the form `D = A*B` is "
"issued when the input predicate argument `%enable_inp_d` is false. The 32-"
"bit register operand idesc is the instruction descriptor as described in "
"`Instruction descriptor <https://docs.nvidia.com/cuda/parallel-thread-"
"execution/#tcgen05-instruction-descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2247
msgid ""
"`nvvm.tcgen05.mma.block_scale` has single thread semantics, unlike the "
"collective instructions `nvvm.mma.sync` or the PTX `wgmma.mma_async` "
"instruction. So, a single thread issuing the `nvvm.tcgen05.mma.block_scale` "
"will result in the initiation of the whole matrix multiply and accumulate "
"operation"
msgstr ""

#: ../../../NVPTXUsage.rst:2249 ../../../NVPTXUsage.rst:2318
#: ../../../NVPTXUsage.rst:2388
msgid ""
"When `.sp` is specifed, the dimension of A matrix is `M x (K / 2)` and "
"requires specifiying an additional `%spmetadata` argument"
msgstr ""

#: ../../../NVPTXUsage.rst:2251
msgid ""
"The `%collector_usage_a_op_flag` flag specifies the usage of collector "
"buffer for matrix `A`"
msgstr ""

#: ../../../NVPTXUsage.rst:2258
msgid "`%cta_group`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2261
msgid "`cta_group`"
msgstr ""

#: ../../../NVPTXUsage.rst:2267 ../../../NVPTXUsage.rst:2348
msgid "`%collector_usage_a_op_flag`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2279
msgid "'``llvm.nvvm.tcgen05.mma.disable_output_lane*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:2312
msgid ""
"`nvvm.tcgen05.mma.disable_output_lane` is an asynchronous intrinsic which "
"initiates an `M x N x K` matrix multiply and accumulate operation, `D = A * "
"B + D` where the `A` matrix is `M x K`, the `B` matrix is `K x N`, and the "
"`D` matrix is `M x N`. The operation of the form `D = A*B` is issued when "
"the input predicate argument `%enable_inp_d` is false. The optional "
"immediate argument `%scale_d_imm` can be specified to scale the input matrix "
"`D` as follows: `D = A*B+D * (2 ^ - %scale_d_imm)`. The valid range of "
"values for argument `%scale_d_imm` is `[0, 15]`. The 32-bit register operand "
"idesc is the instruction descriptor as described in `Instruction descriptor "
"<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-"
"descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2314
msgid ""
"The vector operand `%disable_output_lane` specifies the lane(s) in the "
"Tensor Memory that should be not be updated with the resultant matrix `D`. "
"Elements of the vector operand `%disable_output_lane` forms a mask where "
"each bit corresponds to a lane of the Tensor Memory, with least significant "
"bit of the first element of the vector corresponding to the `lane 0` of the "
"Tensor Memory. If a bit in the mask is 1, then the corresponding lane in the "
"Tensor Memory for the resultant matrix `D` will not be updated"
msgstr ""

#: ../../../NVPTXUsage.rst:2316
msgid ""
"`nvvm.tcgen05.mma.disable_output_lane` has single thread semantics, unlike "
"the collective instructions `nvvm.mma.sync` or the PTX `wgmma.mma_async` "
"instruction. So, a single thread issuing the `nvvm.tcgen05.mma."
"disable_output_lane` will result in the initiation of the whole matrix "
"multiply and accumulate operation"
msgstr ""

#: ../../../NVPTXUsage.rst:2328 ../../../NVPTXUsage.rst:2399
msgid "`%kind_flag`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2339
msgid "`%cta_group_flag`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2361
msgid "'``llvm.nvvm.tcgen05.mma.ws*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:2384
msgid ""
"`nvvm.tcgen05.mma.ws` is an asynchronous intrinsic which initiates an `M x N "
"x K` weight stationary convolution matrix multiply and accumulate operation, "
"`D = A * B + D` where the `A` matrix is `M x K`, the `B` matrix is `K x N`, "
"and the `D` matrix is `M x N`. The operation of the form `D = A*B` is issued "
"when the input predicate argument `%enable_inp_d` is false. The optional "
"immediate argument `%scale_d_imm` can be specified to scale the input matrix "
"`D` as follows: `D = A*B+D * (2 ^ - %scale_d_imm)`. The valid range of "
"values for argument `%scale_d_imm` is `[0, 15]`. The 32-bit register operand "
"idesc is the instruction descriptor as described in `Instruction descriptor "
"<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-"
"descriptor>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2386
msgid ""
"`nvvm.tcgen05.mma` has single thread semantics, unlike the collective "
"instructions `nvvm.mma.sync` or the PTX `wgmma.mma_async` instruction. So, a "
"single thread issuing the `nvvm.tcgen05.mma` will result in the initiation "
"of the whole matrix multiply and accumulate operation"
msgstr ""

#: ../../../NVPTXUsage.rst:2390
msgid ""
"The operand `%zero_col_mask` is a 64-bit register which specifies the `Zero-"
"Column Mask Descriptor <https://docs.nvidia.com/cuda/parallel-thread-"
"execution/#tcgen05-zero-column-mask-descriptor>`__. The zero-column mask "
"descriptor is used to generate a mask that specifies which columns of `B` "
"matrix will have zero value for the matrix multiply and accumulate operation "
"regardless of the values present in the shared memory."
msgstr ""

#: ../../../NVPTXUsage.rst:2392
msgid ""
"The `%collector_usage_b_buffer_flag` and `%collector_usage_b_op_flag` "
"together flag specifies the usage of collector buffer for Matrix `B`"
msgstr ""

#: ../../../NVPTXUsage.rst:2394
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-mma-instructions-mma-ws>`__"
msgstr ""

#: ../../../NVPTXUsage.rst:2410
msgid "`%collector_usage_b_buffer_flag`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2413
msgid "`collector_usage_b_buffer_flag`"
msgstr ""

#: ../../../NVPTXUsage.rst:2415
msgid "B0"
msgstr ""

#: ../../../NVPTXUsage.rst:2416
msgid "B1"
msgstr ""

#: ../../../NVPTXUsage.rst:2417
msgid "B2"
msgstr ""

#: ../../../NVPTXUsage.rst:2418
msgid "B3"
msgstr ""

#: ../../../NVPTXUsage.rst:2421
msgid "`%collector_usage_b_op_flag`:"
msgstr ""

#: ../../../NVPTXUsage.rst:2424
msgid "`collector_usage_b_op_flag`"
msgstr ""

#: ../../../NVPTXUsage.rst:2433
msgid "Store Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2436
msgid "'``llvm.nvvm.st.bulk.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:2449
msgid ""
"The '``@llvm.nvvm.st.bulk.*``' intrinsics initialize a region of shared "
"memory starting from the location specified by the destination address "
"operand `%dst`."
msgstr ""

#: ../../../NVPTXUsage.rst:2452
msgid ""
"The integer operand `%size` specifies the amount of memory to be initialized "
"in terms of number of bytes and must be a multiple of 8. Otherwise, the "
"behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:2456
msgid ""
"The integer immediate operand `%initval` specifies the initialization value "
"for the memory locations. The only numeric value allowed is 0."
msgstr ""

#: ../../../NVPTXUsage.rst:2459
msgid ""
"The ``@llvm.nvvm.st.bulk.shared.cta`` and ``@llvm.nvvm.st.bulk`` intrinsics "
"are similar but the latter uses generic addressing (see `Generic Addressing "
"<https://docs.nvidia.com/cuda/parallel-thread-execution/#generic-"
"addressing>`__)."
msgstr ""

#: ../../../NVPTXUsage.rst:2462
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-st-bulk>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:2466
msgid "clusterlaunchcontrol Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2469
msgid "'``llvm.nvvm.clusterlaunchcontrol.try_cancel*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2482
msgid ""
"The ``clusterlaunchcontrol.try_cancel`` intrinsics requests atomically "
"cancelling the launch of a cluster that has not started running yet. It "
"asynchronously non-atomically writes a 16-byte opaque response to shared "
"memory, pointed to by 16-byte-aligned ``addr`` indicating whether the "
"operation succeeded or failed. ``addr`` and 8-byte-aligned ``mbar`` must "
"refer to ``shared::cta`` otherwise the behavior is undefined. The completion "
"of the asynchronous operation is tracked using the mbarrier completion "
"mechanism at ``.cluster`` scope referenced by the shared memory pointer, "
"``mbar``. On success, the opaque response contains the CTA id of the first "
"CTA of the canceled cluster; no other successful response from other "
"``clusterlaunchcontrol.try_cancel`` operations from the same grid will "
"contain that id."
msgstr ""

#: ../../../NVPTXUsage.rst:2493
msgid ""
"The ``multicast`` variant specifies that the response is asynchronously non-"
"atomically written to the corresponding shared memory location of each CTA "
"in the requesting cluster. The completion of the write of each local "
"response is tracked by independent mbarriers at the corresponding shared "
"memory location of each CTA in the cluster."
msgstr ""

#: ../../../NVPTXUsage.rst:2499
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/?a#parallel-synchronization-and-communication-instructions-"
"clusterlaunchcontrol-try-cancel>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:2502
msgid "'``llvm.nvvm.clusterlaunchcontrol.query_cancel.is_canceled``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:2514
msgid ""
"The ``llvm.nvvm.clusterlaunchcontrol.query_cancel.is_canceled`` intrinsic "
"decodes the opaque response written by the ``llvm.nvvm.clusterlaunchcontrol."
"try_cancel`` operation."
msgstr ""

#: ../../../NVPTXUsage.rst:2517
msgid ""
"The intrinsic returns ``0`` (false) if the request failed. If the request "
"succeeded, it returns ``1`` (true). A true result indicates that:"
msgstr ""

#: ../../../NVPTXUsage.rst:2520
msgid ""
"the thread block cluster whose first CTA id matches that of the response "
"handle will not run, and"
msgstr ""

#: ../../../NVPTXUsage.rst:2522
msgid ""
"no other successful response of another ``try_cancel`` request in the grid "
"will contain the first CTA id of that cluster"
msgstr ""

#: ../../../NVPTXUsage.rst:2525 ../../../NVPTXUsage.rst:2554
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/?a#parallel-synchronization-and-communication-instructions-"
"clusterlaunchcontrol-query-cancel>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:2529
msgid ""
"'``llvm.nvvm.clusterlaunchcontrol.query_cancel.get_first_ctaid.*``' "
"Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2543
msgid ""
"The ``clusterlaunchcontrol.query_cancel.get_first_ctaid.*`` intrinsic can be "
"used to decode the successful opaque response written by the ``llvm.nvvm."
"clusterlaunchcontrol.try_cancel`` operation."
msgstr ""

#: ../../../NVPTXUsage.rst:2547
msgid "If the request succeeded:"
msgstr ""

#: ../../../NVPTXUsage.rst:2549
msgid ""
"``llvm.nvvm.clusterlaunchcontrol.query_cancel.get_first_ctaid.{x,y,z}`` "
"returns the coordinate of the first CTA in the canceled cluster, either x, "
"y, or z."
msgstr ""

#: ../../../NVPTXUsage.rst:2552
msgid "If the request failed, the behavior of these intrinsics is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:2557
msgid "Perf Monitor Event Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2560
msgid "'``llvm.nvvm.pm.event.mask``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:2572
msgid ""
"The '``llvm.nvvm.pm.event.mask``' intrinsic triggers one or more performance "
"monitor events. Each bit in the 16-bit immediate operand ``%mask_val`` "
"controls an event."
msgstr ""

#: ../../../NVPTXUsage.rst:2576
msgid ""
"For more information on the pmevent instructions, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/index."
"html#miscellaneous-instructions-pmevent>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:2580
msgid "Other Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2582
msgid ""
"For the full set of NVPTX intrinsics, please see the ``include/llvm/IR/"
"IntrinsicsNVVM.td`` file in the LLVM source tree."
msgstr ""

#: ../../../NVPTXUsage.rst:2589
msgid "Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:2591
msgid ""
"The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` "
"that implements many common mathematical functions. This library can be used "
"as a high-performance math library for any compilers using the LLVM NVPTX "
"target. The library can be found under ``nvvm/libdevice/`` in the CUDA "
"Toolkit and there is a separate version for each compute architecture."
msgstr ""

#: ../../../NVPTXUsage.rst:2597
msgid ""
"For a list of all math functions implemented in libdevice, see `libdevice "
"Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:2600
msgid ""
"To accommodate various math-related compiler flags that can affect code "
"generation of libdevice code, the library code depends on a special LLVM IR "
"pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. "
"This pass looks for calls to the ``@__nvvm_reflect`` function and replaces "
"them with constants based on the defined reflection parameters. Such "
"conditional code often follows a pattern:"
msgstr ""

#: ../../../NVPTXUsage.rst:2616
msgid "The default value for all unspecified reflection parameters is zero."
msgstr ""

#: ../../../NVPTXUsage.rst:2618
msgid ""
"The ``NVVMReflect`` pass should be executed early in the optimization "
"pipeline, immediately after the link stage. The ``internalize`` pass is also "
"recommended to remove unused math functions from the resulting PTX. For an "
"input IR module ``module.bc``, the following compilation flow is recommended:"
msgstr ""

#: ../../../NVPTXUsage.rst:2623
msgid ""
"The ``NVVMReflect`` pass will attempt to remove dead code even without "
"optimizations. This allows potentially incompatible instructions to be "
"avoided at all optimizations levels by using the ``__CUDA_ARCH`` argument."
msgstr ""

#: ../../../NVPTXUsage.rst:2627
msgid "Save list of external functions in ``module.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:2628
msgid "Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:2629
msgid "Internalize all functions not in list from (1)"
msgstr ""

#: ../../../NVPTXUsage.rst:2630
msgid "Eliminate all unused internal functions"
msgstr ""

#: ../../../NVPTXUsage.rst:2631
msgid "Run ``NVVMReflect`` pass"
msgstr ""

#: ../../../NVPTXUsage.rst:2632
msgid "Run standard optimization pipeline"
msgstr ""

#: ../../../NVPTXUsage.rst:2636
msgid ""
"``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the "
"libdevice functions. It is possible to link two IR modules that have been "
"linked against libdevice using different reflection variables."
msgstr ""

#: ../../../NVPTXUsage.rst:2640
msgid ""
"Since the ``NVVMReflect`` pass replaces conditionals with constants, it will "
"often leave behind dead code of the form:"
msgstr ""

#: ../../../NVPTXUsage.rst:2654
msgid ""
"Therefore, it is recommended that ``NVVMReflect`` is executed early in the "
"optimization pipeline before dead-code elimination."
msgstr ""

#: ../../../NVPTXUsage.rst:2657
msgid ""
"The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the "
"beginning of your pass manager; just use the following code when setting up "
"your pass manager and the PassBuilder will use "
"``registerPassBuilderCallbacks`` to let NVPTXTargetMachine::"
"registerPassBuilderCallbacks add the pass to the pass manager:"
msgstr ""

#: ../../../NVPTXUsage.rst:2671
msgid "Reflection Parameters"
msgstr ""

#: ../../../NVPTXUsage.rst:2673
msgid ""
"The libdevice library currently uses the following reflection parameters to "
"control code generation:"
msgstr ""

#: ../../../NVPTXUsage.rst:2677
msgid "Flag"
msgstr ""

#: ../../../NVPTXUsage.rst:2679
msgid "``__CUDA_FTZ=[0,1]``"
msgstr ""

#: ../../../NVPTXUsage.rst:2679
msgid "Use optimized code paths that flush subnormals to zero"
msgstr ""

#: ../../../NVPTXUsage.rst:2682
msgid ""
"The value of this flag is determined by the \"nvvm-reflect-ftz\" module "
"flag. The following sets the ftz flag to 1."
msgstr ""

#: ../../../NVPTXUsage.rst:2690
msgid ""
"(``i32 4`` indicates that the value set here overrides the value in another "
"module we link with.  See the `LangRef <LangRef.html#module-flags-metadata>` "
"for details.)"
msgstr ""

#: ../../../NVPTXUsage.rst:2695
msgid "Executing PTX"
msgstr ""

#: ../../../NVPTXUsage.rst:2697
msgid ""
"The most common way to execute PTX assembly on a GPU device is to use the "
"CUDA Driver API. This API is a low-level interface to the GPU driver and "
"allows for JIT compilation of PTX code to native GPU machine code."
msgstr ""

#: ../../../NVPTXUsage.rst:2701
msgid "Initializing the Driver API:"
msgstr ""

#: ../../../NVPTXUsage.rst:2715
msgid "JIT compiling a PTX string to a device binary:"
msgstr ""

#: ../../../NVPTXUsage.rst:2728
msgid ""
"For full examples of executing PTX assembly, please see the `CUDA Samples "
"<https://developer.nvidia.com/cuda-downloads>`_ distribution."
msgstr ""

#: ../../../NVPTXUsage.rst:2733
msgid "Common Issues"
msgstr ""

#: ../../../NVPTXUsage.rst:2736
msgid "ptxas complains of undefined function: __nvvm_reflect"
msgstr ""

#: ../../../NVPTXUsage.rst:2738
msgid ""
"When linking with libdevice, the ``NVVMReflect`` pass must be used. See :ref:"
"`libdevice` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:2743
msgid "Tutorial: A Simple Compute Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2745
msgid ""
"To start, let us take a look at a simple compute kernel written directly in "
"LLVM IR. The kernel implements vector addition, where each thread computes "
"one element of the output vector C from the input vectors A and B.  To make "
"this easier, we also assume that only a single CTA (thread block) will be "
"launched, and that it will be one dimensional."
msgstr ""

#: ../../../NVPTXUsage.rst:2753
msgid "The Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2789
msgid ""
"We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:"
msgstr ""

#: ../../../NVPTXUsage.rst:2798
msgid ""
"If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32`` "
"in the module data layout string and use ``nvptx-nvidia-cuda`` as the target "
"triple."
msgstr ""

#: ../../../NVPTXUsage.rst:2803
msgid "The output we get from ``llc`` (as of LLVM 3.4):"
msgstr ""

#: ../../../NVPTXUsage.rst:2845
msgid "Dissecting the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2847
msgid "Now let us dissect the LLVM IR that makes up this kernel."
msgstr ""

#: ../../../NVPTXUsage.rst:2850
msgid "Data Layout"
msgstr ""

#: ../../../NVPTXUsage.rst:2852
msgid ""
"The data layout string determines the size in bits of common data types, "
"their ABI alignment, and their storage size.  For NVPTX, you should use one "
"of the following:"
msgstr ""

#: ../../../NVPTXUsage.rst:2856
msgid "32-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:2862
msgid "64-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:2870
msgid "Target Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2872
msgid ""
"In this example, we use the ``@llvm.nvvm.read.ptx.sreg.tid.x`` intrinsic to "
"read the X component of the current thread's ID, which corresponds to a read "
"of register ``%tid.x`` in PTX. The NVPTX back-end supports a large set of "
"intrinsics.  A short list is shown below; please see ``include/llvm/IR/"
"IntrinsicsNVVM.td`` for the full list."
msgstr ""

#: ../../../NVPTXUsage.rst:2880
msgid "Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:2880
msgid "CUDA Equivalent"
msgstr ""

#: ../../../NVPTXUsage.rst:2882
msgid "``i32 @llvm.nvvm.read.ptx.sreg.tid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2882
msgid "threadIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2883
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2883
msgid "blockIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2884
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ntid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2884
msgid "blockDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2885
msgid "``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2885
msgid "gridDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2886
msgid "``void @llvm.nvvm.barrier0()``"
msgstr ""

#: ../../../NVPTXUsage.rst:2886
msgid "__syncthreads()"
msgstr ""

#: ../../../NVPTXUsage.rst:2893
msgid ""
"You may have noticed that all of the pointer types in the LLVM IR example "
"had an explicit address space specifier. What is address space 1? NVIDIA GPU "
"devices (generally) have four types of memory:"
msgstr ""

#: ../../../NVPTXUsage.rst:2897
msgid "Global: Large, off-chip memory"
msgstr ""

#: ../../../NVPTXUsage.rst:2898
msgid "Shared: Small, on-chip memory shared among all threads in a CTA"
msgstr ""

#: ../../../NVPTXUsage.rst:2899
msgid "Local: Per-thread, private memory"
msgstr ""

#: ../../../NVPTXUsage.rst:2900
msgid "Constant: Read-only memory shared across all threads"
msgstr ""

#: ../../../NVPTXUsage.rst:2902
msgid ""
"These different types of memory are represented in LLVM IR as address "
"spaces. There is also a fifth address space used by the NVPTX code generator "
"that corresponds to the \"generic\" address space.  This address space can "
"represent addresses in any other address space (with a few exceptions).  "
"This allows users to write IR functions that can load/store memory using the "
"same instructions. Intrinsics are provided to convert pointers between the "
"generic and non-generic address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:2910
msgid ""
"See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:2914
msgid "Running the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2916
msgid ""
"Generating PTX from LLVM IR is all well and good, but how do we execute it "
"on a real GPU device? The CUDA Driver API provides a convenient mechanism "
"for loading and JIT compiling PTX to a native GPU device, and launching a "
"kernel. The API is similar to OpenCL.  A simple example showing how to load "
"and execute our vector addition code is shown below. Note that for brevity "
"this code does not perform much error checking!"
msgstr ""

#: ../../../NVPTXUsage.rst:2925
msgid ""
"You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline "
"compile PTX to machine code (SASS) for a specific GPU architecture. Such "
"binaries can be loaded by the CUDA Driver API in the same way as PTX. This "
"can be useful for reducing startup time by precompiling the PTX kernels."
msgstr ""

#: ../../../NVPTXUsage.rst:3054
msgid ""
"You will need to link with the CUDA driver and specify the path to cuda.h."
msgstr ""

#: ../../../NVPTXUsage.rst:3060
msgid ""
"We don't need to specify a path to ``libcuda.so`` since this is installed in "
"a system location by the driver, not the CUDA toolkit."
msgstr ""

#: ../../../NVPTXUsage.rst:3063
msgid ""
"If everything goes as planned, you should see the following output when "
"running the compiled program:"
msgstr ""

#: ../../../NVPTXUsage.rst:3091
msgid ""
"You will likely see a different device identifier based on your hardware"
msgstr ""

#: ../../../NVPTXUsage.rst:3095
msgid "Tutorial: Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:3097
msgid ""
"In this tutorial, we show a simple example of linking LLVM IR with the "
"libdevice library. We will use the same kernel as the previous tutorial, "
"except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``. "
"Libdevice provides an ``__nv_powf`` function that we will use."
msgstr ""

#: ../../../NVPTXUsage.rst:3138
msgid "To compile this kernel, we perform the following steps:"
msgstr ""

#: ../../../NVPTXUsage.rst:3140
msgid "Link with libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:3141
msgid "Internalize all but the public kernel function"
msgstr ""

#: ../../../NVPTXUsage.rst:3142
msgid "Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0"
msgstr ""

#: ../../../NVPTXUsage.rst:3143
msgid "Optimize the linked module"
msgstr ""

#: ../../../NVPTXUsage.rst:3144
msgid "Codegen the module"
msgstr ""

#: ../../../NVPTXUsage.rst:3147
msgid ""
"These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc`` "
"tools. In a complete compiler, these steps can also be performed entirely "
"programmatically by setting up an appropriate pass configuration (see :ref:"
"`libdevice`)."
msgstr ""

#: ../../../NVPTXUsage.rst:3160
msgid ""
"The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any "
"undefined variables will default to zero. It is shown here for evaluation "
"purposes."
msgstr ""

#: ../../../NVPTXUsage.rst:3165
msgid "This gives us the following PTX (excerpt):"
msgstr ""
