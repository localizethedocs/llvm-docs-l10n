# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 21\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-28 09:16+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../NVPTXUsage.rst:3
msgid "User Guide for NVPTX Back-end"
msgstr ""

#: ../../../NVPTXUsage.rst:11
msgid "Introduction"
msgstr ""

#: ../../../NVPTXUsage.rst:13
msgid ""
"To support GPU programming, the NVPTX back-end supports a subset of LLVM IR "
"along with a defined set of conventions used to represent GPU programming "
"concepts. This document provides an overview of the general usage of the "
"back- end, including a description of the conventions used and the set of "
"accepted LLVM IR."
msgstr ""

#: ../../../NVPTXUsage.rst:21
msgid ""
"This document assumes a basic familiarity with CUDA and the PTX assembly "
"language. Information about the CUDA Driver API and the PTX assembly "
"language can be found in the `CUDA documentation <http://docs.nvidia.com/"
"cuda/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:29
msgid "Conventions"
msgstr ""

#: ../../../NVPTXUsage.rst:32
msgid "Marking Functions as Kernels"
msgstr ""

#: ../../../NVPTXUsage.rst:34
msgid ""
"In PTX, there are two types of functions: *device functions*, which are only "
"callable by device code, and *kernel functions*, which are callable by host "
"code. By default, the back-end will emit device functions. The "
"``ptx_kernel`` calling convention is used to declare a function as a kernel "
"function."
msgstr ""

#: ../../../NVPTXUsage.rst:39
msgid ""
"The following example shows a kernel function calling a device function in "
"LLVM IR. The function ``@my_kernel`` is callable from host code, but "
"``@my_fmad`` is not."
msgstr ""

#: ../../../NVPTXUsage.rst:58
msgid "When compiled, the PTX kernel functions are callable by host-side code."
msgstr ""

#: ../../../NVPTXUsage.rst:63
msgid "Function Attributes"
msgstr ""

#: ../../../NVPTXUsage.rst:67
msgid "``\"nvvm.maxclusterrank\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:66
msgid ""
"This attribute specifies the maximum number of blocks per cluster. Must be "
"non-zero. Only supported for Hopper+."
msgstr ""

#: ../../../NVPTXUsage.rst:71
msgid "``\"nvvm.minctasm\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:70
msgid ""
"This indicates a hint/directive to the compiler/driver, asking it to put at "
"least these many CTAs on an SM."
msgstr ""

#: ../../../NVPTXUsage.rst:75
msgid "``\"nvvm.maxnreg\"=\"<n>\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:74
msgid ""
"This attribute indicates the maximum number of registers to be used for the "
"kernel function."
msgstr ""

#: ../../../NVPTXUsage.rst:81
msgid "``\"nvvm.maxntid\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:78
msgid ""
"This attribute declares the maximum number of threads in the thread block "
"(CTA). The maximum number of threads is the product of the maximum extent in "
"each dimension. Exceeding the maximum number of threads results in a runtime "
"error or kernel launch failure."
msgstr ""

#: ../../../NVPTXUsage.rst:87
msgid "``\"nvvm.reqntid\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:84
msgid ""
"This attribute declares the exact number of threads in the thread block "
"(CTA). The number of threads is the product of the value in each dimension. "
"Specifying a different CTA dimension at launch will result in a runtime "
"error or kernel launch failure."
msgstr ""

#: ../../../NVPTXUsage.rst:93
msgid "``\"nvvm.cluster_dim\"=\"<x>[,<y>[,<z>]]\"``"
msgstr ""

#: ../../../NVPTXUsage.rst:90
msgid ""
"This attribute declares the number of thread blocks (CTAs) in the cluster. "
"The total number of CTAs is the product of the number of CTAs in each "
"dimension. Specifying a different cluster dimension at launch will result in "
"a runtime error or kernel launch failure. Only supported for Hopper+."
msgstr ""

#: ../../../NVPTXUsage.rst:98 ../../../NVPTXUsage.rst:2267
msgid "Address Spaces"
msgstr ""

#: ../../../NVPTXUsage.rst:100
msgid "The NVPTX back-end uses the following address space mapping:"
msgstr ""

#: ../../../NVPTXUsage.rst:103
msgid "Address Space"
msgstr ""

#: ../../../NVPTXUsage.rst:103
msgid "Memory Space"
msgstr ""

#: ../../../NVPTXUsage.rst:105 ../../../NVPTXUsage.rst:786
#: ../../../NVPTXUsage.rst:794 ../../../NVPTXUsage.rst:802
#: ../../../NVPTXUsage.rst:810 ../../../NVPTXUsage.rst:818
#: ../../../NVPTXUsage.rst:826
msgid "0"
msgstr ""

#: ../../../NVPTXUsage.rst:105
msgid "Generic"
msgstr ""

#: ../../../NVPTXUsage.rst:106 ../../../NVPTXUsage.rst:788
#: ../../../NVPTXUsage.rst:796 ../../../NVPTXUsage.rst:804
#: ../../../NVPTXUsage.rst:812 ../../../NVPTXUsage.rst:820
#: ../../../NVPTXUsage.rst:828 ../../../NVPTXUsage.rst:1748
msgid "1"
msgstr ""

#: ../../../NVPTXUsage.rst:106
msgid "Global"
msgstr ""

#: ../../../NVPTXUsage.rst:107 ../../../NVPTXUsage.rst:790
#: ../../../NVPTXUsage.rst:798 ../../../NVPTXUsage.rst:806
#: ../../../NVPTXUsage.rst:814 ../../../NVPTXUsage.rst:822
#: ../../../NVPTXUsage.rst:830 ../../../NVPTXUsage.rst:1748
#: ../../../NVPTXUsage.rst:1749
msgid "2"
msgstr ""

#: ../../../NVPTXUsage.rst:107
msgid "Internal Use"
msgstr ""

#: ../../../NVPTXUsage.rst:108 ../../../NVPTXUsage.rst:792
#: ../../../NVPTXUsage.rst:800 ../../../NVPTXUsage.rst:808
#: ../../../NVPTXUsage.rst:816 ../../../NVPTXUsage.rst:824
#: ../../../NVPTXUsage.rst:832
msgid "3"
msgstr ""

#: ../../../NVPTXUsage.rst:108
msgid "Shared"
msgstr ""

#: ../../../NVPTXUsage.rst:109 ../../../NVPTXUsage.rst:1748
#: ../../../NVPTXUsage.rst:1749 ../../../NVPTXUsage.rst:1750
msgid "4"
msgstr ""

#: ../../../NVPTXUsage.rst:109
msgid "Constant"
msgstr ""

#: ../../../NVPTXUsage.rst:110
msgid "5"
msgstr ""

#: ../../../NVPTXUsage.rst:110
msgid "Local"
msgstr ""

#: ../../../NVPTXUsage.rst:111
msgid "7"
msgstr ""

#: ../../../NVPTXUsage.rst:111
msgid "Shared Cluster"
msgstr ""

#: ../../../NVPTXUsage.rst:114
msgid ""
"Every global variable and pointer type is assigned to one of these address "
"spaces, with 0 being the default address space. Intrinsics are provided "
"which can be used to convert pointers between the generic and non-generic "
"address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:119
msgid ""
"As an example, the following IR will define an array ``@g`` that resides in "
"global device memory."
msgstr ""

#: ../../../NVPTXUsage.rst:126
msgid ""
"LLVM IR functions can read and write to this array, and host-side code can "
"copy data to it by name with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:129
msgid ""
"Note that since address space 0 is the generic space, it is illegal to have "
"global variables in address space 0.  Address space 0 is the default address "
"space in LLVM, so the ``addrspace(N)`` annotation is *required* for global "
"variables."
msgstr ""

#: ../../../NVPTXUsage.rst:136
msgid "Triples"
msgstr ""

#: ../../../NVPTXUsage.rst:138
msgid ""
"The NVPTX target uses the module triple to select between 32/64-bit code "
"generation and the driver-compiler interface to use. The triple architecture "
"can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The "
"operating system should be one of ``cuda`` or ``nvcl``, which determines the "
"interface used by the generated code to communicate with the driver.  Most "
"users will want to use ``cuda`` as the operating system, which makes the "
"generated PTX compatible with the CUDA Driver API."
msgstr ""

#: ../../../NVPTXUsage.rst:146
msgid "Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:148
msgid "Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``"
msgstr ""

#: ../../../NVPTXUsage.rst:153
msgid "NVPTX Architecture Hierarchy and Ordering"
msgstr ""

#: ../../../NVPTXUsage.rst:155
msgid ""
"GPU architectures: sm_2Y/sm_3Y/sm_5Y/sm_6Y/sm_7Y/sm_8Y/sm_9Y/sm_10Y/sm_12Y "
"('Y' represents version within the architecture) The architectures have name "
"of form ``sm_XYz`` where ``X`` represent the generation number, ``Y`` "
"represents the version within the architecture, and ``z`` represents the "
"optional feature suffix. If ``X1Y1 <= X2Y2``, then GPU capabilities of "
"``sm_X1Y1`` are included in ``sm_X2Y2``. For example, take ``sm_90`` (9 "
"represents ``X``, 0 represents ``Y``, and no feature suffix) and ``sm_103`` "
"architectures (10 represents ``X``, 3 represents ``Y``, and no feature "
"suffix). Since 90 <= 103, ``sm_90`` is compatible with ``sm_103``."
msgstr ""

#: ../../../NVPTXUsage.rst:165
msgid ""
"The family-specific variants have ``f`` feature suffix and they follow "
"following order: ``sm_X{Y2}f > sm_X{Y1}f`` iff ``Y2 > Y1`` ``sm_XY{f} > "
"sm_{XY}{}``"
msgstr ""

#: ../../../NVPTXUsage.rst:170
msgid ""
"For example, take ``sm_100f`` (10 represents ``X``, 0 represents ``Y``, and "
"``f`` represents ``z``) and ``sm_103f`` (10 represents ``X``, 3 represents "
"``Y``, and ``f`` represents ``z``) architecture variants. Since ``Y1 < Y2``, "
"``sm_100f`` is compatible with ``sm_103f``. Similarly based on the second "
"rule, ``sm_90`` is compatible with ``sm_103f``."
msgstr ""

#: ../../../NVPTXUsage.rst:175
msgid ""
"Some counter examples, take ``sm_100f`` and ``sm_120f`` (12 represents "
"``X``, 0 represents ``Y``, and ``f`` represents ``z``) architecture "
"variants. Since both belongs to different family i.e. ``X1 != X2``, "
"``sm_100f`` is not compatible with ``sm_120f``."
msgstr ""

#: ../../../NVPTXUsage.rst:180
msgid ""
"The architecture-specific variants have ``a`` feature suffix and they follow "
"following order: ``sm_XY{a} > sm_XY{f} > sm_{XY}{}``"
msgstr ""

#: ../../../NVPTXUsage.rst:184
msgid ""
"For example, take ``sm_103a`` (10 represents ``X``, 3 represents ``Y``, and "
"``a`` represents ``z``), ``sm_103f``, and ``sm_103`` architecture variants. "
"The ``sm_103`` is compatible with ``sm_103a`` and ``sm_103f``, and "
"``sm_103f`` is compatible with ``sm_103a``."
msgstr ""

#: ../../../NVPTXUsage.rst:188
msgid "Encoding := Arch * 10 + 2 (for 'f') + 1 (for 'a') Arch := X * 10 + Y"
msgstr ""

#: ../../../NVPTXUsage.rst:191
msgid ""
"For example, ``sm_103f`` is encoded as 1032 (103 * 10 + 2) and ``sm_103a`` "
"is encoded as 1033 (103 * 10 + 2 + 1)."
msgstr ""

#: ../../../NVPTXUsage.rst:194
msgid "This encoding allows simple partial ordering of the architectures."
msgstr ""

#: ../../../NVPTXUsage.rst:196
msgid ""
"Compare Family and Arch by dividing FullSMVersion by 100 and 10 respectively "
"before the comparison."
msgstr ""

#: ../../../NVPTXUsage.rst:198
msgid ""
"Compare within the family by comparing FullSMVersion, given both belongs to "
"the same family."
msgstr ""

#: ../../../NVPTXUsage.rst:200
msgid "Detect ``a`` variants by checking FullSMVersion & 1."
msgstr ""

#: ../../../NVPTXUsage.rst:205
msgid "NVPTX Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:208
msgid "Reading PTX Special Registers"
msgstr ""

#: ../../../NVPTXUsage.rst:211
msgid "'``llvm.nvvm.read.ptx.sreg.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:214 ../../../NVPTXUsage.rst:256
#: ../../../NVPTXUsage.rst:313 ../../../NVPTXUsage.rst:340
#: ../../../NVPTXUsage.rst:375 ../../../NVPTXUsage.rst:402
#: ../../../NVPTXUsage.rst:431 ../../../NVPTXUsage.rst:458
#: ../../../NVPTXUsage.rst:484 ../../../NVPTXUsage.rst:519
#: ../../../NVPTXUsage.rst:553 ../../../NVPTXUsage.rst:581
#: ../../../NVPTXUsage.rst:609 ../../../NVPTXUsage.rst:635
#: ../../../NVPTXUsage.rst:663 ../../../NVPTXUsage.rst:693
#: ../../../NVPTXUsage.rst:719 ../../../NVPTXUsage.rst:753
#: ../../../NVPTXUsage.rst:842 ../../../NVPTXUsage.rst:878
#: ../../../NVPTXUsage.rst:910 ../../../NVPTXUsage.rst:934
#: ../../../NVPTXUsage.rst:962 ../../../NVPTXUsage.rst:1002
#: ../../../NVPTXUsage.rst:1026 ../../../NVPTXUsage.rst:1065
#: ../../../NVPTXUsage.rst:1115 ../../../NVPTXUsage.rst:1146
#: ../../../NVPTXUsage.rst:1179 ../../../NVPTXUsage.rst:1207
#: ../../../NVPTXUsage.rst:1241 ../../../NVPTXUsage.rst:1271
#: ../../../NVPTXUsage.rst:1314 ../../../NVPTXUsage.rst:1344
#: ../../../NVPTXUsage.rst:1375 ../../../NVPTXUsage.rst:1402
#: ../../../NVPTXUsage.rst:1431 ../../../NVPTXUsage.rst:1471
#: ../../../NVPTXUsage.rst:1501 ../../../NVPTXUsage.rst:1526
#: ../../../NVPTXUsage.rst:1552 ../../../NVPTXUsage.rst:1579
#: ../../../NVPTXUsage.rst:1604 ../../../NVPTXUsage.rst:1626
#: ../../../NVPTXUsage.rst:1650 ../../../NVPTXUsage.rst:1714
#: ../../../NVPTXUsage.rst:1768 ../../../NVPTXUsage.rst:1812
#: ../../../NVPTXUsage.rst:1845 ../../../NVPTXUsage.rst:1878
#: ../../../NVPTXUsage.rst:1905 ../../../NVPTXUsage.rst:1936
msgid "Syntax:"
msgstr ""

#: ../../../NVPTXUsage.rst:233 ../../../NVPTXUsage.rst:269
#: ../../../NVPTXUsage.rst:320 ../../../NVPTXUsage.rst:355
#: ../../../NVPTXUsage.rst:386 ../../../NVPTXUsage.rst:410
#: ../../../NVPTXUsage.rst:443 ../../../NVPTXUsage.rst:467
#: ../../../NVPTXUsage.rst:495 ../../../NVPTXUsage.rst:529
#: ../../../NVPTXUsage.rst:560 ../../../NVPTXUsage.rst:588
#: ../../../NVPTXUsage.rst:617 ../../../NVPTXUsage.rst:643
#: ../../../NVPTXUsage.rst:673 ../../../NVPTXUsage.rst:701
#: ../../../NVPTXUsage.rst:726 ../../../NVPTXUsage.rst:766
#: ../../../NVPTXUsage.rst:849 ../../../NVPTXUsage.rst:886
#: ../../../NVPTXUsage.rst:917 ../../../NVPTXUsage.rst:941
#: ../../../NVPTXUsage.rst:980 ../../../NVPTXUsage.rst:1010
#: ../../../NVPTXUsage.rst:1034 ../../../NVPTXUsage.rst:1076
#: ../../../NVPTXUsage.rst:1124 ../../../NVPTXUsage.rst:1157
#: ../../../NVPTXUsage.rst:1188 ../../../NVPTXUsage.rst:1218
#: ../../../NVPTXUsage.rst:1250 ../../../NVPTXUsage.rst:1290
#: ../../../NVPTXUsage.rst:1323 ../../../NVPTXUsage.rst:1351
#: ../../../NVPTXUsage.rst:1382 ../../../NVPTXUsage.rst:1409
#: ../../../NVPTXUsage.rst:1439 ../../../NVPTXUsage.rst:1481
#: ../../../NVPTXUsage.rst:1509 ../../../NVPTXUsage.rst:1534
#: ../../../NVPTXUsage.rst:1562 ../../../NVPTXUsage.rst:1587
#: ../../../NVPTXUsage.rst:1612 ../../../NVPTXUsage.rst:1634
#: ../../../NVPTXUsage.rst:1676 ../../../NVPTXUsage.rst:1723
#: ../../../NVPTXUsage.rst:1777 ../../../NVPTXUsage.rst:1820
#: ../../../NVPTXUsage.rst:1853 ../../../NVPTXUsage.rst:1885
#: ../../../NVPTXUsage.rst:1914 ../../../NVPTXUsage.rst:1943
msgid "Overview:"
msgstr ""

#: ../../../NVPTXUsage.rst:235
msgid ""
"The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX "
"special registers, in particular the kernel launch bounds.  These registers "
"map in the following way to CUDA builtins:"
msgstr ""

#: ../../../NVPTXUsage.rst:240
msgid "CUDA Builtin"
msgstr ""

#: ../../../NVPTXUsage.rst:240
msgid "PTX Special Register Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:242
msgid "``threadId``"
msgstr ""

#: ../../../NVPTXUsage.rst:242
msgid "``@llvm.nvvm.read.ptx.sreg.tid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:243
msgid "``blockIdx``"
msgstr ""

#: ../../../NVPTXUsage.rst:243
msgid "``@llvm.nvvm.read.ptx.sreg.ctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:244
msgid "``blockDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:244
msgid "``@llvm.nvvm.read.ptx.sreg.ntid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:245
msgid "``gridDim``"
msgstr ""

#: ../../../NVPTXUsage.rst:245
msgid "``@llvm.nvvm.read.ptx.sreg.nctaid.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:250
msgid "Barriers"
msgstr ""

#: ../../../NVPTXUsage.rst:253
msgid "'``llvm.nvvm.barrier.cta.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:271
msgid ""
"The '``@llvm.nvvm.barrier.cta.*``' family of intrinsics perform barrier "
"synchronization and communication within a CTA. They can be used by the "
"threads within the CTA for synchronization and communication."
msgstr ""

#: ../../../NVPTXUsage.rst:276 ../../../NVPTXUsage.rst:392
#: ../../../NVPTXUsage.rst:418 ../../../NVPTXUsage.rst:448
#: ../../../NVPTXUsage.rst:473 ../../../NVPTXUsage.rst:502
#: ../../../NVPTXUsage.rst:536 ../../../NVPTXUsage.rst:568
#: ../../../NVPTXUsage.rst:596 ../../../NVPTXUsage.rst:623
#: ../../../NVPTXUsage.rst:650 ../../../NVPTXUsage.rst:680
#: ../../../NVPTXUsage.rst:707 ../../../NVPTXUsage.rst:732
#: ../../../NVPTXUsage.rst:773
msgid "Semantics:"
msgstr ""

#: ../../../NVPTXUsage.rst:278
msgid ""
"Operand %id specifies a logical barrier resource and must fall within the "
"range 0 through 15. When present, operand %n specifies the number of threads "
"participating in the barrier. When specifying a thread count, the value must "
"be a multiple of the warp size. With the '``@llvm.nvvm.barrier.cta.sync.*``' "
"variants, the '``.all``' suffix indicates that all threads in the CTA should "
"participate in the barrier while the '``.count``' suffix indicates that only "
"the threads specified by the %n operand should participate in the barrier."
msgstr ""

#: ../../../NVPTXUsage.rst:286
msgid ""
"All forms of the '``@llvm.nvvm.barrier.cta.*``' intrinsic cause the "
"executing thread to wait for all non-exited threads from its warp and then "
"marks the warp's arrival at the barrier. In addition to signaling its "
"arrival at the barrier, the '``@llvm.nvvm.barrier.cta.sync.*``' intrinsics "
"cause the executing thread to wait for non-exited threads of all other warps "
"participating in the barrier to arrive. On the other hand, the '``@llvm.nvvm."
"barrier.cta.arrive.*``' intrinsic does not cause the executing thread to "
"wait for threads of other participating warps."
msgstr ""

#: ../../../NVPTXUsage.rst:295
msgid ""
"When a barrier completes, the waiting threads are restarted without delay, "
"and the barrier is reinitialized so that it can be immediately reused."
msgstr ""

#: ../../../NVPTXUsage.rst:298
msgid ""
"The '``@llvm.nvvm.barrier.cta.*``' intrinsic has an optional '``.aligned``' "
"modifier to indicate textual alignment of the barrier. When specified, it "
"indicates that all threads in the CTA will execute the same '``@llvm.nvvm."
"barrier.cta.*``' instruction. In conditionally executed code, an aligned "
"'``@llvm.nvvm.barrier.cta.*``' instruction should only be used if it is "
"known that all threads in the CTA evaluate the condition identically, "
"otherwise behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:307
msgid "Electing a thread"
msgstr ""

#: ../../../NVPTXUsage.rst:310
msgid "'``llvm.nvvm.elect.sync``'"
msgstr ""

#: ../../../NVPTXUsage.rst:322
msgid ""
"The '``@llvm.nvvm.elect.sync``' intrinsic generates the ``elect.sync`` PTX "
"instruction, which elects one predicated active leader thread from a set of "
"threads specified by ``membermask``. The behavior is undefined if the "
"executing thread is not in ``membermask``. The laneid of the elected thread "
"is captured in the i32 return value. The i1 return value is set to ``True`` "
"for the leader thread and ``False`` for all the other threads. Election of a "
"leader thread happens deterministically, i.e. the same leader thread is "
"elected for the same ``membermask`` every time. For more information, refer "
"PTX ISA `<https://docs.nvidia.com/cuda/parallel-thread-execution/index."
"html#parallel-synchronization-and-communication-instructions-elect-sync>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:334
msgid "Membar/Fences"
msgstr ""

#: ../../../NVPTXUsage.rst:337
msgid "'``llvm.nvvm.fence.proxy.tensormap_generic.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:357
msgid ""
"The ``@llvm.nvvm.fence.proxy.tensormap_generic.*`` is a uni-directional "
"fence used to establish ordering between a prior memory access performed via "
"the generic `proxy<https://docs.nvidia.com/cuda/parallel-thread-execution/"
"index.html#proxies>_` and a subsequent memory access performed via the "
"tensormap proxy. ``nvvm.fence.proxy.tensormap_generic.release`` can form a "
"release sequence that synchronizes with an acquire sequence that contains "
"the ``nvvm.fence.proxy.tensormap_generic.acquire`` proxy fence. The "
"following table describes the mapping between LLVM Intrinsic and the PTX "
"instruction:"
msgstr ""

#: ../../../NVPTXUsage.rst:360
msgid "NVVM Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:360
msgid "PTX Instruction"
msgstr ""

#: ../../../NVPTXUsage.rst:362
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:362
msgid "``fence.proxy.tensormap::generic.release.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:363
msgid "``@llvm.nvvm.fence.proxy.tensormap_generic.acquire.*``"
msgstr ""

#: ../../../NVPTXUsage.rst:363
msgid "``fence.proxy.tensormap::generic.acquire.* [addr], size``"
msgstr ""

#: ../../../NVPTXUsage.rst:366
msgid ""
"The address operand ``addr`` and the operand ``size`` together specify the "
"memory range ``[addr, addr+size)`` on which the ordering guarantees on the "
"memory accesses across the proxies is to be provided. The only supported "
"value for the ``size`` operand is ``128`` and must be an immediate. Generic "
"Addressing is used unconditionally, and the address specified by the operand "
"addr must fall within the ``.global`` state space. Otherwise, the behavior "
"is undefined. For more information, see `PTX ISA <https://docs.nvidia.com/"
"cuda/parallel-thread-execution/#parallel-synchronization-and-communication-"
"instructions-membar>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:369
msgid "Address Space Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:372
msgid "'``llvm.nvvm.isspacep.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:388
msgid ""
"The '``llvm.nvvm.isspacep.*``' intrinsics determine whether the provided "
"generic pointer references memory which falls within a particular address "
"space."
msgstr ""

#: ../../../NVPTXUsage.rst:394 ../../../NVPTXUsage.rst:420
msgid ""
"If the given pointer in the generic address space refers to memory which "
"falls within the state space of the intrinsic (and therefore could be safely "
"address space casted to this space), 1 is returned, otherwise 0 is returned."
msgstr ""

#: ../../../NVPTXUsage.rst:399
msgid "'``llvm.nvvm.mapa.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:412
msgid ""
"The '``llvm.nvvm.mapa.*``' intrinsics map a shared memory pointer ``p`` of "
"another CTA with ``%rank`` to the current CTA. The ``llvm.nvvm.mapa`` form "
"expects a generic pointer to shared memory and returns a generic pointer to "
"shared cluster memory. The ``llvm.nvvm.mapa.shared.cluster`` form expects a "
"pointer to shared memory and returns a pointer to shared cluster memory. "
"They corresponds directly to the ``mapa`` and ``mapa.shared.cluster`` PTX "
"instructions."
msgstr ""

#: ../../../NVPTXUsage.rst:425
msgid "Arithmetic Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:428
msgid "'``llvm.nvvm.fabs.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:445
msgid ""
"The '``llvm.nvvm.fabs.*``' intrinsics return the absolute value of the "
"operand."
msgstr ""

#: ../../../NVPTXUsage.rst:450
msgid ""
"Unlike, '``llvm.fabs.*``', these intrinsics do not perfectly preserve NaN "
"values. Instead, a NaN input yeilds an unspecified NaN output."
msgstr ""

#: ../../../NVPTXUsage.rst:455
msgid "'``llvm.nvvm.fabs.ftz.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:469
msgid ""
"The '``llvm.nvvm.fabs.ftz.*``' intrinsics return the absolute value of the "
"operand, flushing subnormals to sign preserving zero."
msgstr ""

#: ../../../NVPTXUsage.rst:475
msgid ""
"Before the absolute value is taken, the input is flushed to sign preserving "
"zero if it is a subnormal. In addition, unlike '``llvm.fabs.*``', a NaN "
"input yields an unspecified NaN output."
msgstr ""

#: ../../../NVPTXUsage.rst:481
msgid "'``llvm.nvvm.idp2a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:497
msgid ""
"The '``llvm.nvvm.idp2a.[us].[us]``' intrinsics performs a 2-element vector "
"dot product followed by addition. They corresponds directly to the ``dp2a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:504
msgid ""
"The 32-bit value in ``%a`` is broken into 2 16-bit values which are extended "
"to 32 bits. For the '``llvm.nvvm.idp2a.u.[us]``' variants zero-extension is "
"used, while for the '``llvm.nvvm.idp2a.s.[us]``' sign-extension is used. Two "
"bytes are selected from ``%b``, if ``%is.hi`` is true, the most significant "
"bytes are selected, otherwise the least significant bytes are selected. "
"These bytes are then extended to 32-bits. For the '``llvm.nvvm.idp2a.[us]."
"u``' variants zero-extension is used, while for the '``llvm.nvvm.idp2a.[us]."
"s``' sign-extension is used. The dot product of these 2-element vectors is "
"added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:516
msgid "'``llvm.nvvm.idp4a.[us].[us]``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:531
msgid ""
"The '``llvm.nvvm.idp4a.[us].[us]``' intrinsics perform a 4-element vector "
"dot product followed by addition. They corresponds directly to the ``dp4a`` "
"PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:538
msgid ""
"Each of the 4 bytes in both ``%a`` and ``%b`` are extended to 32-bit "
"integers forming 2 ``<4 x i32>``. For ``%a``, zero-extension is used in the "
"'``llvm.nvvm.idp4a.u.[us]``' variants, while sign-extension is used with "
"'``llvm.nvvm.idp4a.s.[us]``' variants. Similarly, for ``%b``, zero-extension "
"is used in the '``llvm.nvvm.idp4a.[us].u``' variants, while sign-extension "
"is used with '``llvm.nvvm.idp4a.[us].s``' variants. The dot product of these "
"4-element vectors is added to ``%c`` to produce the return."
msgstr ""

#: ../../../NVPTXUsage.rst:547
msgid "Bit Manipulation Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:550
msgid "'``llvm.nvvm.fshl.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:562
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsics performs a clamped "
"funnel shift left. These intrinsics are very similar to '``llvm.fshl``', "
"except the shift amount is clamped at the integer width (instead of modulo "
"it). Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:570
msgid ""
"The '``llvm.nvvm.fshl.clamp``' family of intrinsic functions performs a "
"clamped funnel shift left: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted left, and the most significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:578
msgid "'``llvm.nvvm.fshr.clamp.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:590
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsics perform a clamped funnel "
"shift right. These intrinsics are very similar to '``llvm.fshr``', except "
"the shift amount is clamped at the integer width (instead of modulo it). "
"Currently, only ``i32`` is supported."
msgstr ""

#: ../../../NVPTXUsage.rst:598
msgid ""
"The '``llvm.nvvm.fshr.clamp``' family of intrinsic functions performs a "
"clamped funnel shift right: the first two values are concatenated as { %hi : "
"%lo } (%hi is the most significant bits of the wide value), the combined "
"value is shifted right, and the least significant bits are extracted to "
"produce a result that is the same size as the original arguments. The shift "
"amount is the minimum of the value of %n and the bit width of the integer "
"type."
msgstr ""

#: ../../../NVPTXUsage.rst:606
msgid "'``llvm.nvvm.flo.u.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:619
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics identifies the bit position "
"of the leading one, returning either it's offset from the most or least "
"significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:625
msgid ""
"The '``llvm.nvvm.flo.u``' family of intrinsics returns the bit position of "
"the most significant 1. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:632
msgid "'``llvm.nvvm.flo.s.*``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:645
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics identifies the bit position "
"of the leading non-sign bit, returning either it's offset from the most or "
"least significant bit."
msgstr ""

#: ../../../NVPTXUsage.rst:652
msgid ""
"The '``llvm.nvvm.flo.s``' family of intrinsics returns the bit position of "
"the most significant 0 for negative inputs and the most significant 1 for "
"non-negative inputs. If %shiftamt is true, The result is the shift amount "
"needed to left-shift the found bit into the most-significant bit position, "
"otherwise the result is the shift amount needed to right-shift the found bit "
"into the least-significant bit position. 0xffffffff is returned if no 1 bit "
"is found."
msgstr ""

#: ../../../NVPTXUsage.rst:660
msgid "'``llvm.nvvm.{zext,sext}.{wrap,clamp}``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:675
msgid ""
"The '``llvm.nvvm.{zext,sext}.{wrap,clamp}``' family of intrinsics extracts "
"the low bits of the input value, and zero- or sign-extends them back to the "
"original width."
msgstr ""

#: ../../../NVPTXUsage.rst:682
msgid ""
"The '``llvm.nvvm.{zext,sext}.{wrap,clamp}``' family of intrinsics returns "
"extension of N lowest bits of operand %a. For the '``wrap``' variants, N is "
"the value of operand %b modulo 32. For the '``clamp``' variants, N is the "
"value of operand %b clamped to the range [0, 32]. The N lowest bits are then "
"zero-extended the case of the '``zext``' variants, or sign-extended the case "
"of the '``sext``' variants. If N is 0, the result is 0."
msgstr ""

#: ../../../NVPTXUsage.rst:690
msgid "'``llvm.nvvm.bmsk.{wrap,clamp}``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:703
msgid ""
"The '``llvm.nvvm.bmsk.{wrap,clamp}``' family of intrinsics creates a bit "
"mask given a starting bit position and a bit width."
msgstr ""

#: ../../../NVPTXUsage.rst:709
msgid ""
"The '``llvm.nvvm.bmsk.{wrap,clamp}``' family of intrinsics returns a value "
"with all bits set to 0 except for %b bits starting at bit position %a. For "
"the '``wrap``' variants, the values of %a and %b modulo 32 are used. For the "
"'``clamp``' variants, the values of %a and %b are clamped to the range [0, "
"32], which in practice is equivalent to using them as is."
msgstr ""

#: ../../../NVPTXUsage.rst:716
msgid "'``llvm.nvvm.prmt``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:728
msgid ""
"The '``llvm.nvvm.prmt``' constructs a permutation of the bytes of the first "
"two operands, selecting based on the third operand."
msgstr ""

#: ../../../NVPTXUsage.rst:734
msgid ""
"The bytes in the first two source operands are numbered from 0 to 7: {%hi, "
"%lo} = {{b7, b6, b5, b4}, {b3, b2, b1, b0}}. For each byte in the target "
"register, a 4-bit selection value is defined."
msgstr ""

#: ../../../NVPTXUsage.rst:738
msgid ""
"The 3 lsbs of the selection value specify which of the 8 source bytes should "
"be moved into the target position. The msb defines if the byte value should "
"be copied, or if the sign (msb of the byte) should be replicated over all 8 "
"bits of the target position (sign extend of the byte value); msb=0 means "
"copy the literal value; msb=1 means replicate the sign."
msgstr ""

#: ../../../NVPTXUsage.rst:744
msgid ""
"These 4-bit selection values are pulled from the lower 16-bits of the "
"%selector operand, with the least significant selection value corresponding "
"to the least significant byte of the destination."
msgstr ""

#: ../../../NVPTXUsage.rst:750
msgid "'``llvm.nvvm.prmt.*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:768
msgid ""
"The '``llvm.nvvm.prmt.*``' family of intrinsics constructs a permutation of "
"the bytes of the first one or two operands, selecting based on the 2 least "
"significant bits of the final operand."
msgstr ""

#: ../../../NVPTXUsage.rst:775
msgid ""
"As with the generic '``llvm.nvvm.prmt``' intrinsic, the bytes in the first "
"one or two source operands are numbered. The first source operand (%lo) is "
"numbered {b3, b2, b1, b0}, in the case of the '``f4e``' and '``b4e``' "
"variants, the second source operand (%hi) is numbered {b7, b6, b5, b4}."
msgstr ""

#: ../../../NVPTXUsage.rst:780
msgid ""
"Depending on the 2 least significant bits of the %selector operand, the "
"result of the permutation is defined as follows:"
msgstr ""

#: ../../../NVPTXUsage.rst:784
msgid "Mode"
msgstr ""

#: ../../../NVPTXUsage.rst:784
msgid "%selector[1:0]"
msgstr ""

#: ../../../NVPTXUsage.rst:784
msgid "Output"
msgstr ""

#: ../../../NVPTXUsage.rst:786
msgid "'``f4e``'"
msgstr ""

#: ../../../NVPTXUsage.rst:786 ../../../NVPTXUsage.rst:810
#: ../../../NVPTXUsage.rst:824
msgid "{3, 2, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:788
msgid "{4, 3, 2, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:790
msgid "{5, 4, 3, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:792
msgid "{6, 5, 4, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:794
msgid "'``b4e``'"
msgstr ""

#: ../../../NVPTXUsage.rst:794
msgid "{5, 6, 7, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:796
msgid "{6, 7, 0, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:798
msgid "{7, 0, 1, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:800
msgid "{0, 1, 2, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:802
msgid "'``rc8``'"
msgstr ""

#: ../../../NVPTXUsage.rst:802 ../../../NVPTXUsage.rst:818
msgid "{0, 0, 0, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:804
msgid "{1, 1, 1, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:806
msgid "{2, 2, 2, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:808 ../../../NVPTXUsage.rst:816
msgid "{3, 3, 3, 3}"
msgstr ""

#: ../../../NVPTXUsage.rst:810
msgid "'``ecl``'"
msgstr ""

#: ../../../NVPTXUsage.rst:812
msgid "{3, 2, 1, 1}"
msgstr ""

#: ../../../NVPTXUsage.rst:814
msgid "{3, 2, 2, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:818
msgid "'``ecr``'"
msgstr ""

#: ../../../NVPTXUsage.rst:820
msgid "{1, 1, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:822
msgid "{2, 2, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:826
msgid "'``rc16``'"
msgstr ""

#: ../../../NVPTXUsage.rst:826 ../../../NVPTXUsage.rst:830
msgid "{1, 0, 1, 0}"
msgstr ""

#: ../../../NVPTXUsage.rst:828 ../../../NVPTXUsage.rst:832
msgid "{3, 2, 3, 2}"
msgstr ""

#: ../../../NVPTXUsage.rst:836
msgid "TMA family of Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:839
msgid "'``llvm.nvvm.cp.async.bulk.global.to.shared.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:851
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.global.to.shared.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.global.*`` family of PTX "
"instructions. These instructions initiate an asynchronous copy of bulk data "
"from global memory to shared::cluster memory. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:858
msgid ""
"The last two arguments to these intrinsics are boolean flags indicating "
"support for cache_hint and/or multicast modifiers. These flag arguments must "
"be compile-time constants. The backend looks through these flags and lowers "
"the intrinsics appropriately."
msgstr ""

#: ../../../NVPTXUsage.rst:863
msgid ""
"The Nth argument (denoted by ``i1 %flag_ch``) when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:867
msgid ""
"The [N-1]th argument (denoted by ``i1 %flag_mc``) when set, indicates the "
"presence of a multicast mask (``i16 %mc``) and generates the PTX instruction "
"with the ``.multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:871 ../../../NVPTXUsage.rst:903
#: ../../../NVPTXUsage.rst:927
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:875
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.global``'"
msgstr ""

#: ../../../NVPTXUsage.rst:888
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.global``' intrinsic "
"corresponds to the ``cp.async.bulk.global.shared::cta.*`` set of PTX "
"instructions. These instructions initiate an asynchronous copy from shared::"
"cta to global memory. The 32-bit operand ``%size`` specifies the amount of "
"memory to be copied (in bytes) and it must be a multiple of 16. For the ``."
"bytemask`` variant, the 16-bit wide mask operand specifies whether the i-th "
"byte of each 16-byte wide chunk of source data is copied to the destination."
msgstr ""

#: ../../../NVPTXUsage.rst:897
msgid ""
"The ``i1 %flag_ch`` argument to these intrinsics is a boolean flag "
"indicating support for cache_hint. This flag argument must be a compile-time "
"constant. When set, it indicates a valid cache_hint (``i64 %ch``) and "
"generates the ``.L2::cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:907
msgid "'``llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``'"
msgstr ""

#: ../../../NVPTXUsage.rst:919
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.shared.cta.to.cluster``' intrinsic "
"corresponds to the ``cp.async.bulk.shared::cluster.shared::cta.*`` PTX "
"instruction. This instruction initiates an asynchronous copy from shared::"
"cta to shared::cluster memory. The destination has to be in the shared "
"memory of a different CTA within the cluster. The 32-bit operand ``%size`` "
"specifies the amount of memory to be copied and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:931
msgid "'``llvm.nvvm.cp.async.bulk.prefetch.L2``'"
msgstr ""

#: ../../../NVPTXUsage.rst:943
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.prefetch.L2``' intrinsic corresponds to the "
"``cp.async.bulk.prefetch.L2.*`` family of PTX instructions. These "
"instructions initiate an asynchronous prefetch of bulk data from global "
"memory to the L2 cache. The 32-bit operand ``%size`` specifies the amount of "
"memory to be prefetched in terms of bytes and it must be a multiple of 16."
msgstr ""

#: ../../../NVPTXUsage.rst:950
msgid ""
"The last argument to these intrinsics is boolean flag indicating support for "
"cache_hint. These flag argument must be compile-time constant. When set, it "
"indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:955
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:959
msgid "'``llvm.nvvm.prefetch.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:982
msgid ""
"The '``@llvm.nvvm.prefetch.*``' and '``@llvm.nvvm.prefetchu.*``' intrinsic "
"correspond to the '``prefetch.*``;' and '``prefetchu.*``' family of PTX "
"instructions. The '``prefetch.*``' instructions bring the cache line "
"containing the specified address in global or local memory address space "
"into the specified cache level (L1 or L2). The '`prefetchu.*``' instruction "
"brings the cache line containing the specified generic address into the "
"specified uniform cache level. If no address space is specified, it is "
"assumed to be generic address. The intrinsic uses and eviction priority "
"which can be accessed by the '``.level::eviction_priority``' modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:991
msgid "A prefetch to a shared memory location performs no operation."
msgstr ""

#: ../../../NVPTXUsage.rst:992
msgid ""
"A prefetch into the uniform cache requires a generic address, and no "
"operation occurs if the address maps to a const, local, or shared memory "
"location."
msgstr ""

#: ../../../NVPTXUsage.rst:995
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#data-movement-and-conversion-instructions-"
"prefetch-prefetchu>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:999
msgid "'``llvm.nvvm.applypriority.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1012
msgid ""
"The '``@llvm.nvvm.applypriority.*``'  applies the cache eviction priority "
"specified by the .level::eviction_priority qualifier to the address range "
"[a..a+size) in the specified cache level. If no state space is specified "
"then Generic Addressing is used. If the specified address does not fall "
"within the address window of .global state space then the behavior is "
"undefined. The operand size is an integer constant that specifies the amount "
"of data, in bytes, in the specified cache level on which the priority is to "
"be applied. The only supported value for the size operand is 128."
msgstr ""

#: ../../../NVPTXUsage.rst:1019
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#data-movement-and-conversion-instructions-"
"applypriority>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1023
msgid "``llvm.nvvm.discard.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1036
msgid ""
"The *effects* of the ``@llvm.nvvm.discard.L2*`` intrinsics are those of a "
"non-atomic non-volatile ``llvm.memset`` that writes ``undef`` to the "
"destination address range ``[%ptr, %ptr + immarg)``. The ``%ptr`` must be "
"aligned by 128 bytes. Subsequent reads from the address range may read "
"``undef`` until the memory is overwritten with a different value. These "
"operations *hint* the implementation that data in the L2 cache can be "
"destructively discarded without writing it back to memory. The operand "
"``immarg`` is an integer constant that specifies the length in bytes of the "
"address range ``[%ptr, %ptr + immarg)`` to write ``undef`` into. The only "
"supported value for the ``immarg`` operand is ``128``. If generic addressing "
"is used and the specified address does not fall within the address window of "
"global memory (``addrspace(1)``) the behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1059
msgid ""
"For more information, refer to the  `CUDA C++ discard documentation <https://"
"nvidia.github.io/cccl/libcudacxx/extended_api/memory_access_properties/"
"discard_memory.html>`__ and to the `PTX ISA discard documentation <https://"
"docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-"
"instructions-discard>`__ ."
msgstr ""

#: ../../../NVPTXUsage.rst:1062
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1078
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``tile`` mode. In tile mode, the multi-dimensional layout of the source "
"tensor is preserved at the destination. The dimension of the tensor data "
"ranges from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 "
"%d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:1087
msgid ""
"The last three arguments to these intrinsics are flags indicating support "
"for multicast, cache_hint and cta_group::1/2 modifiers. These flag arguments "
"must be compile-time constants. The backend looks through these flags and "
"lowers the intrinsics appropriately."
msgstr ""

#: ../../../NVPTXUsage.rst:1093
msgid ""
"The argument denoted by ``i1 %flag_ch`` when set, indicates a valid "
"cache_hint (``i64 %ch``) and generates the ``.L2::cache_hint`` variant of "
"the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1097
msgid ""
"The argument denoted by ``i1 %flag_mc`` when set, indicates the presence of "
"a multicast mask (``i16 %mc``) and generates the PTX instruction with the ``."
"multicast::cluster`` modifier."
msgstr ""

#: ../../../NVPTXUsage.rst:1101
msgid ""
"The argument denoted by ``i32 %flag_cta_group`` takes values within the "
"range [0, 3) i.e. {0,1,2}. When the value of ``%flag_cta_group`` is not "
"within the range, it may raise an error from the Verifier. The default value "
"is '0' with no cta_group modifier in the instruction. The values of '1' and "
"'2' lower to ``cta_group::1`` and ``cta_group::2`` variants of the PTX "
"instruction respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1108 ../../../NVPTXUsage.rst:1139
#: ../../../NVPTXUsage.rst:1172 ../../../NVPTXUsage.rst:1200
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1112
msgid "'``llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1126
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.g2s.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from global "
"memory to shared::cluster memory (indicated by the ``g2s`` prefix) in "
"``im2col`` mode. In im2col mode, some dimensions of the source tensor are "
"unrolled into a single dimensional column at the destination. In this mode, "
"the tensor has to be at least three-dimensional. Along with the tensor "
"coordinates, im2col offsets are also specified (denoted by ``i16 im2col0..."
"i16 %im2col2``). The number of im2col offsets is two less than the number of "
"dimensions of the tensor operation. The last three arguments to these "
"intrinsics are flags, with the same functionality as described in the "
"``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1143
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1159
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``tile`` mode. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:1166 ../../../NVPTXUsage.rst:1228
#: ../../../NVPTXUsage.rst:1301
msgid ""
"The last argument to these intrinsics is a boolean flag indicating support "
"for cache_hint. This flag argument must be a compile-time constant. When "
"set, it indicates a valid cache_hint (``i64 %ch``) and generates the ``.L2::"
"cache_hint`` variant of the PTX instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1176
msgid "'``llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1190
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.s2g.im2col.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.tensor.[1-5]d.*`` set of PTX instructions. "
"These instructions initiate an asynchronous copy of tensor data from shared::"
"cta to global memory (indicated by the ``s2g`` prefix) in ``im2col`` mode. "
"In this mode, the tensor has to be at least three-dimensional. Unlike the "
"``g2s`` variants, there are no im2col_offsets for these intrinsics. The last "
"argument to these intrinsics is a boolean flag, with the same functionality "
"as described in the ``s2g.tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1204
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1220
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.tile.[1-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In tile mode, the multi-"
"dimensional layout of the source tensor is preserved at the destination. The "
"dimension of the tensor data ranges from 1d to 5d with the coordinates "
"specified by the ``i32 %d0 ... i32 %d4`` arguments."
msgstr ""

#: ../../../NVPTXUsage.rst:1234 ../../../NVPTXUsage.rst:1264
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-"
"prefetch-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1238
msgid "'``llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1252
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.prefetch.im2col.[3-5]d``' intrinsics "
"correspond to the ``cp.async.bulk.prefetch.tensor.[1-5]d.L2.global*`` set of "
"PTX instructions. These instructions initiate an asynchronous prefetch of "
"tensor data from global memory to the L2 cache. In im2col mode, some "
"dimensions of the source tensor are unrolled into a single dimensional "
"column at the destination. In this mode, the tensor has to be at least three-"
"dimensional. Along with the tensor coordinates, im2col offsets are also "
"specified (denoted by ``i16 im2col0...i16 %im2col2``). The number of im2col "
"offsets is two less than the number of dimensions of the tensor operation. "
"The last argument to these intrinsics is a boolean flag, with the same "
"functionality as described in the ``tile`` mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1268
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].tile.[1-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1292
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.tile.[1-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[1-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``tile`` mode. The dimension of the tensor data ranges "
"from 1d to 5d with the coordinates specified by the ``i32 %d0 ... i32 %d4`` "
"arguments. The supported reduction operations are {add, min, max, inc, dec, "
"and, or, xor} as described in the ``tile.1d`` intrinsics."
msgstr ""

#: ../../../NVPTXUsage.rst:1307 ../../../NVPTXUsage.rst:1334
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#data-movement-and-conversion-instructions-cp-"
"reduce-async-bulk-tensor>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1311
msgid "'``llvm.nvvm.cp.async.bulk.tensor.reduce.[red_op].im2col.[3-5]d``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1325
msgid ""
"The '``@llvm.nvvm.cp.async.bulk.tensor.reduce.<red_op>.im2col.[3-5]d``' "
"intrinsics correspond to the ``cp.reduce.async.bulk.tensor.[3-5]d.*`` set of "
"PTX instructions. These instructions initiate an asynchronous reduction "
"operation of tensor data in global memory with the tensor data in shared{::"
"cta} memory, using ``im2col`` mode. In this mode, the tensor has to be at "
"least three-dimensional. The supported reduction operations supported are "
"the same as the ones in the tile mode. The last argument to these intrinsics "
"is a boolean flag, with the same functionality as described in the ``tile`` "
"mode intrinsics above."
msgstr ""

#: ../../../NVPTXUsage.rst:1338
msgid "Warp Group Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1341
msgid "'``llvm.nvvm.wgmma.fence.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1353
msgid ""
"The '``@llvm.nvvm.wgmma.fence.sync.aligned``' intrinsic generates the "
"``wgmma.fence.sync.aligned`` PTX instruction, which establishes an ordering "
"between prior accesses to any warpgroup registers and subsequent accesses to "
"the same registers by a ``wgmma.mma_async`` instruction."
msgstr ""

#: ../../../NVPTXUsage.rst:1358
msgid ""
"The ``wgmma.fence`` instruction must be issued by all warps of the warpgroup "
"in the following locations:"
msgstr ""

#: ../../../NVPTXUsage.rst:1361
msgid "Before the first ``wgmma.mma_async`` operation in a warpgroup."
msgstr ""

#: ../../../NVPTXUsage.rst:1362
msgid ""
"Between a register access by a thread in the warpgroup and any ``wgmma."
"mma_async`` instruction that accesses the same registers, except when these "
"are accumulator register accesses across multiple ``wgmma.mma_async`` "
"instructions of the same shape in which case an ordering guarantee is "
"provided by default."
msgstr ""

#: ../../../NVPTXUsage.rst:1368
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"fence>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1372
msgid "'``llvm.nvvm.wgmma.commit_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1384
msgid ""
"The '``@llvm.nvvm.wgmma.commit_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned`` PTX instruction, which creates a new "
"wgmma-group per warpgroup and batches all prior ``wgmma.mma_async`` "
"instructions initiated by the executing warp but not committed to any wgmma-"
"group into the new wgmma-group. If there are no uncommitted ``wgmma "
"mma_async`` instructions then, ``wgmma.commit_group`` results in an empty "
"wgmma-group."
msgstr ""

#: ../../../NVPTXUsage.rst:1392
msgid ""
"An executing thread can wait for the completion of all ``wgmma.mma_async`` "
"operations in a wgmma-group by using ``wgmma.wait_group``."
msgstr ""

#: ../../../NVPTXUsage.rst:1395
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"commit-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1399
msgid "'``llvm.nvvm.wgmma.wait_group.sync.aligned``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1411
msgid ""
"The '``@llvm.nvvm.wgmma.wait_group.sync.aligned``' intrinsic generates the "
"``wgmma.commit_group.sync.aligned N`` PTX instruction, which will cause the "
"executing thread to wait until only ``N`` or fewer of the most recent wgmma-"
"groups are pending and all the prior wgmma-groups committed by the executing "
"threads are complete. For example, when ``N`` is 0, the executing thread "
"waits on all the prior wgmma-groups to complete. Operand ``N`` is an integer "
"constant."
msgstr ""

#: ../../../NVPTXUsage.rst:1419
msgid ""
"Accessing the accumulator register or the input register containing the "
"fragments of matrix A of a ``wgmma.mma_async`` instruction without first "
"performing a ``wgmma.wait_group`` instruction that waits on a wgmma-group "
"including that ``wgmma.mma_async`` instruction is undefined behavior."
msgstr ""

#: ../../../NVPTXUsage.rst:1424
msgid ""
"For more information, refer PTX ISA `<https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-"
"wait-group>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1428
msgid "'``llvm.nvvm.griddepcontrol.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1441
msgid ""
"The ``griddepcontrol`` intrinsics allows the dependent grids and "
"prerequisite grids as defined by the runtime, to control execution in the "
"following way:"
msgstr ""

#: ../../../NVPTXUsage.rst:1443
msgid ""
"``griddepcontrol.launch_dependents`` intrinsic signals that the dependents "
"can be scheduled, before the current grid completes. The intrinsic can be "
"invoked by multiple threads in the current CTA and repeated invocations of "
"the intrinsic will have no additional side effects past that of the first "
"invocation."
msgstr ""

#: ../../../NVPTXUsage.rst:1445
msgid ""
"``griddepcontrol.wait`` intrinsic causes the executing thread to wait until "
"all prerequisite grids in flight have completed and all the memory "
"operations from the prerequisite grids are performed and made visible to the "
"current grid."
msgstr ""

#: ../../../NVPTXUsage.rst:1447
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#parallel-synchronization-and-communication-instructions-"
"griddepcontrol>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1451
msgid "TCGEN05 family of Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1453
msgid ""
"The llvm.nvvm.tcgen05.* intrinsics model the TCGEN05 family of instructions "
"exposed by PTX. These intrinsics use 'Tensor Memory' (henceforth ``tmem``). "
"NVPTX represents this memory using ``addrspace(6)`` and is always 32-bits."
msgstr ""

#: ../../../NVPTXUsage.rst:1457
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensor-memory>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1460
msgid ""
"The tensor-memory pointers may only be used with the tcgen05 intrinsics. "
"There are specialized load/store instructions provided (tcgen05.ld/st) to "
"work with tensor-memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1464
msgid ""
"See the PTX ISA for more information on tensor-memory load/store "
"instructions `<https://docs.nvidia.com/cuda/parallel-thread-execution/"
"#tensor-memory-and-register-load-store-instructions>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1468
msgid "'``llvm.nvvm.tcgen05.alloc``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1483
msgid ""
"The '``@llvm.nvvm.tcgen05.alloc.*``' intrinsics correspond to the ``tcgen05."
"alloc.cta_group*.sync.aligned.b32`` family of PTX instructions. The "
"``tcgen05.alloc`` is a potentially blocking instruction which dynamically "
"allocates the specified number of columns in the Tensor Memory and writes "
"the address of the allocated Tensor Memory into shared memory at the "
"location specified by ``%dst``. The 32-bit operand ``%ncols`` specifies the "
"number of columns to be allocated and it must be a power-of-two. The ``."
"shared`` variant explicitly uses shared memory address space for the "
"``%dst`` operand. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` variants of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1494 ../../../NVPTXUsage.rst:1519
#: ../../../NVPTXUsage.rst:1545
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensor-memory-allocation-and-management-"
"instructions>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1498
msgid "'``llvm.nvvm.tcgen05.dealloc``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1511
msgid ""
"The '``@llvm.nvvm.tcgen05.dealloc.*``' intrinsics correspond to the "
"``tcgen05.dealloc.*`` set of PTX instructions. The ``tcgen05.dealloc`` "
"instructions deallocates the Tensor Memory specified by the Tensor Memory "
"address ``%tmem_addr``. The operand ``%tmem_addr`` must point to a previous "
"Tensor Memory allocation. The 32-bit operand ``%ncols`` specifies the number "
"of columns to be de-allocated. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` variants of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1523
msgid "'``llvm.nvvm.tcgen05.relinq.alloc.permit``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1536
msgid ""
"The '``@llvm.nvvm.tcgen05.relinq.alloc.permit.*``' intrinsics correspond to "
"the ``tcgen05.relinquish_alloc_permit.*`` set of PTX instructions. This "
"instruction specifies that the CTA of the executing thread is relinquishing "
"the right to allocate Tensor Memory. So, it is illegal for a CTA to perform "
"``tcgen05.alloc`` after any of its constituent threads execute ``tcgen05."
"relinquish_alloc_permit``. The ``.cg1`` and ``.cg2`` variants generate "
"``cta_group::1`` and ``cta_group::2`` flavors of the instruction "
"respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1549
msgid "'``llvm.nvvm.tcgen05.commit``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1564
msgid ""
"The '``@llvm.nvvm.tcgen05.commit.*``' intrinsics correspond to the ``tcgen05."
"commit.{cg1/cg2}.mbarrier::arrive::one.*`` set of PTX instructions. The "
"``tcgen05.commit`` is an asynchronous instruction which makes the mbarrier "
"object (``%mbar``) track the completion of all prior asynchronous tcgen05 "
"operations. The ``.mc`` variants allow signaling on the mbarrier objects of "
"multiple CTAs (specified by ``%mc``) in the cluster. The ``.cg1`` and ``."
"cg2`` variants generate ``cta_group::1`` and ``cta_group::2`` flavors of the "
"instruction respectively."
msgstr ""

#: ../../../NVPTXUsage.rst:1572
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen-async-sync-operations-commit>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1576
msgid "'``llvm.nvvm.tcgen05.wait``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1589
msgid ""
"The '``@llvm.nvvm.tcgen05.wait.ld/st``' intrinsics correspond to the "
"``tcgen05.wait::{ld/st}.sync.aligned`` pair of PTX instructions. The "
"``tcgen05.wait::ld`` causes the executing thread to block until all prior "
"``tcgen05.ld`` operations issued by the executing thread have completed. The "
"``tcgen05.wait::st`` causes the executing thread to block until all prior "
"``tcgen05.st`` operations issued by the executing thread have completed."
msgstr ""

#: ../../../NVPTXUsage.rst:1597
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-wait>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1601
msgid "'``llvm.nvvm.tcgen05.fence``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1614
msgid ""
"The '``@llvm.nvvm.tcgen05.fence.*``' intrinsics correspond to the ``tcgen05."
"fence::{before/after}_thread_sync`` pair of PTX instructions. These "
"instructions act as code motion fences for asynchronous tcgen05 operations."
msgstr ""

#: ../../../NVPTXUsage.rst:1619
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-"
"fence>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1623
msgid "'``llvm.nvvm.tcgen05.shift``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1636
msgid ""
"The '``@llvm.nvvm.tcgen05.shift.{cg1/cg2}``' intrinsics correspond to the "
"``tcgen05.shift.{cg1/cg2}`` PTX instructions. The ``tcgen05.shift`` is an "
"asynchronous instruction which initiates the shifting of 32-byte elements "
"downwards across all the rows, except the last, by one row. The address "
"operand ``%tmem_addr`` specifies the base address of the matrix in the "
"Tensor Memory whose rows must be down shifted."
msgstr ""

#: ../../../NVPTXUsage.rst:1643
msgid ""
"For more information, refer to the PTX ISA `<https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-shift>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1647
msgid "'``llvm.nvvm.tcgen05.cp``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1678
msgid ""
"The '``@llvm.nvvm.tcgen05.cp.{shape}.{src_fmt}.{cg1/cg2}``' intrinsics "
"correspond to the ``tcgen05.cp.*`` family of PTX instructions. The ``tcgen05."
"cp`` instruction initiates an asynchronous copy operation from shared memory "
"to the location specified by ``%tmem_addr`` in Tensor Memory. The 64-bit "
"register operand ``%sdesc`` is the matrix descriptor representing the source "
"matrix in shared memory that needs to be copied."
msgstr ""

#: ../../../NVPTXUsage.rst:1685
msgid ""
"The valid shapes for the copy operation are: {128x256b, 4x256b, 128x128b, "
"64x128b_warpx2_02_13, 64x128b_warpx2_01_23, 32x128b_warpx4}."
msgstr ""

#: ../../../NVPTXUsage.rst:1688
msgid ""
"Shapes ``64x128b`` and ``32x128b`` require dedicated multicast qualifiers, "
"which are appended to the corresponding intrinsic names."
msgstr ""

#: ../../../NVPTXUsage.rst:1691
msgid ""
"Optionally, the data can be decompressed from the source format in the "
"shared memory to the destination format in Tensor Memory during the copy "
"operation. Currently, only ``.b8x16`` is supported as destination format. "
"The valid source formats are ``.b6x16_p32`` and ``.b4x16_p64``."
msgstr ""

#: ../../../NVPTXUsage.rst:1696
msgid ""
"When the source format is ``.b6x16_p32``, a contiguous set of 16 elements of "
"6-bits each followed by four bytes of padding (``_p32``) in shared memory is "
"decompressed into 16 elements of 8-bits (``.b8x16``) each in the Tensor "
"Memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1700
msgid ""
"When the source format is ``.b4x16_p64``, a contiguous set of 16 elements of "
"4-bits each followed by eight bytes of padding (``_p64``) in shared memory "
"is decompressed into 16 elements of 8-bits (``.b8x16``) each in the Tensor "
"Memory."
msgstr ""

#: ../../../NVPTXUsage.rst:1704
msgid ""
"For more information on the decompression schemes, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/#optional-"
"decompression>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1707
msgid ""
"For more information on the tcgen05.cp instruction, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-"
"instructions-tcgen05-cp>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1711
msgid "'``llvm.nvvm.tcgen05.ld.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1725
msgid ""
"This group of intrinsics asynchronously load data from the Tensor Memory at "
"the location specified by the 32-bit address operand `tmem_addr` into the "
"destination registers, collectively across all threads of the warps."
msgstr ""

#: ../../../NVPTXUsage.rst:1729 ../../../NVPTXUsage.rst:1782
msgid ""
"All the threads in the warp must specify the same value of `tmem_addr`, "
"which must be the base address of the collective load operation. Otherwise, "
"the behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1732 ../../../NVPTXUsage.rst:1785
msgid ""
"The `shape` qualifier and the `num` qualifier together determines the total "
"dimension of the data ('n') which is loaded from the Tensor Memory. The "
"`shape` qualifier indicates the base dimension of data. The `num` qualifier "
"indicates the repeat factor on the base dimension resulting in the total "
"dimension of the data that is accessed."
msgstr ""

#: ../../../NVPTXUsage.rst:1736 ../../../NVPTXUsage.rst:1789
msgid "Allowed values for the 'num' are `x1, x2, x4, x8, x16, x32, x64, x128`."
msgstr ""

#: ../../../NVPTXUsage.rst:1738 ../../../NVPTXUsage.rst:1791
msgid ""
"Allowed values for the 'shape' in the first intrinsic are `16x64b, 16x128b, "
"16x256b, 32x32b`."
msgstr ""

#: ../../../NVPTXUsage.rst:1740 ../../../NVPTXUsage.rst:1793
msgid "Allowed value for the 'shape' in the second intrinsic is `16x32bx2`."
msgstr ""

#: ../../../NVPTXUsage.rst:1742
msgid ""
"The result of the intrinsic is a vector consisting of one or more 32-bit "
"registers derived from `shape` and `num` as shown below."
msgstr ""

#: ../../../NVPTXUsage.rst:1746
msgid "num/shape"
msgstr ""

#: ../../../NVPTXUsage.rst:1746
msgid "16x32bx2/16x64b/32x32b"
msgstr ""

#: ../../../NVPTXUsage.rst:1746
msgid "16x128b"
msgstr ""

#: ../../../NVPTXUsage.rst:1746
msgid "16x256b"
msgstr ""

#: ../../../NVPTXUsage.rst:1748
msgid "x1"
msgstr ""

#: ../../../NVPTXUsage.rst:1749
msgid "x2"
msgstr ""

#: ../../../NVPTXUsage.rst:1749 ../../../NVPTXUsage.rst:1750
#: ../../../NVPTXUsage.rst:1751
msgid "8"
msgstr ""

#: ../../../NVPTXUsage.rst:1750
msgid "x4"
msgstr ""

#: ../../../NVPTXUsage.rst:1750 ../../../NVPTXUsage.rst:1751
#: ../../../NVPTXUsage.rst:1752
msgid "16"
msgstr ""

#: ../../../NVPTXUsage.rst:1751
msgid "x8"
msgstr ""

#: ../../../NVPTXUsage.rst:1751 ../../../NVPTXUsage.rst:1752
#: ../../../NVPTXUsage.rst:1753
msgid "32"
msgstr ""

#: ../../../NVPTXUsage.rst:1752
msgid "x16"
msgstr ""

#: ../../../NVPTXUsage.rst:1752 ../../../NVPTXUsage.rst:1753
#: ../../../NVPTXUsage.rst:1754
msgid "64"
msgstr ""

#: ../../../NVPTXUsage.rst:1753
msgid "x32"
msgstr ""

#: ../../../NVPTXUsage.rst:1753 ../../../NVPTXUsage.rst:1754
#: ../../../NVPTXUsage.rst:1755
msgid "128"
msgstr ""

#: ../../../NVPTXUsage.rst:1754
msgid "x64"
msgstr ""

#: ../../../NVPTXUsage.rst:1754 ../../../NVPTXUsage.rst:1755
msgid "NA"
msgstr ""

#: ../../../NVPTXUsage.rst:1755
msgid "x128"
msgstr ""

#: ../../../NVPTXUsage.rst:1758
msgid ""
"The last argument `i1 %pack` is a compile-time constant which when set, "
"indicates that the adjacent columns are packed into a single 32-bit element "
"during the load"
msgstr ""

#: ../../../NVPTXUsage.rst:1760
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-ld>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1765
msgid "'``llvm.nvvm.tcgen05.st.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1779
msgid ""
"This group of intrinsics asynchronously store data from the source vector "
"into the Tensor Memory at the location specified by the 32-bit address "
"operand 'tmem_addr` collectively across all threads of the warps."
msgstr ""

#: ../../../NVPTXUsage.rst:1795
msgid ""
"`args` argument is a vector consisting of one or more 32-bit registers "
"derived from `shape` and `num` as listed in the table listed in the `tcgen05."
"ld` section."
msgstr ""

#: ../../../NVPTXUsage.rst:1798
msgid ""
"Each shape support an `unpack` mode to allow a 32-bit element in the "
"register to be unpacked into two 16-bit elements and store them in adjacent "
"columns. `unpack` mode can be enabled by setting the `%unpack` operand to 1 "
"and can be disabled by setting it to 0."
msgstr ""

#: ../../../NVPTXUsage.rst:1800
msgid ""
"The last argument `i1 %unpack` is a compile-time constant which when set, "
"indicates that a 32-bit element in the register to be unpacked into two 16-"
"bit elements and store them in adjacent columns."
msgstr ""

#: ../../../NVPTXUsage.rst:1802
msgid ""
"For more information, refer to the `PTX ISA <https://docs.nvidia.com/cuda/"
"parallel-thread-execution/#tcgen05-instructions-tcgen05-st>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1806
msgid "Store Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1809
msgid "'``llvm.nvvm.st.bulk.*``'"
msgstr ""

#: ../../../NVPTXUsage.rst:1822
msgid ""
"The '``@llvm.nvvm.st.bulk.*``' intrinsics initialize a region of shared "
"memory starting from the location specified by the destination address "
"operand `%dst`."
msgstr ""

#: ../../../NVPTXUsage.rst:1825
msgid ""
"The integer operand `%size` specifies the amount of memory to be initialized "
"in terms of number of bytes and must be a multiple of 8. Otherwise, the "
"behavior is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1829
msgid ""
"The integer immediate operand `%initval` specifies the initialization value "
"for the memory locations. The only numeric value allowed is 0."
msgstr ""

#: ../../../NVPTXUsage.rst:1832
msgid ""
"The ``@llvm.nvvm.st.bulk.shared.cta`` and ``@llvm.nvvm.st.bulk`` intrinsics "
"are similar but the latter uses generic addressing (see `Generic Addressing "
"<https://docs.nvidia.com/cuda/parallel-thread-execution/#generic-"
"addressing>`__)."
msgstr ""

#: ../../../NVPTXUsage.rst:1835
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/#data-movement-and-conversion-instructions-st-bulk>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1839
msgid "clusterlaunchcontrol Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1842
msgid "'``llvm.nvvm.clusterlaunchcontrol.try_cancel*``' Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1855
msgid ""
"The ``clusterlaunchcontrol.try_cancel`` intrinsics requests atomically "
"cancelling the launch of a cluster that has not started running yet. It "
"asynchronously non-atomically writes a 16-byte opaque response to shared "
"memory, pointed to by 16-byte-aligned ``addr`` indicating whether the "
"operation succeeded or failed. ``addr`` and 8-byte-aligned ``mbar`` must "
"refer to ``shared::cta`` otherwise the behavior is undefined. The completion "
"of the asynchronous operation is tracked using the mbarrier completion "
"mechanism at ``.cluster`` scope referenced by the shared memory pointer, "
"``mbar``. On success, the opaque response contains the CTA id of the first "
"CTA of the canceled cluster; no other successful response from other "
"``clusterlaunchcontrol.try_cancel`` operations from the same grid will "
"contain that id."
msgstr ""

#: ../../../NVPTXUsage.rst:1866
msgid ""
"The ``multicast`` variant specifies that the response is asynchronously non-"
"atomically written to the corresponding shared memory location of each CTA "
"in the requesting cluster. The completion of the write of each local "
"response is tracked by independent mbarriers at the corresponding shared "
"memory location of each CTA in the cluster."
msgstr ""

#: ../../../NVPTXUsage.rst:1872
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/?a#parallel-synchronization-and-communication-instructions-"
"clusterlaunchcontrol-try-cancel>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1875
msgid "'``llvm.nvvm.clusterlaunchcontrol.query_cancel.is_canceled``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:1887
msgid ""
"The ``llvm.nvvm.clusterlaunchcontrol.query_cancel.is_canceled`` intrinsic "
"decodes the opaque response written by the ``llvm.nvvm.clusterlaunchcontrol."
"try_cancel`` operation."
msgstr ""

#: ../../../NVPTXUsage.rst:1890
msgid ""
"The intrinsic returns ``0`` (false) if the request failed. If the request "
"succeeded, it returns ``1`` (true). A true result indicates that:"
msgstr ""

#: ../../../NVPTXUsage.rst:1893
msgid ""
"the thread block cluster whose first CTA id matches that of the response "
"handle will not run, and"
msgstr ""

#: ../../../NVPTXUsage.rst:1895
msgid ""
"no other successful response of another ``try_cancel`` request in the grid "
"will contain the first CTA id of that cluster"
msgstr ""

#: ../../../NVPTXUsage.rst:1898 ../../../NVPTXUsage.rst:1927
msgid ""
"For more information, refer `PTX ISA <https://docs.nvidia.com/cuda/parallel-"
"thread-execution/?a#parallel-synchronization-and-communication-instructions-"
"clusterlaunchcontrol-query-cancel>`__."
msgstr ""

#: ../../../NVPTXUsage.rst:1902
msgid ""
"'``llvm.nvvm.clusterlaunchcontrol.query_cancel.get_first_ctaid.*``' "
"Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1916
msgid ""
"The ``clusterlaunchcontrol.query_cancel.get_first_ctaid.*`` intrinsic can be "
"used to decode the successful opaque response written by the ``llvm.nvvm."
"clusterlaunchcontrol.try_cancel`` operation."
msgstr ""

#: ../../../NVPTXUsage.rst:1920
msgid "If the request succeeded:"
msgstr ""

#: ../../../NVPTXUsage.rst:1922
msgid ""
"``llvm.nvvm.clusterlaunchcontrol.query_cancel.get_first_ctaid.{x,y,z}`` "
"returns the coordinate of the first CTA in the canceled cluster, either x, "
"y, or z."
msgstr ""

#: ../../../NVPTXUsage.rst:1925
msgid "If the request failed, the behavior of these intrinsics is undefined."
msgstr ""

#: ../../../NVPTXUsage.rst:1930
msgid "Perf Monitor Event Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1933
msgid "'``llvm.nvvm.pm.event.mask``' Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:1945
msgid ""
"The '``llvm.nvvm.pm.event.mask``' intrinsic triggers one or more performance "
"monitor events. Each bit in the 16-bit immediate operand ``%mask_val`` "
"controls an event."
msgstr ""

#: ../../../NVPTXUsage.rst:1949
msgid ""
"For more information on the pmevent instructions, refer to the PTX ISA "
"`<https://docs.nvidia.com/cuda/parallel-thread-execution/index."
"html#miscellaneous-instructions-pmevent>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1953
msgid "Other Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:1955
msgid ""
"For the full set of NVPTX intrinsics, please see the ``include/llvm/IR/"
"IntrinsicsNVVM.td`` file in the LLVM source tree."
msgstr ""

#: ../../../NVPTXUsage.rst:1962
msgid "Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:1964
msgid ""
"The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` "
"that implements many common mathematical functions. This library can be used "
"as a high-performance math library for any compilers using the LLVM NVPTX "
"target. The library can be found under ``nvvm/libdevice/`` in the CUDA "
"Toolkit and there is a separate version for each compute architecture."
msgstr ""

#: ../../../NVPTXUsage.rst:1970
msgid ""
"For a list of all math functions implemented in libdevice, see `libdevice "
"Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_."
msgstr ""

#: ../../../NVPTXUsage.rst:1973
msgid ""
"To accommodate various math-related compiler flags that can affect code "
"generation of libdevice code, the library code depends on a special LLVM IR "
"pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. "
"This pass looks for calls to the ``@__nvvm_reflect`` function and replaces "
"them with constants based on the defined reflection parameters. Such "
"conditional code often follows a pattern:"
msgstr ""

#: ../../../NVPTXUsage.rst:1989
msgid "The default value for all unspecified reflection parameters is zero."
msgstr ""

#: ../../../NVPTXUsage.rst:1991
msgid ""
"The ``NVVMReflect`` pass should be executed early in the optimization "
"pipeline, immediately after the link stage. The ``internalize`` pass is also "
"recommended to remove unused math functions from the resulting PTX. For an "
"input IR module ``module.bc``, the following compilation flow is recommended:"
msgstr ""

#: ../../../NVPTXUsage.rst:1996
msgid ""
"The ``NVVMReflect`` pass will attempt to remove dead code even without "
"optimizations. This allows potentially incompatible instructions to be "
"avoided at all optimizations levels by using the ``__CUDA_ARCH`` argument."
msgstr ""

#: ../../../NVPTXUsage.rst:2000
msgid "Save list of external functions in ``module.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:2001
msgid "Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``"
msgstr ""

#: ../../../NVPTXUsage.rst:2002
msgid "Internalize all functions not in list from (1)"
msgstr ""

#: ../../../NVPTXUsage.rst:2003
msgid "Eliminate all unused internal functions"
msgstr ""

#: ../../../NVPTXUsage.rst:2004
msgid "Run ``NVVMReflect`` pass"
msgstr ""

#: ../../../NVPTXUsage.rst:2005
msgid "Run standard optimization pipeline"
msgstr ""

#: ../../../NVPTXUsage.rst:2009
msgid ""
"``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the "
"libdevice functions. It is possible to link two IR modules that have been "
"linked against libdevice using different reflection variables."
msgstr ""

#: ../../../NVPTXUsage.rst:2013
msgid ""
"Since the ``NVVMReflect`` pass replaces conditionals with constants, it will "
"often leave behind dead code of the form:"
msgstr ""

#: ../../../NVPTXUsage.rst:2027
msgid ""
"Therefore, it is recommended that ``NVVMReflect`` is executed early in the "
"optimization pipeline before dead-code elimination."
msgstr ""

#: ../../../NVPTXUsage.rst:2030
msgid ""
"The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the "
"beginning of your pass manager; just use the following code when setting up "
"your pass manager and the PassBuilder will use "
"``registerPassBuilderCallbacks`` to let NVPTXTargetMachine::"
"registerPassBuilderCallbacks add the pass to the pass manager:"
msgstr ""

#: ../../../NVPTXUsage.rst:2044
msgid "Reflection Parameters"
msgstr ""

#: ../../../NVPTXUsage.rst:2046
msgid ""
"The libdevice library currently uses the following reflection parameters to "
"control code generation:"
msgstr ""

#: ../../../NVPTXUsage.rst:2050
msgid "Flag"
msgstr ""

#: ../../../NVPTXUsage.rst:2050
msgid "Description"
msgstr ""

#: ../../../NVPTXUsage.rst:2052
msgid "``__CUDA_FTZ=[0,1]``"
msgstr ""

#: ../../../NVPTXUsage.rst:2052
msgid "Use optimized code paths that flush subnormals to zero"
msgstr ""

#: ../../../NVPTXUsage.rst:2055
msgid ""
"The value of this flag is determined by the \"nvvm-reflect-ftz\" module "
"flag. The following sets the ftz flag to 1."
msgstr ""

#: ../../../NVPTXUsage.rst:2063
msgid ""
"(``i32 4`` indicates that the value set here overrides the value in another "
"module we link with.  See the `LangRef <LangRef.html#module-flags-metadata>` "
"for details.)"
msgstr ""

#: ../../../NVPTXUsage.rst:2068
msgid "Executing PTX"
msgstr ""

#: ../../../NVPTXUsage.rst:2070
msgid ""
"The most common way to execute PTX assembly on a GPU device is to use the "
"CUDA Driver API. This API is a low-level interface to the GPU driver and "
"allows for JIT compilation of PTX code to native GPU machine code."
msgstr ""

#: ../../../NVPTXUsage.rst:2074
msgid "Initializing the Driver API:"
msgstr ""

#: ../../../NVPTXUsage.rst:2088
msgid "JIT compiling a PTX string to a device binary:"
msgstr ""

#: ../../../NVPTXUsage.rst:2101
msgid ""
"For full examples of executing PTX assembly, please see the `CUDA Samples "
"<https://developer.nvidia.com/cuda-downloads>`_ distribution."
msgstr ""

#: ../../../NVPTXUsage.rst:2106
msgid "Common Issues"
msgstr ""

#: ../../../NVPTXUsage.rst:2109
msgid "ptxas complains of undefined function: __nvvm_reflect"
msgstr ""

#: ../../../NVPTXUsage.rst:2111
msgid ""
"When linking with libdevice, the ``NVVMReflect`` pass must be used. See :ref:"
"`libdevice` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:2116
msgid "Tutorial: A Simple Compute Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2118
msgid ""
"To start, let us take a look at a simple compute kernel written directly in "
"LLVM IR. The kernel implements vector addition, where each thread computes "
"one element of the output vector C from the input vectors A and B.  To make "
"this easier, we also assume that only a single CTA (thread block) will be "
"launched, and that it will be one dimensional."
msgstr ""

#: ../../../NVPTXUsage.rst:2126
msgid "The Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2165
msgid ""
"We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:"
msgstr ""

#: ../../../NVPTXUsage.rst:2174
msgid ""
"If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32`` "
"in the module data layout string and use ``nvptx-nvidia-cuda`` as the target "
"triple."
msgstr ""

#: ../../../NVPTXUsage.rst:2179
msgid "The output we get from ``llc`` (as of LLVM 3.4):"
msgstr ""

#: ../../../NVPTXUsage.rst:2221
msgid "Dissecting the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2223
msgid "Now let us dissect the LLVM IR that makes up this kernel."
msgstr ""

#: ../../../NVPTXUsage.rst:2226
msgid "Data Layout"
msgstr ""

#: ../../../NVPTXUsage.rst:2228
msgid ""
"The data layout string determines the size in bits of common data types, "
"their ABI alignment, and their storage size.  For NVPTX, you should use one "
"of the following:"
msgstr ""

#: ../../../NVPTXUsage.rst:2232
msgid "32-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:2238
msgid "64-bit PTX:"
msgstr ""

#: ../../../NVPTXUsage.rst:2246
msgid "Target Intrinsics"
msgstr ""

#: ../../../NVPTXUsage.rst:2248
msgid ""
"In this example, we use the ``@llvm.nvvm.read.ptx.sreg.tid.x`` intrinsic to "
"read the X component of the current thread's ID, which corresponds to a read "
"of register ``%tid.x`` in PTX. The NVPTX back-end supports a large set of "
"intrinsics.  A short list is shown below; please see ``include/llvm/IR/"
"IntrinsicsNVVM.td`` for the full list."
msgstr ""

#: ../../../NVPTXUsage.rst:2256
msgid "Intrinsic"
msgstr ""

#: ../../../NVPTXUsage.rst:2256
msgid "CUDA Equivalent"
msgstr ""

#: ../../../NVPTXUsage.rst:2258
msgid "``i32 @llvm.nvvm.read.ptx.sreg.tid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2258
msgid "threadIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2259
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2259
msgid "blockIdx.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2260
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ntid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2260
msgid "blockDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2261
msgid "``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}``"
msgstr ""

#: ../../../NVPTXUsage.rst:2261
msgid "gridDim.{x,y,z}"
msgstr ""

#: ../../../NVPTXUsage.rst:2262
msgid "``void @llvm.nvvm.barrier0()``"
msgstr ""

#: ../../../NVPTXUsage.rst:2262
msgid "__syncthreads()"
msgstr ""

#: ../../../NVPTXUsage.rst:2269
msgid ""
"You may have noticed that all of the pointer types in the LLVM IR example "
"had an explicit address space specifier. What is address space 1? NVIDIA GPU "
"devices (generally) have four types of memory:"
msgstr ""

#: ../../../NVPTXUsage.rst:2273
msgid "Global: Large, off-chip memory"
msgstr ""

#: ../../../NVPTXUsage.rst:2274
msgid "Shared: Small, on-chip memory shared among all threads in a CTA"
msgstr ""

#: ../../../NVPTXUsage.rst:2275
msgid "Local: Per-thread, private memory"
msgstr ""

#: ../../../NVPTXUsage.rst:2276
msgid "Constant: Read-only memory shared across all threads"
msgstr ""

#: ../../../NVPTXUsage.rst:2278
msgid ""
"These different types of memory are represented in LLVM IR as address "
"spaces. There is also a fifth address space used by the NVPTX code generator "
"that corresponds to the \"generic\" address space.  This address space can "
"represent addresses in any other address space (with a few exceptions).  "
"This allows users to write IR functions that can load/store memory using the "
"same instructions. Intrinsics are provided to convert pointers between the "
"generic and non-generic address spaces."
msgstr ""

#: ../../../NVPTXUsage.rst:2286
msgid ""
"See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information."
msgstr ""

#: ../../../NVPTXUsage.rst:2290
msgid "Kernel Metadata"
msgstr ""

#: ../../../NVPTXUsage.rst:2292
msgid ""
"In PTX, a function can be either a `kernel` function (callable from the host "
"program), or a `device` function (callable only from GPU code). You can "
"think of `kernel` functions as entry-points in the GPU program. To mark an "
"LLVM IR function as a `kernel` function, we make use of special LLVM "
"metadata. The NVPTX back-end will look for a named metadata node called "
"``nvvm.annotations``. This named metadata must contain a list of metadata "
"that describe the IR. For our purposes, we need to declare a metadata node "
"that assigns the \"kernel\" attribute to the LLVM IR function that should be "
"emitted as a PTX `kernel` function. These metadata nodes take the form:"
msgstr ""

#: ../../../NVPTXUsage.rst:2306
msgid "For the previous example, we have:"
msgstr ""

#: ../../../NVPTXUsage.rst:2313
msgid ""
"Here, we have a single metadata declaration in ``nvvm.annotations``. This "
"metadata annotates our ``@kernel`` function with the ``kernel`` attribute."
msgstr ""

#: ../../../NVPTXUsage.rst:2318
msgid "Running the Kernel"
msgstr ""

#: ../../../NVPTXUsage.rst:2320
msgid ""
"Generating PTX from LLVM IR is all well and good, but how do we execute it "
"on a real GPU device? The CUDA Driver API provides a convenient mechanism "
"for loading and JIT compiling PTX to a native GPU device, and launching a "
"kernel. The API is similar to OpenCL.  A simple example showing how to load "
"and execute our vector addition code is shown below. Note that for brevity "
"this code does not perform much error checking!"
msgstr ""

#: ../../../NVPTXUsage.rst:2329
msgid ""
"You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline "
"compile PTX to machine code (SASS) for a specific GPU architecture. Such "
"binaries can be loaded by the CUDA Driver API in the same way as PTX. This "
"can be useful for reducing startup time by precompiling the PTX kernels."
msgstr ""

#: ../../../NVPTXUsage.rst:2458
msgid ""
"You will need to link with the CUDA driver and specify the path to cuda.h."
msgstr ""

#: ../../../NVPTXUsage.rst:2464
msgid ""
"We don't need to specify a path to ``libcuda.so`` since this is installed in "
"a system location by the driver, not the CUDA toolkit."
msgstr ""

#: ../../../NVPTXUsage.rst:2467
msgid ""
"If everything goes as planned, you should see the following output when "
"running the compiled program:"
msgstr ""

#: ../../../NVPTXUsage.rst:2495
msgid ""
"You will likely see a different device identifier based on your hardware"
msgstr ""

#: ../../../NVPTXUsage.rst:2499
msgid "Tutorial: Linking with Libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:2501
msgid ""
"In this tutorial, we show a simple example of linking LLVM IR with the "
"libdevice library. We will use the same kernel as the previous tutorial, "
"except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``. "
"Libdevice provides an ``__nv_powf`` function that we will use."
msgstr ""

#: ../../../NVPTXUsage.rst:2545
msgid "To compile this kernel, we perform the following steps:"
msgstr ""

#: ../../../NVPTXUsage.rst:2547
msgid "Link with libdevice"
msgstr ""

#: ../../../NVPTXUsage.rst:2548
msgid "Internalize all but the public kernel function"
msgstr ""

#: ../../../NVPTXUsage.rst:2549
msgid "Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0"
msgstr ""

#: ../../../NVPTXUsage.rst:2550
msgid "Optimize the linked module"
msgstr ""

#: ../../../NVPTXUsage.rst:2551
msgid "Codegen the module"
msgstr ""

#: ../../../NVPTXUsage.rst:2554
msgid ""
"These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc`` "
"tools. In a complete compiler, these steps can also be performed entirely "
"programmatically by setting up an appropriate pass configuration (see :ref:"
"`libdevice`)."
msgstr ""

#: ../../../NVPTXUsage.rst:2567
msgid ""
"The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any "
"undefined variables will default to zero. It is shown here for evaluation "
"purposes."
msgstr ""

#: ../../../NVPTXUsage.rst:2572
msgid "This gives us the following PTX (excerpt):"
msgstr ""
