
<!DOCTYPE html>

<html lang="fr-FR" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Compiling CUDA C/C++ with LLVM &#8212; Documentation LLVM 3.9</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="_static/llvm-theme.css?v=4c4af0c1" />
    <script src="_static/documentation_options.js?v=9c8a3c23"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/translations.js?v=aa914e54"></script>
    <link rel="canonical" href="https://projects.localizethedocs.org/llvm-docs-l10n/CompileCudaWithLLVM.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" />
    <link rel="next" title="Reporting Guide" href="ReportingGuide.html" />
    <link rel="prev" title="LLVM Community Code of Conduct" href="CodeOfConduct.html" />
<script type="text/javascript" src="ltd-provenance.js"></script>
<script type="text/javascript" src="ltd-current.js"></script>
<script type="text/javascript" src="../../ltd-config.js"></script>
<script type="text/javascript" src="../../ltd-flyout.js"></script>

  </head><body>
<div class="logo">
  <a href="index.html">
    <img src="_static/logo.png"
         alt="LLVM Logo" width="250" height="88"/></a>
</div>

    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Index général"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="ReportingGuide.html" title="Reporting Guide"
             accesskey="N">suivant</a> |</li>
        <li class="right" >
          <a href="CodeOfConduct.html" title="LLVM Community Code of Conduct"
             accesskey="P">précédent</a> |</li>
  <li><a href="https://llvm.org/">LLVM Home</a>&nbsp;|&nbsp;</li>
  <li><a href="index.html">Documentation</a>&raquo;</li>

        <li class="nav-item nav-item-this"><a href="">Compiling CUDA C/C++ with LLVM</a></li> 
      </ul>
    </div>


    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <section id="compiling-cuda-c-c-with-llvm">
<h1>Compiling CUDA C/C++ with LLVM<a class="headerlink" href="#compiling-cuda-c-c-with-llvm" title="Lien vers cette rubrique">¶</a></h1>
<nav class="contents local" id="sommaire">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#how-to-build-llvm-with-cuda-support" id="id2">How to Build LLVM with CUDA Support</a></p></li>
<li><p><a class="reference internal" href="#how-to-compile-cuda-c-c-with-llvm" id="id3">How to Compile CUDA C/C++ with LLVM</a></p></li>
<li><p><a class="reference internal" href="#detecting-clang-vs-nvcc" id="id4">Detecting clang vs NVCC</a></p></li>
<li><p><a class="reference internal" href="#flags-that-control-numerical-code" id="id5">Flags that control numerical code</a></p></li>
<li><p><a class="reference internal" href="#optimizations" id="id6">Optimizations</a></p></li>
<li><p><a class="reference internal" href="#publication" id="id7">Publication</a></p></li>
<li><p><a class="reference internal" href="#tutorial" id="id8">Tutorial</a></p></li>
<li><p><a class="reference internal" href="#obtaining-help" id="id9">Obtaining Help</a></p></li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Lien vers cette rubrique">¶</a></h2>
<p>This document contains the user guides and the internals of compiling CUDA
C/C++ with LLVM. It is aimed at both users who want to compile CUDA with LLVM
and developers who want to improve LLVM for GPUs. This document assumes a basic
familiarity with CUDA. Information about CUDA programming can be found in the
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA programming guide</a>.</p>
</section>
<section id="how-to-build-llvm-with-cuda-support">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">How to Build LLVM with CUDA Support</a><a class="headerlink" href="#how-to-build-llvm-with-cuda-support" title="Lien vers cette rubrique">¶</a></h2>
<p>CUDA support is still in development and works the best in the trunk version
of LLVM. Below is a quick summary of downloading and building the trunk
version. Consult the <a class="reference external" href="http://llvm.org/docs/GettingStarted.html">Getting Started</a> page for more details on setting
up LLVM.</p>
<ol class="arabic">
<li><p>Checkout LLVM</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>where-you-want-llvm-to-live
<span class="gp">$ </span>svn<span class="w"> </span>co<span class="w"> </span>http://llvm.org/svn/llvm-project/llvm/trunk<span class="w"> </span>llvm
</pre></div>
</div>
</li>
<li><p>Checkout Clang</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>where-you-want-llvm-to-live
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>llvm/tools
<span class="gp">$ </span>svn<span class="w"> </span>co<span class="w"> </span>http://llvm.org/svn/llvm-project/cfe/trunk<span class="w"> </span>clang
</pre></div>
</div>
</li>
<li><p>Configure and build LLVM and Clang</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>where-you-want-llvm-to-live
<span class="gp">$ </span>mkdir<span class="w"> </span>build
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>build
<span class="gp">$ </span>cmake<span class="w"> </span><span class="o">[</span>options<span class="o">]</span><span class="w"> </span>..
<span class="gp">$ </span>make
</pre></div>
</div>
</li>
</ol>
</section>
<section id="how-to-compile-cuda-c-c-with-llvm">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">How to Compile CUDA C/C++ with LLVM</a><a class="headerlink" href="#how-to-compile-cuda-c-c-with-llvm" title="Lien vers cette rubrique">¶</a></h2>
<p>We assume you have installed the CUDA driver and runtime. Consult the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">NVIDIA
CUDA installation guide</a> if
you have not.</p>
<p>Suppose you want to compile and run the following CUDA program (<code class="docutils literal notranslate"><span class="pre">axpy.cu</span></code>)
which multiplies a <code class="docutils literal notranslate"><span class="pre">float</span></code> array by a <code class="docutils literal notranslate"><span class="pre">float</span></code> scalar (AXPY).</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">axpy</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">y</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">argv</span><span class="p">[])</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">kDataLen</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>

<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">;</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="n">host_x</span><span class="p">[</span><span class="n">kDataLen</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">1.0f</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">,</span><span class="w"> </span><span class="mf">3.0f</span><span class="p">,</span><span class="w"> </span><span class="mf">4.0f</span><span class="p">};</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="n">host_y</span><span class="p">[</span><span class="n">kDataLen</span><span class="p">];</span>

<span class="w">  </span><span class="c1">// Copy input data to device.</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">device_x</span><span class="p">;</span>
<span class="w">  </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">device_y</span><span class="p">;</span>
<span class="w">  </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_x</span><span class="p">,</span><span class="w"> </span><span class="n">kDataLen</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device_y</span><span class="p">,</span><span class="w"> </span><span class="n">kDataLen</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_x</span><span class="p">,</span><span class="w"> </span><span class="n">host_x</span><span class="p">,</span><span class="w"> </span><span class="n">kDataLen</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">             </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Launch the kernel.</span>
<span class="w">  </span><span class="n">axpy</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">kDataLen</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">device_x</span><span class="p">,</span><span class="w"> </span><span class="n">device_y</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Copy output data to host.</span>
<span class="w">  </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="w">  </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">host_y</span><span class="p">,</span><span class="w"> </span><span class="n">device_y</span><span class="p">,</span><span class="w"> </span><span class="n">kDataLen</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">             </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Print the results.</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">kDataLen</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;y[&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;] = &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">host_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">cudaDeviceReset</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The command line for compilation is similar to what you would use for C++.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>clang++<span class="w"> </span>axpy.cu<span class="w"> </span>-o<span class="w"> </span>axpy<span class="w"> </span>--cuda-gpu-arch<span class="o">=</span>&lt;GPU<span class="w"> </span>arch&gt;<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>-L&lt;CUDA<span class="w"> </span>install<span class="w"> </span>path&gt;/&lt;lib64<span class="w"> </span>or<span class="w"> </span>lib&gt;<span class="w">              </span><span class="se">\</span>
<span class="w">    </span>-lcudart_static<span class="w"> </span>-ldl<span class="w"> </span>-lrt<span class="w"> </span>-pthread
<span class="gp">$ </span>./axpy
<span class="go">y[0] = 2</span>
<span class="go">y[1] = 4</span>
<span class="go">y[2] = 6</span>
<span class="go">y[3] = 8</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;CUDA</span> <span class="pre">install</span> <span class="pre">path&gt;</span></code> is the root directory where you installed CUDA SDK,
typically <code class="docutils literal notranslate"><span class="pre">/usr/local/cuda</span></code>. <code class="docutils literal notranslate"><span class="pre">&lt;GPU</span> <span class="pre">arch&gt;</span></code> is <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">the compute capability of
your GPU</a>. For example, if you want
to run your program on a GPU with compute capability of 3.5, you should specify
<code class="docutils literal notranslate"><span class="pre">--cuda-gpu-arch=sm_35</span></code>.</p>
</section>
<section id="detecting-clang-vs-nvcc">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Detecting clang vs NVCC</a><a class="headerlink" href="#detecting-clang-vs-nvcc" title="Lien vers cette rubrique">¶</a></h2>
<p>Although clang’s CUDA implementation is largely compatible with NVCC’s, you may
still want to detect when you’re compiling CUDA code specifically with clang.</p>
<p>This is tricky, because NVCC may invoke clang as part of its own compilation
process!  For example, NVCC uses the host compiler’s preprocessor when
compiling for device code, and that host compiler may in fact be clang.</p>
<p>When clang is actually compiling CUDA code – rather than being used as a
subtool of NVCC’s – it defines the <code class="docutils literal notranslate"><span class="pre">__CUDA__</span></code> macro.  <code class="docutils literal notranslate"><span class="pre">__CUDA_ARCH__</span></code> is
defined only in device mode (but will be defined if NVCC is using clang as a
preprocessor).  So you can use the following incantations to detect clang CUDA
compilation, in host and device modes:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#if defined(__clang__) &amp;&amp; defined(__CUDA__) &amp;&amp; !defined(__CUDA_ARCH__)</span>
<span class="w">  </span><span class="c1">// clang compiling CUDA code, host mode.</span>
<span class="cp">#endif</span>

<span class="cp">#if defined(__clang__) &amp;&amp; defined(__CUDA__) &amp;&amp; defined(__CUDA_ARCH__)</span>
<span class="w">  </span><span class="c1">// clang compiling CUDA code, device mode.</span>
<span class="cp">#endif</span>
</pre></div>
</div>
<p>Both clang and nvcc define <code class="docutils literal notranslate"><span class="pre">__CUDACC__</span></code> during CUDA compilation.  You can
detect NVCC specifically by looking for <code class="docutils literal notranslate"><span class="pre">__NVCC__</span></code>.</p>
</section>
<section id="flags-that-control-numerical-code">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Flags that control numerical code</a><a class="headerlink" href="#flags-that-control-numerical-code" title="Lien vers cette rubrique">¶</a></h2>
<p>If you’re using GPUs, you probably care about making numerical code run fast.
GPU hardware allows for more control over numerical operations than most CPUs,
but this results in more compiler options for you to juggle.</p>
<p>Flags you may wish to tweak include:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">-ffp-contract={on,off,fast}</span></code> (defaults to <code class="docutils literal notranslate"><span class="pre">fast</span></code> on host and device when
compiling CUDA) Controls whether the compiler emits fused multiply-add
operations.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">off</span></code>: never emit fma operations, and prevent ptxas from fusing multiply
and add instructions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">on</span></code>: fuse multiplies and adds within a single statement, but never
across statements (C11 semantics).  Prevent ptxas from fusing other
multiplies and adds.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fast</span></code>: fuse multiplies and adds wherever profitable, even across
statements.  Doesn’t prevent ptxas from fusing additional multiplies and
adds.</p></li>
</ul>
<p>Fused multiply-add instructions can be much faster than the unfused
equivalents, but because the intermediate result in an fma is not rounded,
this flag can affect numerical code.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-fcuda-flush-denormals-to-zero</span></code> (default: off) When this is enabled,
floating point operations may flush <a class="reference external" href="https://en.wikipedia.org/wiki/Denormal_number">denormal</a> inputs and/or outputs to 0.
Operations on denormal numbers are often much slower than the same operations
on normal numbers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-fcuda-approx-transcendentals</span></code> (default: off) When this is enabled, the
compiler may emit calls to faster, approximate versions of transcendental
functions, instead of using the slower, fully IEEE-compliant versions.  For
example, this flag allows clang to emit the ptx <code class="docutils literal notranslate"><span class="pre">sin.approx.f32</span></code>
instruction.</p>
<p>This is implied by <code class="docutils literal notranslate"><span class="pre">-ffast-math</span></code>.</p>
</li>
</ul>
</section>
<section id="optimizations">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Optimizations</a><a class="headerlink" href="#optimizations" title="Lien vers cette rubrique">¶</a></h2>
<p>CPU and GPU have different design philosophies and architectures. For example, a
typical CPU has branch prediction, out-of-order execution, and is superscalar,
whereas a typical GPU has none of these. Due to such differences, an
optimization pipeline well-tuned for CPUs may be not suitable for GPUs.</p>
<p>LLVM performs several general and CUDA-specific optimizations for GPUs. The
list below shows some of the more important optimizations for GPUs. Most of
them have been upstreamed to <code class="docutils literal notranslate"><span class="pre">lib/Transforms/Scalar</span></code> and
<code class="docutils literal notranslate"><span class="pre">lib/Target/NVPTX</span></code>. A few of them have not been upstreamed due to lack of a
customizable target-independent optimization pipeline.</p>
<ul class="simple">
<li><p><strong>Straight-line scalar optimizations</strong>. These optimizations reduce redundancy
in straight-line code. Details can be found in the <a class="reference external" href="https://goo.gl/4Rb9As">design document for
straight-line scalar optimizations</a>.</p></li>
<li><p><strong>Inferring memory spaces</strong>. <a class="reference external" href="https://github.com/llvm-mirror/llvm/blob/master/lib/Target/NVPTX/NVPTXInferAddressSpaces.cpp">This optimization</a>
infers the memory space of an address so that the backend can emit faster
special loads and stores from it.</p></li>
<li><p><strong>Aggressive loop unrooling and function inlining</strong>. Loop unrolling and
function inlining need to be more aggressive for GPUs than for CPUs because
control flow transfer in GPU is more expensive. They also promote other
optimizations such as constant propagation and SROA which sometimes speed up
code by over 10x. An empirical inline threshold for GPUs is 1100. This
configuration has yet to be upstreamed with a target-specific optimization
pipeline. LLVM also provides <a class="reference external" href="http://clang.llvm.org/docs/AttributeReference.html#pragma-unroll-pragma-nounroll">loop unrolling pragmas</a>
and <code class="docutils literal notranslate"><span class="pre">__attribute__((always_inline))</span></code> for programmers to force unrolling and
inling.</p></li>
<li><p><strong>Aggressive speculative execution</strong>. <a class="reference external" href="http://llvm.org/docs/doxygen/html/SpeculativeExecution_8cpp_source.html">This transformation</a> is
mainly for promoting straight-line scalar optimizations which are most
effective on code along dominator paths.</p></li>
<li><p><strong>Memory-space alias analysis</strong>. <a class="reference external" href="http://reviews.llvm.org/D12414">This alias analysis</a> infers that two pointers in different
special memory spaces do not alias. It has yet to be integrated to the new
alias analysis infrastructure; the new infrastructure does not run
target-specific alias analysis.</p></li>
<li><p><strong>Bypassing 64-bit divides</strong>. <a class="reference external" href="http://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html">An existing optimization</a>
enabled in the NVPTX backend. 64-bit integer divides are much slower than
32-bit ones on NVIDIA GPUs due to lack of a divide unit. Many of the 64-bit
divides in our benchmarks have a divisor and dividend which fit in 32-bits at
runtime. This optimization provides a fast path for this common case.</p></li>
</ul>
</section>
<section id="publication">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Publication</a><a class="headerlink" href="#publication" title="Lien vers cette rubrique">¶</a></h2>
<div class="line-block">
<div class="line"><a class="reference external" href="http://dl.acm.org/citation.cfm?id=2854041">gpucc: An Open-Source GPGPU Compiler</a></div>
<div class="line">Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan, Chris Leary, Jacques Pienaar, Bjarke Roune, Rob Springer, Xuetian Weng, Robert Hundt</div>
<div class="line"><em>Proceedings of the 2016 International Symposium on Code Generation and Optimization (CGO 2016)</em></div>
<div class="line"><a class="reference external" href="http://wujingyue.com/docs/gpucc-talk.pdf">Slides for the CGO talk</a></div>
</div>
</section>
<section id="tutorial">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Tutorial</a><a class="headerlink" href="#tutorial" title="Lien vers cette rubrique">¶</a></h2>
<p><a class="reference external" href="http://wujingyue.com/docs/gpucc-tutorial.pdf">CGO 2016 gpucc tutorial</a></p>
</section>
<section id="obtaining-help">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Obtaining Help</a><a class="headerlink" href="#obtaining-help" title="Lien vers cette rubrique">¶</a></h2>
<p>To obtain help on LLVM in general and its CUDA support, see <a class="reference external" href="http://llvm.org/docs/#mailing-lists">the LLVM
community</a>.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Index général"
             >index</a></li>
        <li class="right" >
          <a href="ReportingGuide.html" title="Reporting Guide"
             >suivant</a> |</li>
        <li class="right" >
          <a href="CodeOfConduct.html" title="LLVM Community Code of Conduct"
             >précédent</a> |</li>
  <li><a href="https://llvm.org/">LLVM Home</a>&nbsp;|&nbsp;</li>
  <li><a href="index.html">Documentation</a>&raquo;</li>

        <li class="nav-item nav-item-this"><a href="">Compiling CUDA C/C++ with LLVM</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2003-2025, LLVM Project.
      Mis à jour le 2025-11-11.
      Créé en utilisant <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>