# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-07 18:11+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../CompileCudaWithLLVM.rst:3
msgid "Compiling CUDA C/C++ with LLVM"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:9
msgid "Introduction"
msgstr "簡介"

#: ../../../CompileCudaWithLLVM.rst:11
msgid ""
"This document contains the user guides and the internals of compiling CUDA C/"
"C++ with LLVM. It is aimed at both users who want to compile CUDA with LLVM "
"and developers who want to improve LLVM for GPUs. This document assumes a "
"basic familiarity with CUDA. Information about CUDA programming can be found "
"in the `CUDA programming guide <http://docs.nvidia.com/cuda/cuda-c-"
"programming-guide/index.html>`_."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:19
msgid "How to Build LLVM with CUDA Support"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:21
msgid ""
"CUDA support is still in development and works the best in the trunk version "
"of LLVM. Below is a quick summary of downloading and building the trunk "
"version. Consult the `Getting Started <http://llvm.org/docs/GettingStarted."
"html>`_ page for more details on setting up LLVM."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:27
msgid "Checkout LLVM"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:34
msgid "Checkout Clang"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:42
msgid "Configure and build LLVM and Clang"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:53
msgid "How to Compile CUDA C/C++ with LLVM"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:55
msgid ""
"We assume you have installed the CUDA driver and runtime. Consult the "
"`NVIDIA CUDA installation guide <https://docs.nvidia.com/cuda/cuda-"
"installation-guide-linux/index.html>`_ if you have not."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:60
msgid ""
"Suppose you want to compile and run the following CUDA program (``axpy.cu``) "
"which multiplies a ``float`` array by a ``float`` scalar (AXPY)."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:103
msgid ""
"The command line for compilation is similar to what you would use for C++."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:116
msgid ""
"``<CUDA install path>`` is the root directory where you installed CUDA SDK, "
"typically ``/usr/local/cuda``. ``<GPU arch>`` is `the compute capability of "
"your GPU <https://developer.nvidia.com/cuda-gpus>`_. For example, if you "
"want to run your program on a GPU with compute capability of 3.5, you should "
"specify ``--cuda-gpu-arch=sm_35``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:123
msgid "Detecting clang vs NVCC"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:125
msgid ""
"Although clang's CUDA implementation is largely compatible with NVCC's, you "
"may still want to detect when you're compiling CUDA code specifically with "
"clang."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:128
msgid ""
"This is tricky, because NVCC may invoke clang as part of its own compilation "
"process!  For example, NVCC uses the host compiler's preprocessor when "
"compiling for device code, and that host compiler may in fact be clang."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:132
msgid ""
"When clang is actually compiling CUDA code -- rather than being used as a "
"subtool of NVCC's -- it defines the ``__CUDA__`` macro.  ``__CUDA_ARCH__`` "
"is defined only in device mode (but will be defined if NVCC is using clang "
"as a preprocessor).  So you can use the following incantations to detect "
"clang CUDA compilation, in host and device modes:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:148
msgid ""
"Both clang and nvcc define ``__CUDACC__`` during CUDA compilation.  You can "
"detect NVCC specifically by looking for ``__NVCC__``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:152
msgid "Flags that control numerical code"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:154
msgid ""
"If you're using GPUs, you probably care about making numerical code run "
"fast. GPU hardware allows for more control over numerical operations than "
"most CPUs, but this results in more compiler options for you to juggle."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:158
msgid "Flags you may wish to tweak include:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:160
msgid ""
"``-ffp-contract={on,off,fast}`` (defaults to ``fast`` on host and device "
"when compiling CUDA) Controls whether the compiler emits fused multiply-add "
"operations."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:164
msgid ""
"``off``: never emit fma operations, and prevent ptxas from fusing multiply "
"and add instructions."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:166
msgid ""
"``on``: fuse multiplies and adds within a single statement, but never across "
"statements (C11 semantics).  Prevent ptxas from fusing other multiplies and "
"adds."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:169
msgid ""
"``fast``: fuse multiplies and adds wherever profitable, even across "
"statements.  Doesn't prevent ptxas from fusing additional multiplies and "
"adds."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:173
msgid ""
"Fused multiply-add instructions can be much faster than the unfused "
"equivalents, but because the intermediate result in an fma is not rounded, "
"this flag can affect numerical code."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:177
msgid ""
"``-fcuda-flush-denormals-to-zero`` (default: off) When this is enabled, "
"floating point operations may flush `denormal <https://en.wikipedia.org/wiki/"
"Denormal_number>`_ inputs and/or outputs to 0. Operations on denormal "
"numbers are often much slower than the same operations on normal numbers."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:183
msgid ""
"``-fcuda-approx-transcendentals`` (default: off) When this is enabled, the "
"compiler may emit calls to faster, approximate versions of transcendental "
"functions, instead of using the slower, fully IEEE-compliant versions.  For "
"example, this flag allows clang to emit the ptx ``sin.approx.f32`` "
"instruction."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:189
msgid "This is implied by ``-ffast-math``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:192
msgid "Optimizations"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:194
msgid ""
"CPU and GPU have different design philosophies and architectures. For "
"example, a typical CPU has branch prediction, out-of-order execution, and is "
"superscalar, whereas a typical GPU has none of these. Due to such "
"differences, an optimization pipeline well-tuned for CPUs may be not "
"suitable for GPUs."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:199
msgid ""
"LLVM performs several general and CUDA-specific optimizations for GPUs. The "
"list below shows some of the more important optimizations for GPUs. Most of "
"them have been upstreamed to ``lib/Transforms/Scalar`` and ``lib/Target/"
"NVPTX``. A few of them have not been upstreamed due to lack of a "
"customizable target-independent optimization pipeline."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:205
msgid ""
"**Straight-line scalar optimizations**. These optimizations reduce "
"redundancy in straight-line code. Details can be found in the `design "
"document for straight-line scalar optimizations <https://goo.gl/4Rb9As>`_."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:209
msgid ""
"**Inferring memory spaces**. `This optimization <https://github.com/llvm-"
"mirror/llvm/blob/master/lib/Target/NVPTX/NVPTXInferAddressSpaces.cpp>`_ "
"infers the memory space of an address so that the backend can emit faster "
"special loads and stores from it."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:214
msgid ""
"**Aggressive loop unrooling and function inlining**. Loop unrolling and "
"function inlining need to be more aggressive for GPUs than for CPUs because "
"control flow transfer in GPU is more expensive. They also promote other "
"optimizations such as constant propagation and SROA which sometimes speed up "
"code by over 10x. An empirical inline threshold for GPUs is 1100. This "
"configuration has yet to be upstreamed with a target-specific optimization "
"pipeline. LLVM also provides `loop unrolling pragmas <http://clang.llvm.org/"
"docs/AttributeReference.html#pragma-unroll-pragma-nounroll>`_ and "
"``__attribute__((always_inline))`` for programmers to force unrolling and "
"inling."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:225
msgid ""
"**Aggressive speculative execution**. `This transformation <http://llvm.org/"
"docs/doxygen/html/SpeculativeExecution_8cpp_source.html>`_ is mainly for "
"promoting straight-line scalar optimizations which are most effective on "
"code along dominator paths."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:230
msgid ""
"**Memory-space alias analysis**. `This alias analysis <http://reviews.llvm."
"org/D12414>`_ infers that two pointers in different special memory spaces do "
"not alias. It has yet to be integrated to the new alias analysis "
"infrastructure; the new infrastructure does not run target-specific alias "
"analysis."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:236
msgid ""
"**Bypassing 64-bit divides**. `An existing optimization <http://llvm.org/"
"docs/doxygen/html/BypassSlowDivision_8cpp_source.html>`_ enabled in the "
"NVPTX backend. 64-bit integer divides are much slower than 32-bit ones on "
"NVIDIA GPUs due to lack of a divide unit. Many of the 64-bit divides in our "
"benchmarks have a divisor and dividend which fit in 32-bits at runtime. This "
"optimization provides a fast path for this common case."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:244
msgid "Publication"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:246
msgid ""
"`gpucc: An Open-Source GPGPU Compiler <http://dl.acm.org/citation.cfm?"
"id=2854041>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:247
msgid ""
"Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan, Chris Leary, "
"Jacques Pienaar, Bjarke Roune, Rob Springer, Xuetian Weng, Robert Hundt"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:248
msgid ""
"*Proceedings of the 2016 International Symposium on Code Generation and "
"Optimization (CGO 2016)*"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:249
msgid "`Slides for the CGO talk <http://wujingyue.com/docs/gpucc-talk.pdf>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:252
msgid "Tutorial"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:254
msgid ""
"`CGO 2016 gpucc tutorial <http://wujingyue.com/docs/gpucc-tutorial.pdf>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:257
msgid "Obtaining Help"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:259
msgid ""
"To obtain help on LLVM in general and its CUDA support, see `the LLVM "
"community <http://llvm.org/docs/#mailing-lists>`_."
msgstr ""
