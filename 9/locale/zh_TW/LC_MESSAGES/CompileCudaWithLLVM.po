# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2025, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 9\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-07 18:11+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../CompileCudaWithLLVM.rst:3
msgid "Compiling CUDA with clang"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:9
msgid "Introduction"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:11
msgid ""
"This document describes how to compile CUDA code with clang, and gives some "
"details about LLVM and clang's CUDA implementations."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:14
msgid ""
"This document assumes a basic familiarity with CUDA. Information about CUDA "
"programming can be found in the `CUDA programming guide <http://docs.nvidia."
"com/cuda/cuda-c-programming-guide/index.html>`_."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:20
msgid "Compiling CUDA Code"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:23
msgid "Prerequisites"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:25
msgid ""
"CUDA is supported since llvm 3.9. Current release of clang (7.0.0) supports "
"CUDA 7.0 through 9.2. If you need support for CUDA 10, you will need to use "
"clang built from r342924 or newer."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:29
msgid ""
"Before you build CUDA code, you'll need to have installed the appropriate "
"driver for your nvidia GPU and the CUDA SDK.  See `NVIDIA's CUDA "
"installation guide <https://docs.nvidia.com/cuda/cuda-installation-guide-"
"linux/index.html>`_ for details.  Note that clang `does not support <https://"
"llvm.org/bugs/show_bug.cgi?id=26966>`_ the CUDA toolkit as installed by many "
"Linux package managers; you probably need to install CUDA in a single "
"directory from NVIDIA's package."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:37
msgid ""
"CUDA compilation is supported on Linux. Compilation on MacOS and Windows may "
"or may not work and currently have no maintainers. Compilation with CUDA-9.x "
"is `currently broken on Windows <https://bugs.llvm.org/show_bug.cgi?"
"id=38811>`_."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:42
msgid "Invoking clang"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:44
msgid ""
"Invoking clang for CUDA compilation works similarly to compiling regular C+"
"+. You just need to be aware of a few additional flags."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:47
msgid ""
"You can use `this <https://gist.github."
"com/855e277884eb6b388cd2f00d956c2fd4>`_ program as a toy example.  Save it "
"as ``axpy.cu``.  (Clang detects that you're compiling CUDA code by noticing "
"that your filename ends with ``.cu``. Alternatively, you can pass ``-x "
"cuda``.)"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:52
msgid ""
"To build and run, run the following commands, filling in the parts in angle "
"brackets as described below:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:66
msgid ""
"On MacOS, replace `-lcudart_static` with `-lcudart`; otherwise, you may get "
"\"CUDA driver version is insufficient for CUDA runtime version\" errors when "
"you run your program."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:70
msgid ""
"``<CUDA install path>`` -- the directory where you installed CUDA SDK. "
"Typically, ``/usr/local/cuda``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:73
msgid ""
"Pass e.g. ``-L/usr/local/cuda/lib64`` if compiling in 64-bit mode; "
"otherwise, pass e.g. ``-L/usr/local/cuda/lib``.  (In CUDA, the device code "
"and host code always have the same pointer widths, so if you're compiling 64-"
"bit code for the host, you're also compiling 64-bit code for the device.) "
"Note that as of v10.0 CUDA SDK `no longer supports compilation of 32-bit "
"applications <https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index."
"html#deprecated-features>`_."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:80
msgid ""
"``<GPU arch>`` -- the `compute capability <https://developer.nvidia.com/cuda-"
"gpus>`_ of your GPU. For example, if you want to run your program on a GPU "
"with compute capability of 3.5, specify ``--cuda-gpu-arch=sm_35``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:85
msgid ""
"Note: You cannot pass ``compute_XX`` as an argument to ``--cuda-gpu-arch``; "
"only ``sm_XX`` is currently supported.  However, clang always includes PTX "
"in its binaries, so e.g. a binary compiled with ``--cuda-gpu-arch=sm_30`` "
"would be forwards-compatible with e.g. ``sm_35`` GPUs."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:90
msgid ""
"You can pass ``--cuda-gpu-arch`` multiple times to compile for multiple "
"archs."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:92
msgid ""
"The `-L` and `-l` flags only need to be passed when linking.  When "
"compiling, you may also need to pass ``--cuda-path=/path/to/cuda`` if you "
"didn't install the CUDA SDK into ``/usr/local/cuda`` or ``/usr/local/cuda-X."
"Y``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:97
msgid "Flags that control numerical code"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:99
msgid ""
"If you're using GPUs, you probably care about making numerical code run "
"fast. GPU hardware allows for more control over numerical operations than "
"most CPUs, but this results in more compiler options for you to juggle."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:103
msgid "Flags you may wish to tweak include:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:105
msgid ""
"``-ffp-contract={on,off,fast}`` (defaults to ``fast`` on host and device "
"when compiling CUDA) Controls whether the compiler emits fused multiply-add "
"operations."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:109
msgid ""
"``off``: never emit fma operations, and prevent ptxas from fusing multiply "
"and add instructions."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:111
msgid ""
"``on``: fuse multiplies and adds within a single statement, but never across "
"statements (C11 semantics).  Prevent ptxas from fusing other multiplies and "
"adds."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:114
msgid ""
"``fast``: fuse multiplies and adds wherever profitable, even across "
"statements.  Doesn't prevent ptxas from fusing additional multiplies and "
"adds."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:118
msgid ""
"Fused multiply-add instructions can be much faster than the unfused "
"equivalents, but because the intermediate result in an fma is not rounded, "
"this flag can affect numerical code."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:122
msgid ""
"``-fcuda-flush-denormals-to-zero`` (default: off) When this is enabled, "
"floating point operations may flush `denormal <https://en.wikipedia.org/wiki/"
"Denormal_number>`_ inputs and/or outputs to 0. Operations on denormal "
"numbers are often much slower than the same operations on normal numbers."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:128
msgid ""
"``-fcuda-approx-transcendentals`` (default: off) When this is enabled, the "
"compiler may emit calls to faster, approximate versions of transcendental "
"functions, instead of using the slower, fully IEEE-compliant versions.  For "
"example, this flag allows clang to emit the ptx ``sin.approx.f32`` "
"instruction."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:134
msgid "This is implied by ``-ffast-math``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:137
msgid "Standard library support"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:139
msgid ""
"In clang and nvcc, most of the C++ standard library is not supported on the "
"device side."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:143
msgid "``<math.h>`` and ``<cmath>``"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:145
msgid ""
"In clang, ``math.h`` and ``cmath`` are available and `pass <https://github."
"com/llvm/llvm-test-suite/blob/master/External/CUDA/math_h.cu>`_ `tests "
"<https://github.com/llvm/llvm-test-suite/blob/master/External/CUDA/cmath."
"cu>`_ adapted from libc++'s test suite."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:151
msgid ""
"In nvcc ``math.h`` and ``cmath`` are mostly available.  Versions of ``::"
"foof`` in namespace std (e.g. ``std::sinf``) are not available, and where "
"the standard calls for overloads that take integral arguments, these are "
"usually not available."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:172
msgid "``<std::complex>``"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:174
msgid ""
"nvcc does not officially support ``std::complex``.  It's an error to use "
"``std::complex`` in ``__device__`` code, but it often works in ``__host__ "
"__device__`` code due to nvcc's interpretation of the \"wrong-side "
"rule\" (see below).  However, we have heard from implementers that it's "
"possible to get into situations where nvcc will omit a call to an ``std::"
"complex`` function, especially when compiling without optimizations."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:181
msgid ""
"As of 2016-11-16, clang supports ``std::complex`` without these caveats.  It "
"is tested with libstdc++ 4.8.5 and newer, but is known to work only with "
"libc++ newer than 2016-11-16."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:186
msgid "``<algorithm>``"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:188
msgid ""
"In C++14, many useful functions from ``<algorithm>`` (notably, ``std::min`` "
"and ``std::max``) become constexpr.  You can therefore use these in device "
"code, when compiling with clang."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:193
msgid "Detecting clang vs NVCC from code"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:195
msgid ""
"Although clang's CUDA implementation is largely compatible with NVCC's, you "
"may still want to detect when you're compiling CUDA code specifically with "
"clang."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:198
msgid ""
"This is tricky, because NVCC may invoke clang as part of its own compilation "
"process!  For example, NVCC uses the host compiler's preprocessor when "
"compiling for device code, and that host compiler may in fact be clang."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:202
msgid ""
"When clang is actually compiling CUDA code -- rather than being used as a "
"subtool of NVCC's -- it defines the ``__CUDA__`` macro.  ``__CUDA_ARCH__`` "
"is defined only in device mode (but will be defined if NVCC is using clang "
"as a preprocessor).  So you can use the following incantations to detect "
"clang CUDA compilation, in host and device modes:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:218
msgid ""
"Both clang and nvcc define ``__CUDACC__`` during CUDA compilation.  You can "
"detect NVCC specifically by looking for ``__NVCC__``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:222
msgid "Dialect Differences Between clang and nvcc"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:224
msgid ""
"There is no formal CUDA spec, and clang and nvcc speak slightly different "
"dialects of the language.  Below, we describe some of the differences."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:227
msgid ""
"This section is painful; hopefully you can skip this section and live your "
"life blissfully unaware."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:231
msgid "Compilation Models"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:233
msgid ""
"Most of the differences between clang and nvcc stem from the different "
"compilation models used by clang and nvcc.  nvcc uses *split compilation*, "
"which works roughly as follows:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:237
msgid ""
"Run a preprocessor over the input ``.cu`` file to split it into two source "
"files: ``H``, containing source code for the host, and ``D``, containing "
"source code for the device."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:241 ../../../CompileCudaWithLLVM.rst:261
msgid "For each GPU architecture ``arch`` that we're compiling for, do:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:243
msgid ""
"Compile ``D`` using nvcc proper.  The result of this is a ``ptx`` file for "
"``P_arch``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:246
msgid ""
"Optionally, invoke ``ptxas``, the PTX assembler, to generate a file, "
"``S_arch``, containing GPU machine code (SASS) for ``arch``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:249
msgid ""
"Invoke ``fatbin`` to combine all ``P_arch`` and ``S_arch`` files into a "
"single \"fat binary\" file, ``F``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:252
msgid ""
"Compile ``H`` using an external host compiler (gcc, clang, or whatever you "
"like).  ``F`` is packaged up into a header file which is force-included into "
"``H``; nvcc generates code that calls into this header to e.g. launch "
"kernels."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:257
msgid ""
"clang uses *merged parsing*.  This is similar to split compilation, except "
"all of the host and device code is present and must be semantically-correct "
"in both compilation steps."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:263
msgid ""
"Compile the input ``.cu`` file for device, using clang.  ``__host__`` code "
"is parsed and must be semantically correct, even though we're not generating "
"code for the host at this time."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:267
msgid "The output of this step is a ``ptx`` file ``P_arch``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:269
msgid ""
"Invoke ``ptxas`` to generate a SASS file, ``S_arch``.  Note that, unlike "
"nvcc, clang always generates SASS code."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:272
msgid ""
"Invoke ``fatbin`` to combine all ``P_arch`` and ``S_arch`` files into a "
"single fat binary file, ``F``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:275
msgid ""
"Compile ``H`` using clang.  ``__device__`` code is parsed and must be "
"semantically correct, even though we're not generating code for the device "
"at this time."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:279
msgid ""
"``F`` is passed to this compilation, and clang includes it in a special ELF "
"section, where it can be found by tools like ``cuobjdump``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:282
msgid ""
"(You may ask at this point, why does clang need to parse the input file "
"multiple times?  Why not parse it just once, and then use the AST to "
"generate code for the host and each device architecture?"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:286
msgid ""
"Unfortunately this can't work because we have to define different macros "
"during host compilation and during device compilation for each GPU "
"architecture.)"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:289
msgid ""
"clang's approach allows it to be highly robust to C++ edge cases, as it "
"doesn't need to decide at an early stage which declarations to keep and "
"which to throw away.  But it has some consequences you should be aware of."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:294
msgid "Overloading Based on ``__host__`` and ``__device__`` Attributes"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:296
msgid ""
"Let \"H\", \"D\", and \"HD\" stand for \"``__host__`` functions\", "
"\"``__device__`` functions\", and \"``__host__ __device__`` functions\", "
"respectively.  Functions with no attributes behave the same as H."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:300
msgid ""
"nvcc does not allow you to create H and D functions with the same signature:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:308
msgid ""
"However, nvcc allows you to \"overload\" H and D functions with different "
"signatures:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:317
msgid ""
"In clang, the ``__host__`` and ``__device__`` attributes are part of a "
"function's signature, and so it's legal to have H and D functions with "
"(otherwise) the same signature:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:327
msgid ""
"HD functions cannot be overloaded by H or D functions with the same "
"signature:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:341
msgid ""
"When resolving an overloaded function, clang considers the host/device "
"attributes of the caller and callee.  These are used as a tiebreaker during "
"overload resolution.  See `IdentifyCUDAPreference <http://clang.llvm.org/"
"doxygen/SemaCUDA_8cpp.html>`_ for the full set of rules, but at a high level "
"they are:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:347
msgid "D functions prefer to call other Ds.  HDs are given lower priority."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:349
msgid ""
"Similarly, H functions prefer to call other Hs, or ``__global__`` functions "
"(with equal priority).  HDs are given lower priority."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:352
msgid "HD functions prefer to call other HDs."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:354
msgid ""
"When compiling for device, HDs will call Ds with lower priority than HD, and "
"will call Hs with still lower priority.  If it's forced to call an H, the "
"program is malformed if we emit code for this HD function.  We call this the "
"\"wrong-side rule\", see example below."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:359
msgid "The rules are symmetrical when compiling for host."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:361
msgid "Some examples:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:386
msgid "Wrong-side rule example:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:406
msgid ""
"For the purposes of the wrong-side rule, templated functions also behave "
"like ``inline`` functions: They aren't codegen'ed unless they're "
"instantiated (usually as part of the process of invoking them)."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:410
msgid ""
"clang's behavior with respect to the wrong-side rule matches nvcc's, except "
"nvcc only emits a warning for ``not_inline_hd``; device code is allowed to "
"call ``not_inline_hd``.  In its generated code, nvcc may omit "
"``not_inline_hd``'s call to ``host_only`` entirely, or it may try to "
"generate code for ``host_only`` on the device.  What you get seems to depend "
"on whether or not the compiler chooses to inline ``host_only``."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:417
msgid ""
"Member functions, including constructors, may be overloaded using H and D "
"attributes.  However, destructors cannot be overloaded."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:421
msgid "Using a Different Class on Host/Device"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:423
msgid ""
"Occasionally you may want to have a class with different host/device "
"versions."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:425
msgid ""
"If all of the class's members are the same on the host and device, you can "
"just provide overloads for the class's member functions."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:428
msgid ""
"However, if you want your class to have different members on host/device, "
"you won't be able to provide working H and D overloads in both classes. In "
"this case, clang is likely to be unhappy with you."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:454
msgid ""
"We posit that you don't really want to have classes with different members "
"on H and D.  For example, if you were to pass one of these as a parameter to "
"a kernel, it would have a different layout on H and D, so would not work "
"properly."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:459
msgid ""
"To make code like this compatible with clang, we recommend you separate it "
"out into two classes.  If you need to write code that works on both host and "
"device, consider writing an overloaded wrapper function that returns "
"different types on host and device."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:474
msgid ""
"Unfortunately, this idiom isn't compatible with nvcc, because it doesn't "
"allow you to overload based on the H/D attributes.  Here's an idiom that "
"works with both clang and nvcc:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:496
msgid "Hopefully you don't have to do this sort of thing often."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:499
msgid "Optimizations"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:501
msgid ""
"Modern CPUs and GPUs are architecturally quite different, so code that's "
"fast on a CPU isn't necessarily fast on a GPU.  We've made a number of "
"changes to LLVM to make it generate good GPU code.  Among these changes are:"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:505
msgid ""
"`Straight-line scalar optimizations <https://goo.gl/4Rb9As>`_ -- These "
"reduce redundancy within straight-line code."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:508
msgid ""
"`Aggressive speculative execution <http://llvm.org/docs/doxygen/html/"
"SpeculativeExecution_8cpp_source.html>`_ -- This is mainly for promoting "
"straight-line scalar optimizations, which are most effective on code along "
"dominator paths."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:513
msgid ""
"`Memory space inference <http://llvm.org/doxygen/"
"NVPTXInferAddressSpaces_8cpp_source.html>`_ -- In PTX, we can operate on "
"pointers that are in a paricular \"address space\" (global, shared, "
"constant, or local), or we can operate on pointers in the \"generic\" "
"address space, which can point to anything.  Operations in a non-generic "
"address space are faster, but pointers in CUDA are not explicitly annotated "
"with their address space, so it's up to LLVM to infer it where possible."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:522
msgid ""
"`Bypassing 64-bit divides <http://llvm.org/docs/doxygen/html/"
"BypassSlowDivision_8cpp_source.html>`_ -- This was an existing optimization "
"that we enabled for the PTX backend."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:526
msgid ""
"64-bit integer divides are much slower than 32-bit ones on NVIDIA GPUs. Many "
"of the 64-bit divides in our benchmarks have a divisor and dividend which "
"fit in 32-bits at runtime. This optimization provides a fast path for this "
"common case."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:531
msgid ""
"Aggressive loop unrooling and function inlining -- Loop unrolling and "
"function inlining need to be more aggressive for GPUs than for CPUs because "
"control flow transfer in GPU is more expensive. More aggressive unrolling "
"and inlining also promote other optimizations, such as constant propagation "
"and SROA, which sometimes speed up code by over 10x."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:537
msgid ""
"(Programmers can force unrolling and inline using clang's `loop unrolling "
"pragmas <http://clang.llvm.org/docs/AttributeReference.html#pragma-unroll-"
"pragma-nounroll>`_ and ``__attribute__((always_inline))``.)"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:542
msgid "Publication"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:544
msgid ""
"The team at Google published a paper in CGO 2016 detailing the optimizations "
"they'd made to clang/LLVM.  Note that \"gpucc\" is no longer a meaningful "
"name: The relevant tools are now just vanilla clang/LLVM."
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:548
msgid ""
"`gpucc: An Open-Source GPGPU Compiler <http://dl.acm.org/citation.cfm?"
"id=2854041>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:549
msgid ""
"Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan, Chris Leary, "
"Jacques Pienaar, Bjarke Roune, Rob Springer, Xuetian Weng, Robert Hundt"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:550
msgid ""
"*Proceedings of the 2016 International Symposium on Code Generation and "
"Optimization (CGO 2016)*"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:552
msgid ""
"`Slides from the CGO talk <http://wujingyue.github.io/docs/gpucc-talk.pdf>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:554
msgid ""
"`Tutorial given at CGO <http://wujingyue.github.io/docs/gpucc-tutorial.pdf>`_"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:557
msgid "Obtaining Help"
msgstr ""

#: ../../../CompileCudaWithLLVM.rst:559
msgid ""
"To obtain help on LLVM in general and its CUDA support, see `the LLVM "
"community <http://llvm.org/docs/#mailing-lists>`_."
msgstr ""
